{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Boo-specific libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Yunzhou-specific libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEPS = 10\n",
    "SPLIT = 0.2\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import and Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "raw_cases_df = pd.read_csv('Data/confirmed_diff.csv')\n",
    "raw_deaths_df = pd.read_csv('Data/deaths_diff.csv')\n",
    "raw_mobility_df = pd.read_csv('Data/Google_Mobility.csv')\n",
    "raw_whole_usa_cases_df = pd.read_csv('Data/us_confirmed_cases.csv')\n",
    "raw_whole_usa_deaths_df = pd.read_csv('Data/us_confirmed_deaths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR STATES\n",
    "# Clean Mobility Data\n",
    "# mobility_df already contains only those counties and states for which\n",
    "# complete data on all 6 mobility categories exist\n",
    "mobility_df = raw_mobility_df.copy()\n",
    "mobility_df = mobility_df.rename(columns={\"retail_and_recreation_percent_change_from_baseline\": \"retail_and_recreation\",\n",
    "        \"grocery_and_pharmacy_percent_change_from_baseline\": \"grocery_and_pharmacy\",\n",
    "        \"parks_percent_change_from_baseline\": \"parks\",\n",
    "        \"transit_stations_percent_change_from_baseline\": \"transit_stations\",\n",
    "        \"workplaces_percent_change_from_baseline\": \"workplaces\",\n",
    "        \"residential_percent_change_from_baseline\": \"residential\"})\n",
    "# Optional extreme shortening of names\n",
    "if True:\n",
    "    mobility_df = mobility_df.rename(columns={\"retail_and_recreation\": \"rr\",\n",
    "            \"grocery_and_pharmacy\": \"gp\",\n",
    "            \"transit_stations\": \"ts\",\n",
    "            \"workplaces\": \"wp\",\n",
    "            \"residential\": \"res\"})\n",
    "# US is the country already, do not need that data\n",
    "mobility_df.drop(columns=['country_region_code', 'country_region'], inplace=True)\n",
    "mobility_df.rename(columns={'sub_region_1': 'state', 'sub_region_2': 'county'}, inplace=True)\n",
    "\n",
    "# Extract all states only data, which have been marked with 'ZZZ' in the county name\n",
    "state_mobility_df = (mobility_df[mobility_df['county'] == 'ZZZ']).copy()\n",
    "state_mobility_df.drop(columns=['county'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR STATES\n",
    "# Clean Cases Data\n",
    "cases_df = raw_cases_df.copy()\n",
    "cases_df.rename(columns={'Admin2': 'county', 'Province_State': 'state', 'Date': 'date', 'Value': 'cases'}, inplace=True)\n",
    "# Since this is on the state level, county info is not needed\n",
    "# 'region' and 'diff' are legacy columns that are not needed here\n",
    "cases_df.drop(columns=['Country_Region', 'county', 'Lat', 'Long_', 'region', 'diff'], inplace=True)\n",
    "# Sum up cases for each state\n",
    "state_cases_df = cases_df.groupby(['state', 'date'], as_index=False).agg({'cases':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR STATES\n",
    "# Merge Google Mobility and Cases Data\n",
    "state_cases_mobility_df = state_cases_df.merge(state_mobility_df, how='inner', on=['state', 'date'])\n",
    "# Sort to ensure proper order\n",
    "state_cases_mobility_df['date'] = pd.to_datetime(state_cases_mobility_df['date'], format=\"%m/%d/%Y\")\n",
    "state_cases_mobility_df.sort_values(['state', 'date'], ascending=[True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out all applicable states for later\n",
    "state_names = ['South Carolina', 'Louisiana', 'Virginia', 'Idaho', 'Iowa',\n",
    "               'Kentucky', 'Missouri', 'Oklahoma', 'Colorado', 'Illinois',\n",
    "               'Indiana', 'Mississippi', 'Nebraska', 'North Dakota', 'Ohio',\n",
    "               'Pennsylvania', 'Washington', 'Wisconsin', 'Vermont', 'Minnesota',\n",
    "               'Florida', 'North Carolina', 'California', 'New York', 'Wyoming',\n",
    "               'Michigan', 'Alaska', 'Maryland', 'Kansas', 'Tennessee', 'Texas',\n",
    "               'Maine', 'Arizona', 'Georgia', 'Arkansas', 'New Jersey',\n",
    "               'South Dakota', 'Alabama', 'Oregon', 'West Virginia',\n",
    "               'Massachusetts', 'Utah', 'Montana', 'New Hampshire', 'New Mexico',\n",
    "               'Rhode Island', 'Nevada', 'District of Columbia', 'Connecticut',\n",
    "               'Hawaii']\n",
    "# total of 50 statewide data (49 states + Washington D.C.)\n",
    "# Missing Delaware\n",
    "state_names_no_mobility = state_names.copy()\n",
    "state_names_no_mobility.append('Delaware')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is, \"state_cases_mobility_df\" contains confirmed cases data for each state at each point in time, with mobility data.\n",
    "\n",
    "\"state_cases_df\" only contains confirmed cases data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform Data**\n",
    "\n",
    "*Note that these models all assume,\n",
    "the underlying trend of cases for each state is the same,\n",
    "and there are no state-by-state variations in how the disease spreads,\n",
    "which is likely unrealistic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Data\n",
    "# Following Boo\n",
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# state_cases_mobility_normalized_df = scaler.fit_transform(state_cases_mobility_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to Split Data into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize Data for LSTM\n",
    "# Objective is to predict cases for current day given previous N_STEPS days\n",
    "# However, having multiple states, each as their own individual time series,\n",
    "# complicates this.\n",
    "# input should be of shape (n_steps, n_features) with multiple samples\n",
    "\n",
    "# Modified from Boo's code\n",
    "def get_LSTM_states_dataset(dataset, n_steps=1):\n",
    "    dataset = dataset.copy()\n",
    "    dataset.drop(columns=['date'], inplace=True)\n",
    "    #print(dataset.shape)\n",
    "    dataset_by_states = []\n",
    "    # Normalize data while you are at it\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    for state in dataset['state'].unique():\n",
    "        state_values = dataset[dataset['state'] == state].drop(columns='state').values\n",
    "        state_values = scaler.fit_transform(state_values)\n",
    "        dataset_by_states.append(state_values)\n",
    "    X, y = [], []\n",
    "    for state_dataset in dataset_by_states:\n",
    "        for i in range(len(state_dataset) - n_steps - 1):\n",
    "            a = state_dataset[i:(i+n_steps)]\n",
    "            X.append(a)\n",
    "            y.append(state_dataset[i + n_steps][0])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return X, y\n",
    "\n",
    "X_LSTM, y_LSTM = get_LSTM_states_dataset(state_cases_mobility_df, n_steps=N_STEPS)\n",
    "X_train_LSTM, X_test_LSTM, y_train_LSTM, y_test_LSTM = train_test_split(X_LSTM, y_LSTM, test_size=SPLIT, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize Data for CNN/Regular NN\n",
    "# Objective is to predict cases for current day given previous N_STEPS days.\n",
    "# We must add the value of the columns from previous\n",
    "# days as extra features per observation\n",
    "def get_CNN_states_dataset(dataset, n_steps=1):\n",
    "    dataset = dataset.copy()\n",
    "    dataset.drop(columns=['date'], inplace=True)\n",
    "    #print(dataset.shape)\n",
    "    dataset_by_states = []\n",
    "    # Normalize data while you are at it\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    for state in dataset['state'].unique():\n",
    "        state_values = dataset[dataset['state'] == state].drop(columns='state').values\n",
    "        state_values = scaler.fit_transform(state_values)\n",
    "        dataset_by_states.append(state_values)\n",
    "    X, y = [], []\n",
    "    for state_dataset in dataset_by_states:\n",
    "        for i in range(len(state_dataset) - n_steps - 1):\n",
    "            a = state_dataset[i:(i+n_steps)]\n",
    "            X.append(a)\n",
    "            y.append(state_dataset[i + n_steps][0])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return X, y\n",
    "\n",
    "X_CNN, y_CNN = get_CNN_states_dataset(state_cases_mobility_df, n_steps=N_STEPS)\n",
    "X_train_CNN, X_test_CNN, y_train_CNN, y_test_CNN = train_test_split(X_CNN, y_CNN, test_size=SPLIT, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize Data for Yunzhou's NN\n",
    "\n",
    "def get_YNN_states_dataset(dataset, n_steps=1):\n",
    "    dataset = dataset.copy()\n",
    "    dataset.drop(columns=['date'], inplace=True)\n",
    "    #print(dataset.shape)\n",
    "    dataset_by_states = []\n",
    "    # Normalize data while you are at it\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    for state in dataset['state'].unique():\n",
    "        state_values = dataset[dataset['state'] == state].drop(columns='state').values\n",
    "        state_values = scaler.fit_transform(state_values)\n",
    "        dataset_by_states.append(state_values)\n",
    "    X, y = [], []\n",
    "    for state_dataset in dataset_by_states:\n",
    "        for i in range(len(state_dataset) - n_steps - 1):\n",
    "            a = state_dataset[i:(i+n_steps)]\n",
    "            # Flatten a, a list of lists, into a single row of features\n",
    "            flat_a = []\n",
    "            for sublist in a:\n",
    "                for item in sublist:\n",
    "                    flat_a.append(item)\n",
    "            X.append(flat_a)\n",
    "            y.append(state_dataset[i + n_steps][0])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return X, y\n",
    "\n",
    "X_YNN, y_YNN = get_YNN_states_dataset(state_cases_mobility_df, n_steps=N_STEPS)\n",
    "X_train_YNN, X_test_YNN, y_train_YNN, y_test_YNN = train_test_split(X_YNN, y_YNN, test_size=SPLIT, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM Model with Mobility Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LSTM, Boo's model\n",
    "def get_lstm(n_steps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(n_steps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer = adam, loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "n_lstm_features = X_train_LSTM.shape[2]\n",
    "lstm = get_lstm(N_STEPS, n_lstm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2440 samples, validate on 610 samples\n",
      "Epoch 1/500\n",
      "2440/2440 [==============================] - 1s 399us/step - loss: 0.0252 - mae: 0.1046 - mse: 0.0252 - val_loss: 0.0031 - val_mae: 0.0398 - val_mse: 0.0031\n",
      "Epoch 2/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 0.0024 - mae: 0.0347 - mse: 0.0024 - val_loss: 0.0019 - val_mae: 0.0282 - val_mse: 0.0019\n",
      "Epoch 3/500\n",
      "2440/2440 [==============================] - 1s 292us/step - loss: 0.0018 - mae: 0.0315 - mse: 0.0018 - val_loss: 0.0015 - val_mae: 0.0269 - val_mse: 0.0015\n",
      "Epoch 4/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 0.0014 - mae: 0.0269 - mse: 0.0014 - val_loss: 0.0014 - val_mae: 0.0237 - val_mse: 0.0014\n",
      "Epoch 5/500\n",
      "2440/2440 [==============================] - 1s 266us/step - loss: 0.0012 - mae: 0.0249 - mse: 0.0012 - val_loss: 0.0012 - val_mae: 0.0227 - val_mse: 0.0012\n",
      "Epoch 6/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 0.0011 - mae: 0.0233 - mse: 0.0011 - val_loss: 0.0011 - val_mae: 0.0214 - val_mse: 0.0011\n",
      "Epoch 7/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 9.6685e-04 - mae: 0.0219 - mse: 9.6685e-04 - val_loss: 9.4089e-04 - val_mae: 0.0202 - val_mse: 9.4089e-04\n",
      "Epoch 8/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 9.0345e-04 - mae: 0.0213 - mse: 9.0345e-04 - val_loss: 8.7361e-04 - val_mae: 0.0189 - val_mse: 8.7361e-04\n",
      "Epoch 9/500\n",
      "2440/2440 [==============================] - 1s 308us/step - loss: 8.7582e-04 - mae: 0.0212 - mse: 8.7582e-04 - val_loss: 8.3631e-04 - val_mae: 0.0186 - val_mse: 8.3631e-04\n",
      "Epoch 10/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 8.5603e-04 - mae: 0.0211 - mse: 8.5603e-04 - val_loss: 7.9515e-04 - val_mae: 0.0183 - val_mse: 7.9515e-04\n",
      "Epoch 11/500\n",
      "2440/2440 [==============================] - 1s 263us/step - loss: 8.1692e-04 - mae: 0.0207 - mse: 8.1692e-04 - val_loss: 7.4250e-04 - val_mae: 0.0174 - val_mse: 7.4250e-04\n",
      "Epoch 12/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 7.6273e-04 - mae: 0.0198 - mse: 7.6273e-04 - val_loss: 6.9514e-04 - val_mae: 0.0166 - val_mse: 6.9514e-04\n",
      "Epoch 13/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 7.1098e-04 - mae: 0.0190 - mse: 7.1098e-04 - val_loss: 6.5596e-04 - val_mae: 0.0161 - val_mse: 6.5596e-04\n",
      "Epoch 14/500\n",
      "2440/2440 [==============================] - 1s 288us/step - loss: 6.6782e-04 - mae: 0.0183 - mse: 6.6782e-04 - val_loss: 6.2185e-04 - val_mae: 0.0157 - val_mse: 6.2185e-04\n",
      "Epoch 15/500\n",
      "2440/2440 [==============================] - 1s 262us/step - loss: 6.3059e-04 - mae: 0.0178 - mse: 6.3059e-04 - val_loss: 5.9032e-04 - val_mae: 0.0154 - val_mse: 5.9032e-04\n",
      "Epoch 16/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 5.9625e-04 - mae: 0.0173 - mse: 5.9625e-04 - val_loss: 5.6016e-04 - val_mae: 0.0151 - val_mse: 5.6016e-04\n",
      "Epoch 17/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 5.6369e-04 - mae: 0.0167 - mse: 5.6369e-04 - val_loss: 5.3123e-04 - val_mae: 0.0149 - val_mse: 5.3123e-04\n",
      "Epoch 18/500\n",
      "2440/2440 [==============================] - 1s 261us/step - loss: 5.3298e-04 - mae: 0.0162 - mse: 5.3298e-04 - val_loss: 5.0386e-04 - val_mae: 0.0146 - val_mse: 5.0386e-04\n",
      "Epoch 19/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 5.0442e-04 - mae: 0.0158 - mse: 5.0442e-04 - val_loss: 4.7839e-04 - val_mae: 0.0143 - val_mse: 4.7839e-04\n",
      "Epoch 20/500\n",
      "2440/2440 [==============================] - 1s 260us/step - loss: 4.7814e-04 - mae: 0.0153 - mse: 4.7814e-04 - val_loss: 4.5507e-04 - val_mae: 0.0141 - val_mse: 4.5507e-04\n",
      "Epoch 21/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 4.5401e-04 - mae: 0.0149 - mse: 4.5401e-04 - val_loss: 4.3400e-04 - val_mae: 0.0138 - val_mse: 4.3400e-04\n",
      "Epoch 22/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 4.3177e-04 - mae: 0.0145 - mse: 4.3177e-04 - val_loss: 4.1501e-04 - val_mae: 0.0136 - val_mse: 4.1501e-04\n",
      "Epoch 23/500\n",
      "2440/2440 [==============================] - 1s 263us/step - loss: 4.1102e-04 - mae: 0.0142 - mse: 4.1102e-04 - val_loss: 3.9762e-04 - val_mae: 0.0134 - val_mse: 3.9762e-04\n",
      "Epoch 24/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 3.9143e-04 - mae: 0.0138 - mse: 3.9143e-04 - val_loss: 3.8121e-04 - val_mae: 0.0131 - val_mse: 3.8121e-04\n",
      "Epoch 25/500\n",
      "2440/2440 [==============================] - 1s 261us/step - loss: 3.7282e-04 - mae: 0.0134 - mse: 3.7282e-04 - val_loss: 3.6529e-04 - val_mae: 0.0127 - val_mse: 3.6529e-04\n",
      "Epoch 26/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 3.5522e-04 - mae: 0.0130 - mse: 3.5522e-04 - val_loss: 3.4979e-04 - val_mae: 0.0122 - val_mse: 3.4979e-04\n",
      "Epoch 27/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 3.3884e-04 - mae: 0.0125 - mse: 3.3884e-04 - val_loss: 3.3512e-04 - val_mae: 0.0117 - val_mse: 3.3512e-04\n",
      "Epoch 28/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 3.2397e-04 - mae: 0.0121 - mse: 3.2397e-04 - val_loss: 3.2195e-04 - val_mae: 0.0112 - val_mse: 3.2195e-04\n",
      "Epoch 29/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 3.1087e-04 - mae: 0.0118 - mse: 3.1087e-04 - val_loss: 3.1086e-04 - val_mae: 0.0108 - val_mse: 3.1086e-04\n",
      "Epoch 30/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 2.9960e-04 - mae: 0.0115 - mse: 2.9960e-04 - val_loss: 3.0208e-04 - val_mae: 0.0104 - val_mse: 3.0208e-04\n",
      "Epoch 31/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 2.9005e-04 - mae: 0.0112 - mse: 2.9005e-04 - val_loss: 2.9538e-04 - val_mae: 0.0101 - val_mse: 2.9538e-04\n",
      "Epoch 32/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 2.8193e-04 - mae: 0.0110 - mse: 2.8193e-04 - val_loss: 2.9032e-04 - val_mae: 0.0099 - val_mse: 2.9032e-04\n",
      "Epoch 33/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 2.7490e-04 - mae: 0.0109 - mse: 2.7490e-04 - val_loss: 2.8641e-04 - val_mae: 0.0098 - val_mse: 2.8641e-04\n",
      "Epoch 34/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 2.6865e-04 - mae: 0.0107 - mse: 2.6865e-04 - val_loss: 2.8327e-04 - val_mae: 0.0097 - val_mse: 2.8327e-04\n",
      "Epoch 35/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 2.6295e-04 - mae: 0.0106 - mse: 2.6295e-04 - val_loss: 2.8064e-04 - val_mae: 0.0096 - val_mse: 2.8064e-04\n",
      "Epoch 36/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 2.5764e-04 - mae: 0.0105 - mse: 2.5764e-04 - val_loss: 2.7836e-04 - val_mae: 0.0096 - val_mse: 2.7836e-04\n",
      "Epoch 37/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 2.5264e-04 - mae: 0.0104 - mse: 2.5264e-04 - val_loss: 2.7632e-04 - val_mae: 0.0095 - val_mse: 2.7632e-04\n",
      "Epoch 38/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 2.4790e-04 - mae: 0.0103 - mse: 2.4790e-04 - val_loss: 2.7442e-04 - val_mae: 0.0095 - val_mse: 2.7442e-04\n",
      "Epoch 39/500\n",
      "2440/2440 [==============================] - 1s 313us/step - loss: 2.4339e-04 - mae: 0.0102 - mse: 2.4339e-04 - val_loss: 2.7263e-04 - val_mae: 0.0094 - val_mse: 2.7263e-04\n",
      "Epoch 40/500\n",
      "2440/2440 [==============================] - 1s 349us/step - loss: 2.3910e-04 - mae: 0.0101 - mse: 2.3910e-04 - val_loss: 2.7090e-04 - val_mae: 0.0094 - val_mse: 2.7090e-04\n",
      "Epoch 41/500\n",
      "2440/2440 [==============================] - 1s 298us/step - loss: 2.3502e-04 - mae: 0.0100 - mse: 2.3502e-04 - val_loss: 2.6921e-04 - val_mae: 0.0093 - val_mse: 2.6921e-04\n",
      "Epoch 42/500\n",
      "2440/2440 [==============================] - 1s 267us/step - loss: 2.3114e-04 - mae: 0.0100 - mse: 2.3114e-04 - val_loss: 2.6754e-04 - val_mae: 0.0093 - val_mse: 2.6754e-04\n",
      "Epoch 43/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 2.2747e-04 - mae: 0.0099 - mse: 2.2747e-04 - val_loss: 2.6589e-04 - val_mae: 0.0092 - val_mse: 2.6589e-04\n",
      "Epoch 44/500\n",
      "2440/2440 [==============================] - 1s 262us/step - loss: 2.2399e-04 - mae: 0.0098 - mse: 2.2399e-04 - val_loss: 2.6424e-04 - val_mae: 0.0092 - val_mse: 2.6424e-04\n",
      "Epoch 45/500\n",
      "2440/2440 [==============================] - 1s 325us/step - loss: 2.2070e-04 - mae: 0.0097 - mse: 2.2070e-04 - val_loss: 2.6261e-04 - val_mae: 0.0091 - val_mse: 2.6261e-04\n",
      "Epoch 46/500\n",
      "2440/2440 [==============================] - 1s 292us/step - loss: 2.1758e-04 - mae: 0.0096 - mse: 2.1758e-04 - val_loss: 2.6101e-04 - val_mae: 0.0090 - val_mse: 2.6101e-04\n",
      "Epoch 47/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 2.1464e-04 - mae: 0.0096 - mse: 2.1464e-04 - val_loss: 2.5944e-04 - val_mae: 0.0090 - val_mse: 2.5944e-04\n",
      "Epoch 48/500\n",
      "2440/2440 [==============================] - 1s 269us/step - loss: 2.1185e-04 - mae: 0.0095 - mse: 2.1185e-04 - val_loss: 2.5791e-04 - val_mae: 0.0089 - val_mse: 2.5791e-04\n",
      "Epoch 49/500\n",
      "2440/2440 [==============================] - 1s 338us/step - loss: 2.0921e-04 - mae: 0.0094 - mse: 2.0921e-04 - val_loss: 2.5644e-04 - val_mae: 0.0089 - val_mse: 2.5644e-04\n",
      "Epoch 50/500\n",
      "2440/2440 [==============================] - 1s 313us/step - loss: 2.0670e-04 - mae: 0.0094 - mse: 2.0670e-04 - val_loss: 2.5502e-04 - val_mae: 0.0088 - val_mse: 2.5502e-04\n",
      "Epoch 51/500\n",
      "2440/2440 [==============================] - 1s 307us/step - loss: 2.0432e-04 - mae: 0.0093 - mse: 2.0432e-04 - val_loss: 2.5368e-04 - val_mae: 0.0088 - val_mse: 2.5368e-04\n",
      "Epoch 52/500\n",
      "2440/2440 [==============================] - 1s 297us/step - loss: 2.0205e-04 - mae: 0.0092 - mse: 2.0205e-04 - val_loss: 2.5240e-04 - val_mae: 0.0088 - val_mse: 2.5240e-04\n",
      "Epoch 53/500\n",
      "2440/2440 [==============================] - 1s 348us/step - loss: 1.9987e-04 - mae: 0.0092 - mse: 1.9987e-04 - val_loss: 2.5118e-04 - val_mae: 0.0087 - val_mse: 2.5118e-04\n",
      "Epoch 54/500\n",
      "2440/2440 [==============================] - 1s 311us/step - loss: 1.9777e-04 - mae: 0.0091 - mse: 1.9777e-04 - val_loss: 2.5003e-04 - val_mae: 0.0087 - val_mse: 2.5003e-04\n",
      "Epoch 55/500\n",
      "2440/2440 [==============================] - 1s 310us/step - loss: 1.9575e-04 - mae: 0.0091 - mse: 1.9575e-04 - val_loss: 2.4892e-04 - val_mae: 0.0087 - val_mse: 2.4892e-04\n",
      "Epoch 56/500\n",
      "2440/2440 [==============================] - 1s 365us/step - loss: 1.9380e-04 - mae: 0.0090 - mse: 1.9380e-04 - val_loss: 2.4784e-04 - val_mae: 0.0087 - val_mse: 2.4784e-04\n",
      "Epoch 57/500\n",
      "2440/2440 [==============================] - 1s 309us/step - loss: 1.9190e-04 - mae: 0.0090 - mse: 1.9190e-04 - val_loss: 2.4678e-04 - val_mae: 0.0087 - val_mse: 2.4678e-04\n",
      "Epoch 58/500\n",
      "2440/2440 [==============================] - 1s 304us/step - loss: 1.9005e-04 - mae: 0.0090 - mse: 1.9005e-04 - val_loss: 2.4573e-04 - val_mae: 0.0087 - val_mse: 2.4573e-04\n",
      "Epoch 59/500\n",
      "2440/2440 [==============================] - 1s 309us/step - loss: 1.8825e-04 - mae: 0.0089 - mse: 1.8825e-04 - val_loss: 2.4466e-04 - val_mae: 0.0087 - val_mse: 2.4466e-04\n",
      "Epoch 60/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 1.8649e-04 - mae: 0.0089 - mse: 1.8649e-04 - val_loss: 2.4356e-04 - val_mae: 0.0087 - val_mse: 2.4356e-04\n",
      "Epoch 61/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 1.8479e-04 - mae: 0.0088 - mse: 1.8478e-04 - val_loss: 2.4242e-04 - val_mae: 0.0086 - val_mse: 2.4242e-04\n",
      "Epoch 62/500\n",
      "2440/2440 [==============================] - 1s 295us/step - loss: 1.8312e-04 - mae: 0.0088 - mse: 1.8312e-04 - val_loss: 2.4124e-04 - val_mae: 0.0086 - val_mse: 2.4124e-04\n",
      "Epoch 63/500\n",
      "2440/2440 [==============================] - 1s 310us/step - loss: 1.8150e-04 - mae: 0.0087 - mse: 1.8150e-04 - val_loss: 2.4001e-04 - val_mae: 0.0086 - val_mse: 2.4001e-04\n",
      "Epoch 64/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.7992e-04 - mae: 0.0087 - mse: 1.7992e-04 - val_loss: 2.3875e-04 - val_mae: 0.0086 - val_mse: 2.3875e-04\n",
      "Epoch 65/500\n",
      "2440/2440 [==============================] - 1s 297us/step - loss: 1.7838e-04 - mae: 0.0087 - mse: 1.7838e-04 - val_loss: 2.3745e-04 - val_mae: 0.0085 - val_mse: 2.3745e-04\n",
      "Epoch 66/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 1.7688e-04 - mae: 0.0086 - mse: 1.7688e-04 - val_loss: 2.3613e-04 - val_mae: 0.0085 - val_mse: 2.3613e-04\n",
      "Epoch 67/500\n",
      "2440/2440 [==============================] - 1s 302us/step - loss: 1.7543e-04 - mae: 0.0086 - mse: 1.7543e-04 - val_loss: 2.3481e-04 - val_mae: 0.0084 - val_mse: 2.3481e-04\n",
      "Epoch 68/500\n",
      "2440/2440 [==============================] - 1s 321us/step - loss: 1.7401e-04 - mae: 0.0085 - mse: 1.7401e-04 - val_loss: 2.3350e-04 - val_mae: 0.0084 - val_mse: 2.3350e-04\n",
      "Epoch 69/500\n",
      "2440/2440 [==============================] - ETA: 0s - loss: 1.7197e-04 - mae: 0.0085 - mse: 1.7198e-0 - 1s 319us/step - loss: 1.7263e-04 - mae: 0.0085 - mse: 1.7263e-04 - val_loss: 2.3220e-04 - val_mae: 0.0083 - val_mse: 2.3220e-04\n",
      "Epoch 70/500\n",
      "2440/2440 [==============================] - 1s 312us/step - loss: 1.7129e-04 - mae: 0.0085 - mse: 1.7129e-04 - val_loss: 2.3093e-04 - val_mae: 0.0083 - val_mse: 2.3093e-04\n",
      "Epoch 71/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.6998e-04 - mae: 0.0084 - mse: 1.6998e-04 - val_loss: 2.2968e-04 - val_mae: 0.0082 - val_mse: 2.2968e-04\n",
      "Epoch 72/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.6871e-04 - mae: 0.0084 - mse: 1.6871e-04 - val_loss: 2.2847e-04 - val_mae: 0.0082 - val_mse: 2.2847e-04\n",
      "Epoch 73/500\n",
      "2440/2440 [==============================] - 1s 303us/step - loss: 1.6747e-04 - mae: 0.0083 - mse: 1.6747e-04 - val_loss: 2.2730e-04 - val_mae: 0.0081 - val_mse: 2.2730e-04\n",
      "Epoch 74/500\n",
      "2440/2440 [==============================] - 1s 315us/step - loss: 1.6626e-04 - mae: 0.0083 - mse: 1.6626e-04 - val_loss: 2.2616e-04 - val_mae: 0.0081 - val_mse: 2.2616e-04\n",
      "Epoch 75/500\n",
      "2440/2440 [==============================] - 1s 310us/step - loss: 1.6508e-04 - mae: 0.0083 - mse: 1.6508e-04 - val_loss: 2.2506e-04 - val_mae: 0.0080 - val_mse: 2.2506e-04\n",
      "Epoch 76/500\n",
      "2440/2440 [==============================] - 1s 310us/step - loss: 1.6394e-04 - mae: 0.0082 - mse: 1.6394e-04 - val_loss: 2.2399e-04 - val_mae: 0.0079 - val_mse: 2.2399e-04\n",
      "Epoch 77/500\n",
      "2440/2440 [==============================] - 1s 310us/step - loss: 1.6284e-04 - mae: 0.0082 - mse: 1.6284e-04 - val_loss: 2.2298e-04 - val_mae: 0.0079 - val_mse: 2.2298e-04\n",
      "Epoch 78/500\n",
      "2440/2440 [==============================] - 1s 323us/step - loss: 1.6177e-04 - mae: 0.0082 - mse: 1.6177e-04 - val_loss: 2.2201e-04 - val_mae: 0.0078 - val_mse: 2.2201e-04\n",
      "Epoch 79/500\n",
      "2440/2440 [==============================] - 1s 307us/step - loss: 1.6076e-04 - mae: 0.0081 - mse: 1.6076e-04 - val_loss: 2.2110e-04 - val_mae: 0.0078 - val_mse: 2.2110e-04\n",
      "Epoch 80/500\n",
      "2440/2440 [==============================] - 1s 312us/step - loss: 1.5980e-04 - mae: 0.0081 - mse: 1.5980e-04 - val_loss: 2.2025e-04 - val_mae: 0.0077 - val_mse: 2.2025e-04\n",
      "Epoch 81/500\n",
      "2440/2440 [==============================] - 1s 303us/step - loss: 1.5891e-04 - mae: 0.0080 - mse: 1.5891e-04 - val_loss: 2.1947e-04 - val_mae: 0.0077 - val_mse: 2.1947e-04\n",
      "Epoch 82/500\n",
      "2440/2440 [==============================] - 1s 305us/step - loss: 1.5810e-04 - mae: 0.0080 - mse: 1.5810e-04 - val_loss: 2.1876e-04 - val_mae: 0.0077 - val_mse: 2.1876e-04\n",
      "Epoch 83/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.5738e-04 - mae: 0.0080 - mse: 1.5738e-04 - val_loss: 2.1812e-04 - val_mae: 0.0076 - val_mse: 2.1812e-04\n",
      "Epoch 84/500\n",
      "2440/2440 [==============================] - 1s 308us/step - loss: 1.5674e-04 - mae: 0.0080 - mse: 1.5674e-04 - val_loss: 2.1754e-04 - val_mae: 0.0076 - val_mse: 2.1754e-04\n",
      "Epoch 85/500\n",
      "2440/2440 [==============================] - 1s 325us/step - loss: 1.5618e-04 - mae: 0.0079 - mse: 1.5618e-04 - val_loss: 2.1703e-04 - val_mae: 0.0076 - val_mse: 2.1703e-04\n",
      "Epoch 86/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.5569e-04 - mae: 0.0079 - mse: 1.5569e-04 - val_loss: 2.1661e-04 - val_mae: 0.0076 - val_mse: 2.1661e-04\n",
      "Epoch 87/500\n",
      "2440/2440 [==============================] - 1s 312us/step - loss: 1.5525e-04 - mae: 0.0079 - mse: 1.5525e-04 - val_loss: 2.1633e-04 - val_mae: 0.0076 - val_mse: 2.1633e-04\n",
      "Epoch 88/500\n",
      "2440/2440 [==============================] - 1s 303us/step - loss: 1.5487e-04 - mae: 0.0079 - mse: 1.5487e-04 - val_loss: 2.1625e-04 - val_mae: 0.0076 - val_mse: 2.1625e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/500\n",
      "2440/2440 [==============================] - 1s 288us/step - loss: 1.5454e-04 - mae: 0.0079 - mse: 1.5454e-04 - val_loss: 2.1646e-04 - val_mae: 0.0077 - val_mse: 2.1646e-04\n",
      "Epoch 90/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.5427e-04 - mae: 0.0079 - mse: 1.5427e-04 - val_loss: 2.1707e-04 - val_mae: 0.0078 - val_mse: 2.1707e-04\n",
      "Epoch 91/500\n",
      "2440/2440 [==============================] - 1s 298us/step - loss: 1.5406e-04 - mae: 0.0079 - mse: 1.5406e-04 - val_loss: 2.1826e-04 - val_mae: 0.0080 - val_mse: 2.1826e-04\n",
      "Epoch 92/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 1.5391e-04 - mae: 0.0079 - mse: 1.5391e-04 - val_loss: 2.2018e-04 - val_mae: 0.0082 - val_mse: 2.2018e-04\n",
      "Epoch 93/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 1.5381e-04 - mae: 0.0079 - mse: 1.5381e-04 - val_loss: 2.2294e-04 - val_mae: 0.0085 - val_mse: 2.2294e-04\n",
      "Epoch 94/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 1.5365e-04 - mae: 0.0079 - mse: 1.5365e-04 - val_loss: 2.2631e-04 - val_mae: 0.0088 - val_mse: 2.2631e-04\n",
      "Epoch 95/500\n",
      "2440/2440 [==============================] - 1s 314us/step - loss: 1.5326e-04 - mae: 0.0079 - mse: 1.5326e-04 - val_loss: 2.2942e-04 - val_mae: 0.0090 - val_mse: 2.2942e-04\n",
      "Epoch 96/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.5247e-04 - mae: 0.0079 - mse: 1.5247e-04 - val_loss: 2.3094e-04 - val_mae: 0.0091 - val_mse: 2.3094e-04\n",
      "Epoch 97/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.5127e-04 - mae: 0.0079 - mse: 1.5127e-04 - val_loss: 2.3003e-04 - val_mae: 0.0091 - val_mse: 2.3003e-04\n",
      "Epoch 98/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.4994e-04 - mae: 0.0078 - mse: 1.4994e-04 - val_loss: 2.2723e-04 - val_mae: 0.0088 - val_mse: 2.2723e-04\n",
      "Epoch 99/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.4893e-04 - mae: 0.0077 - mse: 1.4893e-04 - val_loss: 2.2400e-04 - val_mae: 0.0086 - val_mse: 2.2400e-04\n",
      "Epoch 100/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.4852e-04 - mae: 0.0077 - mse: 1.4852e-04 - val_loss: 2.2170e-04 - val_mae: 0.0084 - val_mse: 2.2170e-04\n",
      "Epoch 101/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.4861e-04 - mae: 0.0077 - mse: 1.4861e-04 - val_loss: 2.2123e-04 - val_mae: 0.0084 - val_mse: 2.2123e-04\n",
      "Epoch 102/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.4899e-04 - mae: 0.0077 - mse: 1.4899e-04 - val_loss: 2.2329e-04 - val_mae: 0.0085 - val_mse: 2.2329e-04\n",
      "Epoch 103/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.4970e-04 - mae: 0.0078 - mse: 1.4970e-04 - val_loss: 2.2895e-04 - val_mae: 0.0090 - val_mse: 2.2895e-04\n",
      "Epoch 104/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 1.5103e-04 - mae: 0.0079 - mse: 1.5103e-04 - val_loss: 2.3937e-04 - val_mae: 0.0097 - val_mse: 2.3937e-04\n",
      "Epoch 105/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 1.5299e-04 - mae: 0.0080 - mse: 1.5299e-04 - val_loss: 2.5206e-04 - val_mae: 0.0103 - val_mse: 2.5206e-04\n",
      "Epoch 106/500\n",
      "2440/2440 [==============================] - 1s 318us/step - loss: 1.5455e-04 - mae: 0.0081 - mse: 1.5455e-04 - val_loss: 2.5130e-04 - val_mae: 0.0102 - val_mse: 2.5130e-04\n",
      "Epoch 107/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 1.5282e-04 - mae: 0.0081 - mse: 1.5282e-04 - val_loss: 2.3203e-04 - val_mae: 0.0090 - val_mse: 2.3203e-04\n",
      "Epoch 108/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 1.5112e-04 - mae: 0.0080 - mse: 1.5112e-04 - val_loss: 2.2016e-04 - val_mae: 0.0083 - val_mse: 2.2016e-04\n",
      "Epoch 109/500\n",
      "2440/2440 [==============================] - 1s 345us/step - loss: 1.6092e-04 - mae: 0.0084 - mse: 1.6092e-04 - val_loss: 2.2348e-04 - val_mae: 0.0086 - val_mse: 2.2348e-04\n",
      "Epoch 110/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 1.7011e-04 - mae: 0.0087 - mse: 1.7011e-04 - val_loss: 2.6212e-04 - val_mae: 0.0110 - val_mse: 2.6212e-04\n",
      "Epoch 111/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.7385e-04 - mae: 0.0090 - mse: 1.7385e-04 - val_loss: 2.7737e-04 - val_mae: 0.0118 - val_mse: 2.7737e-04\n",
      "Epoch 112/500\n",
      "2440/2440 [==============================] - 1s 302us/step - loss: 2.1172e-04 - mae: 0.0105 - mse: 2.1172e-04 - val_loss: 4.4148e-04 - val_mae: 0.0167 - val_mse: 4.4148e-04\n",
      "Epoch 113/500\n",
      "2440/2440 [==============================] - 1s 294us/step - loss: 2.2385e-04 - mae: 0.0106 - mse: 2.2385e-04 - val_loss: 2.5653e-04 - val_mae: 0.0104 - val_mse: 2.5653e-04\n",
      "Epoch 114/500\n",
      "2440/2440 [==============================] - 1s 298us/step - loss: 1.6948e-04 - mae: 0.0088 - mse: 1.6948e-04 - val_loss: 2.3546e-04 - val_mae: 0.0092 - val_mse: 2.3546e-04\n",
      "Epoch 115/500\n",
      "2440/2440 [==============================] - 1s 304us/step - loss: 1.6312e-04 - mae: 0.0085 - mse: 1.6312e-04 - val_loss: 2.2922e-04 - val_mae: 0.0086 - val_mse: 2.2922e-04\n",
      "Epoch 116/500\n",
      "2440/2440 [==============================] - 1s 311us/step - loss: 1.5827e-04 - mae: 0.0083 - mse: 1.5827e-04 - val_loss: 2.2640e-04 - val_mae: 0.0084 - val_mse: 2.2640e-04\n",
      "Epoch 117/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.5542e-04 - mae: 0.0081 - mse: 1.5542e-04 - val_loss: 2.2455e-04 - val_mae: 0.0083 - val_mse: 2.2455e-04\n",
      "Epoch 118/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.5333e-04 - mae: 0.0080 - mse: 1.5333e-04 - val_loss: 2.2310e-04 - val_mae: 0.0082 - val_mse: 2.2310e-04\n",
      "Epoch 119/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 1.5174e-04 - mae: 0.0079 - mse: 1.5174e-04 - val_loss: 2.2186e-04 - val_mae: 0.0081 - val_mse: 2.2186e-04\n",
      "Epoch 120/500\n",
      "2440/2440 [==============================] - 1s 295us/step - loss: 1.5050e-04 - mae: 0.0079 - mse: 1.5050e-04 - val_loss: 2.2071e-04 - val_mae: 0.0080 - val_mse: 2.2071e-04\n",
      "Epoch 121/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 1.4954e-04 - mae: 0.0078 - mse: 1.4954e-04 - val_loss: 2.1963e-04 - val_mae: 0.0080 - val_mse: 2.1963e-04\n",
      "Epoch 122/500\n",
      "2440/2440 [==============================] - 1s 292us/step - loss: 1.4879e-04 - mae: 0.0077 - mse: 1.4879e-04 - val_loss: 2.1860e-04 - val_mae: 0.0079 - val_mse: 2.1860e-04\n",
      "Epoch 123/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.4821e-04 - mae: 0.0077 - mse: 1.4821e-04 - val_loss: 2.1764e-04 - val_mae: 0.0079 - val_mse: 2.1764e-04\n",
      "Epoch 124/500\n",
      "2440/2440 [==============================] - 1s 318us/step - loss: 1.4779e-04 - mae: 0.0077 - mse: 1.4779e-04 - val_loss: 2.1677e-04 - val_mae: 0.0079 - val_mse: 2.1677e-04\n",
      "Epoch 125/500\n",
      "2440/2440 [==============================] - 1s 313us/step - loss: 1.4753e-04 - mae: 0.0077 - mse: 1.4753e-04 - val_loss: 2.1602e-04 - val_mae: 0.0079 - val_mse: 2.1602e-04\n",
      "Epoch 126/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 1.4742e-04 - mae: 0.0077 - mse: 1.4742e-04 - val_loss: 2.1541e-04 - val_mae: 0.0079 - val_mse: 2.1541e-04\n",
      "Epoch 127/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 1.4746e-04 - mae: 0.0077 - mse: 1.4746e-04 - val_loss: 2.1496e-04 - val_mae: 0.0079 - val_mse: 2.1496e-04\n",
      "Epoch 128/500\n",
      "2440/2440 [==============================] - 1s 297us/step - loss: 1.4760e-04 - mae: 0.0077 - mse: 1.4760e-04 - val_loss: 2.1468e-04 - val_mae: 0.0079 - val_mse: 2.1468e-04\n",
      "Epoch 129/500\n",
      "2440/2440 [==============================] - 1s 294us/step - loss: 1.4780e-04 - mae: 0.0077 - mse: 1.4780e-04 - val_loss: 2.1455e-04 - val_mae: 0.0079 - val_mse: 2.1455e-04\n",
      "Epoch 130/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 1.4801e-04 - mae: 0.0077 - mse: 1.4801e-04 - val_loss: 2.1453e-04 - val_mae: 0.0080 - val_mse: 2.1453e-04\n",
      "Epoch 131/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 1.4815e-04 - mae: 0.0077 - mse: 1.4815e-04 - val_loss: 2.1453e-04 - val_mae: 0.0080 - val_mse: 2.1453e-04\n",
      "Epoch 132/500\n",
      "2440/2440 [==============================] - 1s 288us/step - loss: 1.4816e-04 - mae: 0.0078 - mse: 1.4816e-04 - val_loss: 2.1451e-04 - val_mae: 0.0080 - val_mse: 2.1451e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.4803e-04 - mae: 0.0078 - mse: 1.4803e-04 - val_loss: 2.1443e-04 - val_mae: 0.0080 - val_mse: 2.1443e-04\n",
      "Epoch 134/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.4779e-04 - mae: 0.0078 - mse: 1.4779e-04 - val_loss: 2.1429e-04 - val_mae: 0.0080 - val_mse: 2.1429e-04\n",
      "Epoch 135/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 1.4749e-04 - mae: 0.0077 - mse: 1.4749e-04 - val_loss: 2.1412e-04 - val_mae: 0.0080 - val_mse: 2.1412e-04\n",
      "Epoch 136/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.4719e-04 - mae: 0.0077 - mse: 1.4719e-04 - val_loss: 2.1397e-04 - val_mae: 0.0080 - val_mse: 2.1397e-04\n",
      "Epoch 137/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.4691e-04 - mae: 0.0077 - mse: 1.4691e-04 - val_loss: 2.1389e-04 - val_mae: 0.0080 - val_mse: 2.1389e-04\n",
      "Epoch 138/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.4668e-04 - mae: 0.0077 - mse: 1.4668e-04 - val_loss: 2.1390e-04 - val_mae: 0.0081 - val_mse: 2.1390e-04\n",
      "Epoch 139/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.4650e-04 - mae: 0.0077 - mse: 1.4650e-04 - val_loss: 2.1404e-04 - val_mae: 0.0081 - val_mse: 2.1404e-04\n",
      "Epoch 140/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.4637e-04 - mae: 0.0077 - mse: 1.4637e-04 - val_loss: 2.1433e-04 - val_mae: 0.0081 - val_mse: 2.1433e-04\n",
      "Epoch 141/500\n",
      "2440/2440 [==============================] - 1s 313us/step - loss: 1.4628e-04 - mae: 0.0077 - mse: 1.4628e-04 - val_loss: 2.1480e-04 - val_mae: 0.0082 - val_mse: 2.1480e-04\n",
      "Epoch 142/500\n",
      "2440/2440 [==============================] - 1s 309us/step - loss: 1.4623e-04 - mae: 0.0078 - mse: 1.4623e-04 - val_loss: 2.1551e-04 - val_mae: 0.0083 - val_mse: 2.1551e-04\n",
      "Epoch 143/500\n",
      "2440/2440 [==============================] - 1s 312us/step - loss: 1.4624e-04 - mae: 0.0078 - mse: 1.4624e-04 - val_loss: 2.1652e-04 - val_mae: 0.0083 - val_mse: 2.1652e-04\n",
      "Epoch 144/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 1.4633e-04 - mae: 0.0078 - mse: 1.4633e-04 - val_loss: 2.1791e-04 - val_mae: 0.0085 - val_mse: 2.1791e-04\n",
      "Epoch 145/500\n",
      "2440/2440 [==============================] - 1s 298us/step - loss: 1.4651e-04 - mae: 0.0078 - mse: 1.4651e-04 - val_loss: 2.1981e-04 - val_mae: 0.0086 - val_mse: 2.1981e-04\n",
      "Epoch 146/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.4684e-04 - mae: 0.0079 - mse: 1.4684e-04 - val_loss: 2.2235e-04 - val_mae: 0.0088 - val_mse: 2.2235e-04\n",
      "Epoch 147/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.4735e-04 - mae: 0.0079 - mse: 1.4735e-04 - val_loss: 2.2563e-04 - val_mae: 0.0091 - val_mse: 2.2563e-04\n",
      "Epoch 148/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.4813e-04 - mae: 0.0080 - mse: 1.4813e-04 - val_loss: 2.2965e-04 - val_mae: 0.0093 - val_mse: 2.2965e-04\n",
      "Epoch 149/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.4921e-04 - mae: 0.0080 - mse: 1.4921e-04 - val_loss: 2.3423e-04 - val_mae: 0.0096 - val_mse: 2.3423e-04\n",
      "Epoch 150/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.5062e-04 - mae: 0.0081 - mse: 1.5062e-04 - val_loss: 2.3900e-04 - val_mae: 0.0098 - val_mse: 2.3900e-04\n",
      "Epoch 151/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.5233e-04 - mae: 0.0082 - mse: 1.5233e-04 - val_loss: 2.4366e-04 - val_mae: 0.0101 - val_mse: 2.4366e-04\n",
      "Epoch 152/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.5425e-04 - mae: 0.0083 - mse: 1.5425e-04 - val_loss: 2.4843e-04 - val_mae: 0.0103 - val_mse: 2.4843e-04\n",
      "Epoch 153/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.5635e-04 - mae: 0.0084 - mse: 1.5635e-04 - val_loss: 2.5432e-04 - val_mae: 0.0106 - val_mse: 2.5432e-04\n",
      "Epoch 154/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.5876e-04 - mae: 0.0085 - mse: 1.5876e-04 - val_loss: 2.6116e-04 - val_mae: 0.0108 - val_mse: 2.6116e-04\n",
      "Epoch 155/500\n",
      "2440/2440 [==============================] - 1s 305us/step - loss: 1.6154e-04 - mae: 0.0087 - mse: 1.6154e-04 - val_loss: 2.6328e-04 - val_mae: 0.0108 - val_mse: 2.6328e-04\n",
      "Epoch 156/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 1.6487e-04 - mae: 0.0089 - mse: 1.6487e-04 - val_loss: 2.6285e-04 - val_mae: 0.0107 - val_mse: 2.6285e-04\n",
      "Epoch 157/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.7068e-04 - mae: 0.0092 - mse: 1.7068e-04 - val_loss: 2.6550e-04 - val_mae: 0.0107 - val_mse: 2.6550e-04\n",
      "Epoch 158/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.7975e-04 - mae: 0.0096 - mse: 1.7975e-04 - val_loss: 2.5128e-04 - val_mae: 0.0098 - val_mse: 2.5128e-04\n",
      "Epoch 159/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.8368e-04 - mae: 0.0097 - mse: 1.8368e-04 - val_loss: 2.2336e-04 - val_mae: 0.0081 - val_mse: 2.2336e-04\n",
      "Epoch 160/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.7027e-04 - mae: 0.0091 - mse: 1.7027e-04 - val_loss: 2.1904e-04 - val_mae: 0.0085 - val_mse: 2.1904e-04\n",
      "Epoch 161/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.4847e-04 - mae: 0.0080 - mse: 1.4847e-04 - val_loss: 2.1095e-04 - val_mae: 0.0081 - val_mse: 2.1095e-04\n",
      "Epoch 162/500\n",
      "2440/2440 [==============================] - 1s 288us/step - loss: 1.3712e-04 - mae: 0.0074 - mse: 1.3712e-04 - val_loss: 2.1043e-04 - val_mae: 0.0081 - val_mse: 2.1043e-04\n",
      "Epoch 163/500\n",
      "2440/2440 [==============================] - 1s 315us/step - loss: 1.3482e-04 - mae: 0.0073 - mse: 1.3482e-04 - val_loss: 2.1020e-04 - val_mae: 0.0080 - val_mse: 2.1020e-04\n",
      "Epoch 164/500\n",
      "2440/2440 [==============================] - 1s 288us/step - loss: 1.3504e-04 - mae: 0.0073 - mse: 1.3504e-04 - val_loss: 2.0961e-04 - val_mae: 0.0078 - val_mse: 2.0961e-04\n",
      "Epoch 165/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 1.3578e-04 - mae: 0.0074 - mse: 1.3578e-04 - val_loss: 2.0903e-04 - val_mae: 0.0077 - val_mse: 2.0903e-04\n",
      "Epoch 166/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 1.3645e-04 - mae: 0.0075 - mse: 1.3645e-04 - val_loss: 2.0871e-04 - val_mae: 0.0076 - val_mse: 2.0871e-04\n",
      "Epoch 167/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.3696e-04 - mae: 0.0075 - mse: 1.3696e-04 - val_loss: 2.0871e-04 - val_mae: 0.0076 - val_mse: 2.0871e-04\n",
      "Epoch 168/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.3732e-04 - mae: 0.0075 - mse: 1.3732e-04 - val_loss: 2.0896e-04 - val_mae: 0.0076 - val_mse: 2.0896e-04\n",
      "Epoch 169/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.3757e-04 - mae: 0.0076 - mse: 1.3757e-04 - val_loss: 2.0935e-04 - val_mae: 0.0076 - val_mse: 2.0935e-04\n",
      "Epoch 170/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.3770e-04 - mae: 0.0076 - mse: 1.3770e-04 - val_loss: 2.0982e-04 - val_mae: 0.0076 - val_mse: 2.0982e-04\n",
      "Epoch 171/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.3776e-04 - mae: 0.0076 - mse: 1.3776e-04 - val_loss: 2.1034e-04 - val_mae: 0.0076 - val_mse: 2.1034e-04\n",
      "Epoch 172/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.3776e-04 - mae: 0.0076 - mse: 1.3776e-04 - val_loss: 2.1089e-04 - val_mae: 0.0077 - val_mse: 2.1089e-04\n",
      "Epoch 173/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.3772e-04 - mae: 0.0076 - mse: 1.3772e-04 - val_loss: 2.1145e-04 - val_mae: 0.0077 - val_mse: 2.1145e-04\n",
      "Epoch 174/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.3767e-04 - mae: 0.0076 - mse: 1.3767e-04 - val_loss: 2.1205e-04 - val_mae: 0.0077 - val_mse: 2.1205e-04\n",
      "Epoch 175/500\n",
      "2440/2440 [==============================] - 1s 303us/step - loss: 1.3762e-04 - mae: 0.0076 - mse: 1.3762e-04 - val_loss: 2.1266e-04 - val_mae: 0.0078 - val_mse: 2.1266e-04\n",
      "Epoch 176/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.3757e-04 - mae: 0.0076 - mse: 1.3757e-04 - val_loss: 2.1330e-04 - val_mae: 0.0078 - val_mse: 2.1330e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.3755e-04 - mae: 0.0076 - mse: 1.3755e-04 - val_loss: 2.1394e-04 - val_mae: 0.0079 - val_mse: 2.1394e-04\n",
      "Epoch 178/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.3754e-04 - mae: 0.0077 - mse: 1.3754e-04 - val_loss: 2.1456e-04 - val_mae: 0.0079 - val_mse: 2.1456e-04\n",
      "Epoch 179/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 1.3756e-04 - mae: 0.0077 - mse: 1.3756e-04 - val_loss: 2.1515e-04 - val_mae: 0.0079 - val_mse: 2.1515e-04\n",
      "Epoch 180/500\n",
      "2440/2440 [==============================] - 1s 270us/step - loss: 1.3761e-04 - mae: 0.0077 - mse: 1.3761e-04 - val_loss: 2.1567e-04 - val_mae: 0.0079 - val_mse: 2.1567e-04\n",
      "Epoch 181/500\n",
      "2440/2440 [==============================] - 1s 274us/step - loss: 1.3771e-04 - mae: 0.0077 - mse: 1.3771e-04 - val_loss: 2.1607e-04 - val_mae: 0.0079 - val_mse: 2.1607e-04\n",
      "Epoch 182/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.3785e-04 - mae: 0.0077 - mse: 1.3785e-04 - val_loss: 2.1629e-04 - val_mae: 0.0080 - val_mse: 2.1629e-04\n",
      "Epoch 183/500\n",
      "2440/2440 [==============================] - 1s 271us/step - loss: 1.3806e-04 - mae: 0.0078 - mse: 1.3806e-04 - val_loss: 2.1629e-04 - val_mae: 0.0079 - val_mse: 2.1629e-04\n",
      "Epoch 184/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 1.3835e-04 - mae: 0.0078 - mse: 1.3835e-04 - val_loss: 2.1602e-04 - val_mae: 0.0079 - val_mse: 2.1602e-04\n",
      "Epoch 185/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.3870e-04 - mae: 0.0078 - mse: 1.3870e-04 - val_loss: 2.1545e-04 - val_mae: 0.0079 - val_mse: 2.1545e-04\n",
      "Epoch 186/500\n",
      "2440/2440 [==============================] - 1s 272us/step - loss: 1.3908e-04 - mae: 0.0078 - mse: 1.3908e-04 - val_loss: 2.1456e-04 - val_mae: 0.0078 - val_mse: 2.1456e-04\n",
      "Epoch 187/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 1.3936e-04 - mae: 0.0078 - mse: 1.3936e-04 - val_loss: 2.1338e-04 - val_mae: 0.0078 - val_mse: 2.1338e-04\n",
      "Epoch 188/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.3934e-04 - mae: 0.0078 - mse: 1.3934e-04 - val_loss: 2.1196e-04 - val_mae: 0.0077 - val_mse: 2.1196e-04\n",
      "Epoch 189/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.3881e-04 - mae: 0.0078 - mse: 1.3881e-04 - val_loss: 2.1050e-04 - val_mae: 0.0077 - val_mse: 2.1050e-04\n",
      "Epoch 190/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.3768e-04 - mae: 0.0077 - mse: 1.3768e-04 - val_loss: 2.0926e-04 - val_mae: 0.0077 - val_mse: 2.0926e-04\n",
      "Epoch 191/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 1.3620e-04 - mae: 0.0076 - mse: 1.3620e-04 - val_loss: 2.0842e-04 - val_mae: 0.0077 - val_mse: 2.0842e-04\n",
      "Epoch 192/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.3475e-04 - mae: 0.0074 - mse: 1.3475e-04 - val_loss: 2.0797e-04 - val_mae: 0.0078 - val_mse: 2.0797e-04\n",
      "Epoch 193/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.3357e-04 - mae: 0.0074 - mse: 1.3357e-04 - val_loss: 2.0777e-04 - val_mae: 0.0078 - val_mse: 2.0777e-04\n",
      "Epoch 194/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 1.3263e-04 - mae: 0.0073 - mse: 1.3263e-04 - val_loss: 2.0771e-04 - val_mae: 0.0078 - val_mse: 2.0771e-04\n",
      "Epoch 195/500\n",
      "2440/2440 [==============================] - 1s 314us/step - loss: 1.3188e-04 - mae: 0.0073 - mse: 1.3188e-04 - val_loss: 2.0773e-04 - val_mae: 0.0078 - val_mse: 2.0773e-04\n",
      "Epoch 196/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.3120e-04 - mae: 0.0072 - mse: 1.3120e-04 - val_loss: 2.0779e-04 - val_mae: 0.0078 - val_mse: 2.0779e-04\n",
      "Epoch 197/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.3057e-04 - mae: 0.0072 - mse: 1.3057e-04 - val_loss: 2.0786e-04 - val_mae: 0.0077 - val_mse: 2.0786e-04\n",
      "Epoch 198/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.2999e-04 - mae: 0.0072 - mse: 1.2999e-04 - val_loss: 2.0795e-04 - val_mae: 0.0077 - val_mse: 2.0795e-04\n",
      "Epoch 199/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 1.2948e-04 - mae: 0.0072 - mse: 1.2948e-04 - val_loss: 2.0810e-04 - val_mae: 0.0077 - val_mse: 2.0810e-04\n",
      "Epoch 200/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.2903e-04 - mae: 0.0072 - mse: 1.2903e-04 - val_loss: 2.0830e-04 - val_mae: 0.0077 - val_mse: 2.0830e-04\n",
      "Epoch 201/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.2864e-04 - mae: 0.0072 - mse: 1.2864e-04 - val_loss: 2.0859e-04 - val_mae: 0.0077 - val_mse: 2.0859e-04\n",
      "Epoch 202/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.2830e-04 - mae: 0.0072 - mse: 1.2830e-04 - val_loss: 2.0896e-04 - val_mae: 0.0077 - val_mse: 2.0896e-04\n",
      "Epoch 203/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.2801e-04 - mae: 0.0072 - mse: 1.2801e-04 - val_loss: 2.0941e-04 - val_mae: 0.0077 - val_mse: 2.0941e-04\n",
      "Epoch 204/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.2778e-04 - mae: 0.0072 - mse: 1.2778e-04 - val_loss: 2.0994e-04 - val_mae: 0.0077 - val_mse: 2.0994e-04\n",
      "Epoch 205/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 1.2760e-04 - mae: 0.0072 - mse: 1.2760e-04 - val_loss: 2.1055e-04 - val_mae: 0.0077 - val_mse: 2.1055e-04\n",
      "Epoch 206/500\n",
      "2440/2440 [==============================] - 1s 311us/step - loss: 1.2748e-04 - mae: 0.0072 - mse: 1.2748e-04 - val_loss: 2.1122e-04 - val_mae: 0.0077 - val_mse: 2.1122e-04\n",
      "Epoch 207/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.2741e-04 - mae: 0.0072 - mse: 1.2741e-04 - val_loss: 2.1196e-04 - val_mae: 0.0077 - val_mse: 2.1196e-04\n",
      "Epoch 208/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.2739e-04 - mae: 0.0072 - mse: 1.2739e-04 - val_loss: 2.1275e-04 - val_mae: 0.0077 - val_mse: 2.1275e-04\n",
      "Epoch 209/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.2741e-04 - mae: 0.0072 - mse: 1.2741e-04 - val_loss: 2.1359e-04 - val_mae: 0.0078 - val_mse: 2.1359e-04\n",
      "Epoch 210/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.2749e-04 - mae: 0.0072 - mse: 1.2749e-04 - val_loss: 2.1446e-04 - val_mae: 0.0078 - val_mse: 2.1446e-04\n",
      "Epoch 211/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.2761e-04 - mae: 0.0072 - mse: 1.2761e-04 - val_loss: 2.1535e-04 - val_mae: 0.0078 - val_mse: 2.1535e-04\n",
      "Epoch 212/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.2781e-04 - mae: 0.0072 - mse: 1.2781e-04 - val_loss: 2.1622e-04 - val_mae: 0.0079 - val_mse: 2.1622e-04\n",
      "Epoch 213/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.2810e-04 - mae: 0.0073 - mse: 1.2810e-04 - val_loss: 2.1702e-04 - val_mae: 0.0079 - val_mse: 2.1702e-04\n",
      "Epoch 214/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.2851e-04 - mae: 0.0073 - mse: 1.2851e-04 - val_loss: 2.1766e-04 - val_mae: 0.0079 - val_mse: 2.1766e-04\n",
      "Epoch 215/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.2907e-04 - mae: 0.0073 - mse: 1.2907e-04 - val_loss: 2.1802e-04 - val_mae: 0.0079 - val_mse: 2.1802e-04\n",
      "Epoch 216/500\n",
      "2440/2440 [==============================] - 1s 301us/step - loss: 1.2975e-04 - mae: 0.0074 - mse: 1.2975e-04 - val_loss: 2.1797e-04 - val_mae: 0.0079 - val_mse: 2.1797e-04\n",
      "Epoch 217/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.3049e-04 - mae: 0.0074 - mse: 1.3049e-04 - val_loss: 2.1736e-04 - val_mae: 0.0079 - val_mse: 2.1736e-04\n",
      "Epoch 218/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.3108e-04 - mae: 0.0075 - mse: 1.3108e-04 - val_loss: 2.1615e-04 - val_mae: 0.0078 - val_mse: 2.1615e-04\n",
      "Epoch 219/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.3123e-04 - mae: 0.0075 - mse: 1.3123e-04 - val_loss: 2.1448e-04 - val_mae: 0.0077 - val_mse: 2.1448e-04\n",
      "Epoch 220/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.3067e-04 - mae: 0.0074 - mse: 1.3067e-04 - val_loss: 2.1275e-04 - val_mae: 0.0077 - val_mse: 2.1275e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 1.2938e-04 - mae: 0.0073 - mse: 1.2938e-04 - val_loss: 2.1146e-04 - val_mae: 0.0076 - val_mse: 2.1146e-04\n",
      "Epoch 222/500\n",
      "2440/2440 [==============================] - 1s 270us/step - loss: 1.2762e-04 - mae: 0.0072 - mse: 1.2762e-04 - val_loss: 2.1086e-04 - val_mae: 0.0076 - val_mse: 2.1086e-04\n",
      "Epoch 223/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.2577e-04 - mae: 0.0071 - mse: 1.2577e-04 - val_loss: 2.1093e-04 - val_mae: 0.0077 - val_mse: 2.1093e-04\n",
      "Epoch 224/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.2404e-04 - mae: 0.0070 - mse: 1.2404e-04 - val_loss: 2.1153e-04 - val_mae: 0.0077 - val_mse: 2.1153e-04\n",
      "Epoch 225/500\n",
      "2440/2440 [==============================] - 1s 322us/step - loss: 1.2252e-04 - mae: 0.0069 - mse: 1.2252e-04 - val_loss: 2.1250e-04 - val_mae: 0.0077 - val_mse: 2.1250e-04\n",
      "Epoch 226/500\n",
      "2440/2440 [==============================] - 1s 305us/step - loss: 1.2122e-04 - mae: 0.0068 - mse: 1.2122e-04 - val_loss: 2.1374e-04 - val_mae: 0.0078 - val_mse: 2.1374e-04\n",
      "Epoch 227/500\n",
      "2440/2440 [==============================] - 1s 328us/step - loss: 1.2020e-04 - mae: 0.0067 - mse: 1.2020e-04 - val_loss: 2.1516e-04 - val_mae: 0.0078 - val_mse: 2.1516e-04\n",
      "Epoch 228/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.1944e-04 - mae: 0.0067 - mse: 1.1944e-04 - val_loss: 2.1668e-04 - val_mae: 0.0078 - val_mse: 2.1668e-04\n",
      "Epoch 229/500\n",
      "2440/2440 [==============================] - 1s 365us/step - loss: 1.1893e-04 - mae: 0.0066 - mse: 1.1893e-04 - val_loss: 2.1818e-04 - val_mae: 0.0079 - val_mse: 2.1818e-04\n",
      "Epoch 230/500\n",
      "2440/2440 [==============================] - 1s 305us/step - loss: 1.1863e-04 - mae: 0.0066 - mse: 1.1863e-04 - val_loss: 2.1959e-04 - val_mae: 0.0079 - val_mse: 2.1959e-04\n",
      "Epoch 231/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 1.1848e-04 - mae: 0.0066 - mse: 1.1848e-04 - val_loss: 2.2087e-04 - val_mae: 0.0079 - val_mse: 2.2087e-04\n",
      "Epoch 232/500\n",
      "2440/2440 [==============================] - 1s 343us/step - loss: 1.1845e-04 - mae: 0.0066 - mse: 1.1845e-04 - val_loss: 2.2204e-04 - val_mae: 0.0080 - val_mse: 2.2204e-04\n",
      "Epoch 233/500\n",
      "2440/2440 [==============================] - 1s 304us/step - loss: 1.1852e-04 - mae: 0.0066 - mse: 1.1852e-04 - val_loss: 2.2315e-04 - val_mae: 0.0080 - val_mse: 2.2315e-04\n",
      "Epoch 234/500\n",
      "2440/2440 [==============================] - 1s 297us/step - loss: 1.1865e-04 - mae: 0.0066 - mse: 1.1865e-04 - val_loss: 2.2429e-04 - val_mae: 0.0081 - val_mse: 2.2429e-04\n",
      "Epoch 235/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 1.1882e-04 - mae: 0.0066 - mse: 1.1882e-04 - val_loss: 2.2550e-04 - val_mae: 0.0082 - val_mse: 2.2550e-04\n",
      "Epoch 236/500\n",
      "2440/2440 [==============================] - 1s 304us/step - loss: 1.1900e-04 - mae: 0.0067 - mse: 1.1900e-04 - val_loss: 2.2688e-04 - val_mae: 0.0083 - val_mse: 2.2688e-04\n",
      "Epoch 237/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.1916e-04 - mae: 0.0067 - mse: 1.1916e-04 - val_loss: 2.2849e-04 - val_mae: 0.0085 - val_mse: 2.2849e-04\n",
      "Epoch 238/500\n",
      "2440/2440 [==============================] - 1s 308us/step - loss: 1.1930e-04 - mae: 0.0067 - mse: 1.1930e-04 - val_loss: 2.3037e-04 - val_mae: 0.0086 - val_mse: 2.3037e-04\n",
      "Epoch 239/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.1940e-04 - mae: 0.0067 - mse: 1.1940e-04 - val_loss: 2.3253e-04 - val_mae: 0.0088 - val_mse: 2.3253e-04\n",
      "Epoch 240/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.1948e-04 - mae: 0.0067 - mse: 1.1948e-04 - val_loss: 2.3494e-04 - val_mae: 0.0089 - val_mse: 2.3494e-04\n",
      "Epoch 241/500\n",
      "2440/2440 [==============================] - 1s 288us/step - loss: 1.1958e-04 - mae: 0.0068 - mse: 1.1958e-04 - val_loss: 2.3743e-04 - val_mae: 0.0090 - val_mse: 2.3743e-04\n",
      "Epoch 242/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 1.1973e-04 - mae: 0.0068 - mse: 1.1973e-04 - val_loss: 2.3982e-04 - val_mae: 0.0091 - val_mse: 2.3982e-04\n",
      "Epoch 243/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.1998e-04 - mae: 0.0069 - mse: 1.1998e-04 - val_loss: 2.4175e-04 - val_mae: 0.0091 - val_mse: 2.4175e-04\n",
      "Epoch 244/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.2038e-04 - mae: 0.0069 - mse: 1.2038e-04 - val_loss: 2.4268e-04 - val_mae: 0.0091 - val_mse: 2.4268e-04\n",
      "Epoch 245/500\n",
      "2440/2440 [==============================] - 1s 288us/step - loss: 1.2099e-04 - mae: 0.0070 - mse: 1.2098e-04 - val_loss: 2.4194e-04 - val_mae: 0.0090 - val_mse: 2.4194e-04\n",
      "Epoch 246/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.2182e-04 - mae: 0.0071 - mse: 1.2182e-04 - val_loss: 2.3923e-04 - val_mae: 0.0087 - val_mse: 2.3923e-04\n",
      "Epoch 247/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.2293e-04 - mae: 0.0071 - mse: 1.2293e-04 - val_loss: 2.3491e-04 - val_mae: 0.0085 - val_mse: 2.3491e-04\n",
      "Epoch 248/500\n",
      "2440/2440 [==============================] - 1s 318us/step - loss: 1.2424e-04 - mae: 0.0072 - mse: 1.2424e-04 - val_loss: 2.2962e-04 - val_mae: 0.0082 - val_mse: 2.2962e-04\n",
      "Epoch 249/500\n",
      "2440/2440 [==============================] - 1s 318us/step - loss: 1.2527e-04 - mae: 0.0072 - mse: 1.2527e-04 - val_loss: 2.2389e-04 - val_mae: 0.0080 - val_mse: 2.2389e-04\n",
      "Epoch 250/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.2522e-04 - mae: 0.0072 - mse: 1.2522e-04 - val_loss: 2.1852e-04 - val_mae: 0.0079 - val_mse: 2.1852e-04\n",
      "Epoch 251/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.2369e-04 - mae: 0.0071 - mse: 1.2369e-04 - val_loss: 2.1520e-04 - val_mae: 0.0078 - val_mse: 2.1520e-04\n",
      "Epoch 252/500\n",
      "2440/2440 [==============================] - 1s 295us/step - loss: 1.2125e-04 - mae: 0.0069 - mse: 1.2125e-04 - val_loss: 2.1514e-04 - val_mae: 0.0078 - val_mse: 2.1514e-04\n",
      "Epoch 253/500\n",
      "2440/2440 [==============================] - ETA: 0s - loss: 1.1701e-04 - mae: 0.0067 - mse: 1.1701e-0 - 1s 285us/step - loss: 1.1915e-04 - mae: 0.0068 - mse: 1.1915e-04 - val_loss: 2.1720e-04 - val_mae: 0.0079 - val_mse: 2.1720e-04\n",
      "Epoch 254/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 1.1815e-04 - mae: 0.0068 - mse: 1.1815e-04 - val_loss: 2.1943e-04 - val_mae: 0.0080 - val_mse: 2.1943e-04\n",
      "Epoch 255/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.1784e-04 - mae: 0.0068 - mse: 1.1784e-04 - val_loss: 2.2181e-04 - val_mae: 0.0081 - val_mse: 2.2181e-04\n",
      "Epoch 256/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 1.1801e-04 - mae: 0.0068 - mse: 1.1801e-04 - val_loss: 2.2471e-04 - val_mae: 0.0081 - val_mse: 2.2471e-04\n",
      "Epoch 257/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 1.1891e-04 - mae: 0.0069 - mse: 1.1891e-04 - val_loss: 2.2728e-04 - val_mae: 0.0082 - val_mse: 2.2728e-04\n",
      "Epoch 258/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 1.2049e-04 - mae: 0.0070 - mse: 1.2049e-04 - val_loss: 2.2848e-04 - val_mae: 0.0082 - val_mse: 2.2848e-04\n",
      "Epoch 259/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 1.2232e-04 - mae: 0.0071 - mse: 1.2232e-04 - val_loss: 2.2798e-04 - val_mae: 0.0082 - val_mse: 2.2798e-04\n",
      "Epoch 260/500\n",
      "2440/2440 [==============================] - 1s 315us/step - loss: 1.2386e-04 - mae: 0.0072 - mse: 1.2386e-04 - val_loss: 2.2654e-04 - val_mae: 0.0081 - val_mse: 2.2654e-04\n",
      "Epoch 261/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 1.2468e-04 - mae: 0.0072 - mse: 1.2468e-04 - val_loss: 2.2555e-04 - val_mae: 0.0079 - val_mse: 2.2555e-04\n",
      "Epoch 262/500\n",
      "2440/2440 [==============================] - 1s 295us/step - loss: 1.2495e-04 - mae: 0.0072 - mse: 1.2495e-04 - val_loss: 2.2549e-04 - val_mae: 0.0079 - val_mse: 2.2549e-04\n",
      "Epoch 263/500\n",
      "2440/2440 [==============================] - 1s 320us/step - loss: 1.2511e-04 - mae: 0.0072 - mse: 1.2511e-04 - val_loss: 2.2575e-04 - val_mae: 0.0078 - val_mse: 2.2575e-04\n",
      "Epoch 264/500\n",
      "2440/2440 [==============================] - 1s 336us/step - loss: 1.2518e-04 - mae: 0.0072 - mse: 1.2518e-04 - val_loss: 2.2589e-04 - val_mae: 0.0078 - val_mse: 2.2589e-04\n",
      "Epoch 265/500\n",
      "2440/2440 [==============================] - 1s 292us/step - loss: 1.2499e-04 - mae: 0.0072 - mse: 1.2499e-04 - val_loss: 2.2533e-04 - val_mae: 0.0078 - val_mse: 2.2533e-04\n",
      "Epoch 266/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.2428e-04 - mae: 0.0072 - mse: 1.2428e-04 - val_loss: 2.2415e-04 - val_mae: 0.0078 - val_mse: 2.2415e-04\n",
      "Epoch 267/500\n",
      "2440/2440 [==============================] - ETA: 0s - loss: 1.2250e-04 - mae: 0.0070 - mse: 1.2250e-0 - 1s 324us/step - loss: 1.2302e-04 - mae: 0.0071 - mse: 1.2302e-04 - val_loss: 2.2300e-04 - val_mae: 0.0078 - val_mse: 2.2300e-04\n",
      "Epoch 268/500\n",
      "2440/2440 [==============================] - 1s 303us/step - loss: 1.2155e-04 - mae: 0.0070 - mse: 1.2155e-04 - val_loss: 2.2238e-04 - val_mae: 0.0078 - val_mse: 2.2238e-04\n",
      "Epoch 269/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.2007e-04 - mae: 0.0069 - mse: 1.2007e-04 - val_loss: 2.2239e-04 - val_mae: 0.0078 - val_mse: 2.2239e-04\n",
      "Epoch 270/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.1866e-04 - mae: 0.0068 - mse: 1.1866e-04 - val_loss: 2.2288e-04 - val_mae: 0.0079 - val_mse: 2.2288e-04\n",
      "Epoch 271/500\n",
      "2440/2440 [==============================] - 1s 297us/step - loss: 1.1736e-04 - mae: 0.0067 - mse: 1.1736e-04 - val_loss: 2.2370e-04 - val_mae: 0.0079 - val_mse: 2.2370e-04\n",
      "Epoch 272/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.1622e-04 - mae: 0.0066 - mse: 1.1622e-04 - val_loss: 2.2470e-04 - val_mae: 0.0080 - val_mse: 2.2470e-04\n",
      "Epoch 273/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 1.1527e-04 - mae: 0.0066 - mse: 1.1527e-04 - val_loss: 2.2576e-04 - val_mae: 0.0081 - val_mse: 2.2576e-04\n",
      "Epoch 274/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.1459e-04 - mae: 0.0066 - mse: 1.1459e-04 - val_loss: 2.2674e-04 - val_mae: 0.0081 - val_mse: 2.2674e-04\n",
      "Epoch 275/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 1.1428e-04 - mae: 0.0066 - mse: 1.1428e-04 - val_loss: 2.2758e-04 - val_mae: 0.0081 - val_mse: 2.2758e-04\n",
      "Epoch 276/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 1.1435e-04 - mae: 0.0066 - mse: 1.1435e-04 - val_loss: 2.2827e-04 - val_mae: 0.0082 - val_mse: 2.2827e-04\n",
      "Epoch 277/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.1468e-04 - mae: 0.0066 - mse: 1.1468e-04 - val_loss: 2.2879e-04 - val_mae: 0.0082 - val_mse: 2.2879e-04\n",
      "Epoch 278/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 1.1505e-04 - mae: 0.0067 - mse: 1.1505e-04 - val_loss: 2.2903e-04 - val_mae: 0.0082 - val_mse: 2.2903e-04\n",
      "Epoch 279/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.1535e-04 - mae: 0.0067 - mse: 1.1535e-04 - val_loss: 2.2887e-04 - val_mae: 0.0081 - val_mse: 2.2887e-04\n",
      "Epoch 280/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 1.1552e-04 - mae: 0.0067 - mse: 1.1552e-04 - val_loss: 2.2849e-04 - val_mae: 0.0080 - val_mse: 2.2849e-04\n",
      "Epoch 281/500\n",
      "2440/2440 [==============================] - 1s 308us/step - loss: 1.1545e-04 - mae: 0.0068 - mse: 1.1545e-04 - val_loss: 2.2823e-04 - val_mae: 0.0079 - val_mse: 2.2823e-04\n",
      "Epoch 282/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.1507e-04 - mae: 0.0068 - mse: 1.1507e-04 - val_loss: 2.2839e-04 - val_mae: 0.0079 - val_mse: 2.2839e-04\n",
      "Epoch 283/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.1433e-04 - mae: 0.0067 - mse: 1.1433e-04 - val_loss: 2.2909e-04 - val_mae: 0.0078 - val_mse: 2.2909e-04\n",
      "Epoch 284/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.1337e-04 - mae: 0.0067 - mse: 1.1337e-04 - val_loss: 2.3030e-04 - val_mae: 0.0078 - val_mse: 2.3030e-04\n",
      "Epoch 285/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.1234e-04 - mae: 0.0066 - mse: 1.1234e-04 - val_loss: 2.3184e-04 - val_mae: 0.0079 - val_mse: 2.3184e-04\n",
      "Epoch 286/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.1141e-04 - mae: 0.0066 - mse: 1.1141e-04 - val_loss: 2.3350e-04 - val_mae: 0.0079 - val_mse: 2.3350e-04\n",
      "Epoch 287/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.1063e-04 - mae: 0.0065 - mse: 1.1063e-04 - val_loss: 2.3515e-04 - val_mae: 0.0080 - val_mse: 2.3515e-04\n",
      "Epoch 288/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 1.0999e-04 - mae: 0.0065 - mse: 1.0999e-04 - val_loss: 2.3672e-04 - val_mae: 0.0080 - val_mse: 2.3672e-04\n",
      "Epoch 289/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.0943e-04 - mae: 0.0065 - mse: 1.0943e-04 - val_loss: 2.3822e-04 - val_mae: 0.0081 - val_mse: 2.3822e-04\n",
      "Epoch 290/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0892e-04 - mae: 0.0065 - mse: 1.0892e-04 - val_loss: 2.3972e-04 - val_mae: 0.0082 - val_mse: 2.3972e-04\n",
      "Epoch 291/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.0842e-04 - mae: 0.0064 - mse: 1.0842e-04 - val_loss: 2.4125e-04 - val_mae: 0.0082 - val_mse: 2.4125e-04\n",
      "Epoch 292/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 1.0794e-04 - mae: 0.0064 - mse: 1.0794e-04 - val_loss: 2.4282e-04 - val_mae: 0.0083 - val_mse: 2.4282e-04\n",
      "Epoch 293/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.0749e-04 - mae: 0.0064 - mse: 1.0749e-04 - val_loss: 2.4441e-04 - val_mae: 0.0084 - val_mse: 2.4441e-04\n",
      "Epoch 294/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0709e-04 - mae: 0.0064 - mse: 1.0709e-04 - val_loss: 2.4601e-04 - val_mae: 0.0085 - val_mse: 2.4601e-04\n",
      "Epoch 295/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.0675e-04 - mae: 0.0064 - mse: 1.0675e-04 - val_loss: 2.4763e-04 - val_mae: 0.0085 - val_mse: 2.4763e-04\n",
      "Epoch 296/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.0648e-04 - mae: 0.0064 - mse: 1.0648e-04 - val_loss: 2.4934e-04 - val_mae: 0.0086 - val_mse: 2.4934e-04\n",
      "Epoch 297/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.0629e-04 - mae: 0.0064 - mse: 1.0629e-04 - val_loss: 2.5124e-04 - val_mae: 0.0087 - val_mse: 2.5124e-04\n",
      "Epoch 298/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.0625e-04 - mae: 0.0064 - mse: 1.0625e-04 - val_loss: 2.5349e-04 - val_mae: 0.0089 - val_mse: 2.5349e-04\n",
      "Epoch 299/500\n",
      "2440/2440 [==============================] - 1s 297us/step - loss: 1.0645e-04 - mae: 0.0065 - mse: 1.0645e-04 - val_loss: 2.5628e-04 - val_mae: 0.0091 - val_mse: 2.5628e-04\n",
      "Epoch 300/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.0701e-04 - mae: 0.0066 - mse: 1.0701e-04 - val_loss: 2.5975e-04 - val_mae: 0.0093 - val_mse: 2.5975e-04\n",
      "Epoch 301/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0803e-04 - mae: 0.0067 - mse: 1.0803e-04 - val_loss: 2.6384e-04 - val_mae: 0.0095 - val_mse: 2.6384e-04\n",
      "Epoch 302/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0945e-04 - mae: 0.0068 - mse: 1.0945e-04 - val_loss: 2.6800e-04 - val_mae: 0.0097 - val_mse: 2.6800e-04\n",
      "Epoch 303/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.1091e-04 - mae: 0.0069 - mse: 1.1091e-04 - val_loss: 2.7134e-04 - val_mae: 0.0098 - val_mse: 2.7134e-04\n",
      "Epoch 304/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.1212e-04 - mae: 0.0070 - mse: 1.1212e-04 - val_loss: 2.7378e-04 - val_mae: 0.0099 - val_mse: 2.7378e-04\n",
      "Epoch 305/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.1321e-04 - mae: 0.0071 - mse: 1.1321e-04 - val_loss: 2.7610e-04 - val_mae: 0.0098 - val_mse: 2.7610e-04\n",
      "Epoch 306/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.1505e-04 - mae: 0.0072 - mse: 1.1505e-04 - val_loss: 2.7653e-04 - val_mae: 0.0097 - val_mse: 2.7653e-04\n",
      "Epoch 307/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.1711e-04 - mae: 0.0073 - mse: 1.1711e-04 - val_loss: 2.7475e-04 - val_mae: 0.0093 - val_mse: 2.7475e-04\n",
      "Epoch 308/500\n",
      "2440/2440 [==============================] - 1s 272us/step - loss: 1.1935e-04 - mae: 0.0074 - mse: 1.1935e-04 - val_loss: 2.6739e-04 - val_mae: 0.0089 - val_mse: 2.6739e-04\n",
      "Epoch 309/500\n",
      "2440/2440 [==============================] - 1s 297us/step - loss: 1.1334e-04 - mae: 0.0069 - mse: 1.1334e-04 - val_loss: 2.4911e-04 - val_mae: 0.0083 - val_mse: 2.4911e-04\n",
      "Epoch 310/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.1226e-04 - mae: 0.0068 - mse: 1.1226e-04 - val_loss: 2.3343e-04 - val_mae: 0.0084 - val_mse: 2.3343e-04\n",
      "Epoch 311/500\n",
      "2440/2440 [==============================] - 1s 272us/step - loss: 1.1090e-04 - mae: 0.0067 - mse: 1.1090e-04 - val_loss: 2.3534e-04 - val_mae: 0.0087 - val_mse: 2.3534e-04\n",
      "Epoch 312/500\n",
      "2440/2440 [==============================] - 1s 274us/step - loss: 1.0835e-04 - mae: 0.0066 - mse: 1.0835e-04 - val_loss: 2.4501e-04 - val_mae: 0.0093 - val_mse: 2.4501e-04\n",
      "Epoch 313/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.0730e-04 - mae: 0.0066 - mse: 1.0730e-04 - val_loss: 2.4569e-04 - val_mae: 0.0091 - val_mse: 2.4569e-04\n",
      "Epoch 314/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.0647e-04 - mae: 0.0066 - mse: 1.0647e-04 - val_loss: 2.4800e-04 - val_mae: 0.0088 - val_mse: 2.4800e-04\n",
      "Epoch 315/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.0504e-04 - mae: 0.0066 - mse: 1.0504e-04 - val_loss: 2.5505e-04 - val_mae: 0.0091 - val_mse: 2.5505e-04\n",
      "Epoch 316/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.0498e-04 - mae: 0.0066 - mse: 1.0498e-04 - val_loss: 2.6246e-04 - val_mae: 0.0098 - val_mse: 2.6246e-04\n",
      "Epoch 317/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.0490e-04 - mae: 0.0067 - mse: 1.0490e-04 - val_loss: 2.6925e-04 - val_mae: 0.0104 - val_mse: 2.6925e-04\n",
      "Epoch 318/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0495e-04 - mae: 0.0067 - mse: 1.0495e-04 - val_loss: 2.7631e-04 - val_mae: 0.0107 - val_mse: 2.7631e-04\n",
      "Epoch 319/500\n",
      "2440/2440 [==============================] - 1s 339us/step - loss: 1.0515e-04 - mae: 0.0067 - mse: 1.0515e-04 - val_loss: 2.8139e-04 - val_mae: 0.0109 - val_mse: 2.8139e-04\n",
      "Epoch 320/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.0781e-04 - mae: 0.0069 - mse: 1.0781e-04 - val_loss: 2.8381e-04 - val_mae: 0.0109 - val_mse: 2.8381e-04\n",
      "Epoch 321/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.1295e-04 - mae: 0.0071 - mse: 1.1295e-04 - val_loss: 2.8712e-04 - val_mae: 0.0113 - val_mse: 2.8712e-04\n",
      "Epoch 322/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.1344e-04 - mae: 0.0072 - mse: 1.1344e-04 - val_loss: 2.9329e-04 - val_mae: 0.0118 - val_mse: 2.9329e-04\n",
      "Epoch 323/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.1022e-04 - mae: 0.0071 - mse: 1.1022e-04 - val_loss: 2.9387e-04 - val_mae: 0.0120 - val_mse: 2.9387e-04\n",
      "Epoch 324/500\n",
      "2440/2440 [==============================] - 1s 274us/step - loss: 1.1199e-04 - mae: 0.0072 - mse: 1.1199e-04 - val_loss: 3.0827e-04 - val_mae: 0.0126 - val_mse: 3.0827e-04\n",
      "Epoch 325/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.1385e-04 - mae: 0.0073 - mse: 1.1385e-04 - val_loss: 3.1188e-04 - val_mae: 0.0127 - val_mse: 3.1188e-04\n",
      "Epoch 326/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.2395e-04 - mae: 0.0078 - mse: 1.2395e-04 - val_loss: 3.1723e-04 - val_mae: 0.0130 - val_mse: 3.1723e-04\n",
      "Epoch 327/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.3626e-04 - mae: 0.0084 - mse: 1.3626e-04 - val_loss: 2.8395e-04 - val_mae: 0.0116 - val_mse: 2.8395e-04\n",
      "Epoch 328/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.4456e-04 - mae: 0.0087 - mse: 1.4456e-04 - val_loss: 2.3994e-04 - val_mae: 0.0089 - val_mse: 2.3994e-04\n",
      "Epoch 329/500\n",
      "2440/2440 [==============================] - 1s 301us/step - loss: 1.4670e-04 - mae: 0.0085 - mse: 1.4670e-04 - val_loss: 2.4583e-04 - val_mae: 0.0091 - val_mse: 2.4583e-04\n",
      "Epoch 330/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 1.3623e-04 - mae: 0.0080 - mse: 1.3623e-04 - val_loss: 2.7453e-04 - val_mae: 0.0107 - val_mse: 2.7453e-04\n",
      "Epoch 331/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.2039e-04 - mae: 0.0073 - mse: 1.2039e-04 - val_loss: 2.7021e-04 - val_mae: 0.0100 - val_mse: 2.7021e-04\n",
      "Epoch 332/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.1162e-04 - mae: 0.0069 - mse: 1.1162e-04 - val_loss: 2.6282e-04 - val_mae: 0.0092 - val_mse: 2.6282e-04\n",
      "Epoch 333/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.0773e-04 - mae: 0.0067 - mse: 1.0773e-04 - val_loss: 2.6307e-04 - val_mae: 0.0089 - val_mse: 2.6307e-04\n",
      "Epoch 334/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.0570e-04 - mae: 0.0066 - mse: 1.0570e-04 - val_loss: 2.6376e-04 - val_mae: 0.0087 - val_mse: 2.6376e-04\n",
      "Epoch 335/500\n",
      "2440/2440 [==============================] - 1s 288us/step - loss: 1.0445e-04 - mae: 0.0065 - mse: 1.0445e-04 - val_loss: 2.6427e-04 - val_mae: 0.0086 - val_mse: 2.6427e-04\n",
      "Epoch 336/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.0359e-04 - mae: 0.0065 - mse: 1.0359e-04 - val_loss: 2.6490e-04 - val_mae: 0.0087 - val_mse: 2.6490e-04\n",
      "Epoch 337/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.0294e-04 - mae: 0.0065 - mse: 1.0294e-04 - val_loss: 2.6584e-04 - val_mae: 0.0089 - val_mse: 2.6584e-04\n",
      "Epoch 338/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0252e-04 - mae: 0.0065 - mse: 1.0252e-04 - val_loss: 2.6723e-04 - val_mae: 0.0092 - val_mse: 2.6723e-04\n",
      "Epoch 339/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.0237e-04 - mae: 0.0066 - mse: 1.0237e-04 - val_loss: 2.6938e-04 - val_mae: 0.0095 - val_mse: 2.6938e-04\n",
      "Epoch 340/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.0257e-04 - mae: 0.0066 - mse: 1.0257e-04 - val_loss: 2.7242e-04 - val_mae: 0.0099 - val_mse: 2.7242e-04\n",
      "Epoch 341/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0316e-04 - mae: 0.0067 - mse: 1.0316e-04 - val_loss: 2.7592e-04 - val_mae: 0.0103 - val_mse: 2.7592e-04\n",
      "Epoch 342/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.0408e-04 - mae: 0.0068 - mse: 1.0408e-04 - val_loss: 2.7882e-04 - val_mae: 0.0106 - val_mse: 2.7882e-04\n",
      "Epoch 343/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0505e-04 - mae: 0.0069 - mse: 1.0505e-04 - val_loss: 2.7969e-04 - val_mae: 0.0107 - val_mse: 2.7969e-04\n",
      "Epoch 344/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.0573e-04 - mae: 0.0069 - mse: 1.0573e-04 - val_loss: 2.7796e-04 - val_mae: 0.0106 - val_mse: 2.7796e-04\n",
      "Epoch 345/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 1.0618e-04 - mae: 0.0069 - mse: 1.0618e-04 - val_loss: 2.7507e-04 - val_mae: 0.0103 - val_mse: 2.7507e-04\n",
      "Epoch 346/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.0683e-04 - mae: 0.0069 - mse: 1.0683e-04 - val_loss: 2.7348e-04 - val_mae: 0.0099 - val_mse: 2.7348e-04\n",
      "Epoch 347/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.0732e-04 - mae: 0.0069 - mse: 1.0732e-04 - val_loss: 2.7428e-04 - val_mae: 0.0098 - val_mse: 2.7428e-04\n",
      "Epoch 348/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.0724e-04 - mae: 0.0069 - mse: 1.0724e-04 - val_loss: 2.7848e-04 - val_mae: 0.0101 - val_mse: 2.7848e-04\n",
      "Epoch 349/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.0743e-04 - mae: 0.0070 - mse: 1.0743e-04 - val_loss: 2.8443e-04 - val_mae: 0.0106 - val_mse: 2.8443e-04\n",
      "Epoch 350/500\n",
      "2440/2440 [==============================] - 1s 303us/step - loss: 1.0780e-04 - mae: 0.0070 - mse: 1.0780e-04 - val_loss: 2.8986e-04 - val_mae: 0.0111 - val_mse: 2.8986e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 351/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.0776e-04 - mae: 0.0070 - mse: 1.0776e-04 - val_loss: 2.9247e-04 - val_mae: 0.0112 - val_mse: 2.9247e-04\n",
      "Epoch 352/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 1.0654e-04 - mae: 0.0070 - mse: 1.0654e-04 - val_loss: 2.8964e-04 - val_mae: 0.0110 - val_mse: 2.8964e-04\n",
      "Epoch 353/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 1.0504e-04 - mae: 0.0068 - mse: 1.0504e-04 - val_loss: 2.8622e-04 - val_mae: 0.0105 - val_mse: 2.8622e-04\n",
      "Epoch 354/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.0642e-04 - mae: 0.0068 - mse: 1.0642e-04 - val_loss: 2.8711e-04 - val_mae: 0.0099 - val_mse: 2.8711e-04\n",
      "Epoch 355/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.0692e-04 - mae: 0.0069 - mse: 1.0692e-04 - val_loss: 2.8345e-04 - val_mae: 0.0095 - val_mse: 2.8345e-04\n",
      "Epoch 356/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 1.0579e-04 - mae: 0.0068 - mse: 1.0579e-04 - val_loss: 2.9011e-04 - val_mae: 0.0104 - val_mse: 2.9011e-04\n",
      "Epoch 357/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.0370e-04 - mae: 0.0068 - mse: 1.0370e-04 - val_loss: 2.9411e-04 - val_mae: 0.0108 - val_mse: 2.9411e-04\n",
      "Epoch 358/500\n",
      "2440/2440 [==============================] - 1s 274us/step - loss: 1.0255e-04 - mae: 0.0068 - mse: 1.0255e-04 - val_loss: 2.9329e-04 - val_mae: 0.0108 - val_mse: 2.9329e-04\n",
      "Epoch 359/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.0125e-04 - mae: 0.0067 - mse: 1.0125e-04 - val_loss: 2.8970e-04 - val_mae: 0.0105 - val_mse: 2.8970e-04\n",
      "Epoch 360/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0139e-04 - mae: 0.0067 - mse: 1.0139e-04 - val_loss: 2.8874e-04 - val_mae: 0.0102 - val_mse: 2.8874e-04\n",
      "Epoch 361/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 1.0218e-04 - mae: 0.0067 - mse: 1.0218e-04 - val_loss: 2.8985e-04 - val_mae: 0.0100 - val_mse: 2.8985e-04\n",
      "Epoch 362/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.0205e-04 - mae: 0.0067 - mse: 1.0205e-04 - val_loss: 2.9064e-04 - val_mae: 0.0099 - val_mse: 2.9064e-04\n",
      "Epoch 363/500\n",
      "2440/2440 [==============================] - 1s 348us/step - loss: 1.0120e-04 - mae: 0.0067 - mse: 1.0120e-04 - val_loss: 2.9573e-04 - val_mae: 0.0101 - val_mse: 2.9573e-04\n",
      "Epoch 364/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.0035e-04 - mae: 0.0066 - mse: 1.0035e-04 - val_loss: 2.9697e-04 - val_mae: 0.0103 - val_mse: 2.9697e-04\n",
      "Epoch 365/500\n",
      "2440/2440 [==============================] - 1s 272us/step - loss: 9.8941e-05 - mae: 0.0066 - mse: 9.8941e-05 - val_loss: 2.9783e-04 - val_mae: 0.0104 - val_mse: 2.9783e-04\n",
      "Epoch 366/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 9.9748e-05 - mae: 0.0066 - mse: 9.9748e-05 - val_loss: 2.9827e-04 - val_mae: 0.0106 - val_mse: 2.9827e-04\n",
      "Epoch 367/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 9.8703e-05 - mae: 0.0066 - mse: 9.8703e-05 - val_loss: 2.9876e-04 - val_mae: 0.0106 - val_mse: 2.9876e-04\n",
      "Epoch 368/500\n",
      "2440/2440 [==============================] - 1s 271us/step - loss: 9.8884e-05 - mae: 0.0066 - mse: 9.8884e-05 - val_loss: 3.0853e-04 - val_mae: 0.0112 - val_mse: 3.0853e-04\n",
      "Epoch 369/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 9.9650e-05 - mae: 0.0067 - mse: 9.9650e-05 - val_loss: 3.0872e-04 - val_mae: 0.0114 - val_mse: 3.0872e-04\n",
      "Epoch 370/500\n",
      "2440/2440 [==============================] - 1s 305us/step - loss: 9.9616e-05 - mae: 0.0067 - mse: 9.9616e-05 - val_loss: 3.1178e-04 - val_mae: 0.0113 - val_mse: 3.1178e-04\n",
      "Epoch 371/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 9.8426e-05 - mae: 0.0066 - mse: 9.8426e-05 - val_loss: 3.0929e-04 - val_mae: 0.0111 - val_mse: 3.0929e-04\n",
      "Epoch 372/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 9.7837e-05 - mae: 0.0065 - mse: 9.7837e-05 - val_loss: 3.0500e-04 - val_mae: 0.0110 - val_mse: 3.0500e-04\n",
      "Epoch 373/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 9.8050e-05 - mae: 0.0064 - mse: 9.8050e-05 - val_loss: 3.0014e-04 - val_mae: 0.0109 - val_mse: 3.0014e-04\n",
      "Epoch 374/500\n",
      "2440/2440 [==============================] - 1s 265us/step - loss: 9.7323e-05 - mae: 0.0065 - mse: 9.7323e-05 - val_loss: 3.0030e-04 - val_mae: 0.0109 - val_mse: 3.0030e-04\n",
      "Epoch 375/500\n",
      "2440/2440 [==============================] - 1s 268us/step - loss: 9.8149e-05 - mae: 0.0066 - mse: 9.8149e-05 - val_loss: 3.0296e-04 - val_mae: 0.0110 - val_mse: 3.0296e-04\n",
      "Epoch 376/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 9.8589e-05 - mae: 0.0067 - mse: 9.8589e-05 - val_loss: 3.0549e-04 - val_mae: 0.0111 - val_mse: 3.0549e-04\n",
      "Epoch 377/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 9.8742e-05 - mae: 0.0067 - mse: 9.8742e-05 - val_loss: 3.0816e-04 - val_mae: 0.0110 - val_mse: 3.0816e-04\n",
      "Epoch 378/500\n",
      "2440/2440 [==============================] - 1s 302us/step - loss: 9.8913e-05 - mae: 0.0067 - mse: 9.8913e-05 - val_loss: 3.1212e-04 - val_mae: 0.0110 - val_mse: 3.1212e-04\n",
      "Epoch 379/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.0424e-04 - mae: 0.0069 - mse: 1.0424e-04 - val_loss: 3.1474e-04 - val_mae: 0.0112 - val_mse: 3.1474e-04\n",
      "Epoch 380/500\n",
      "2440/2440 [==============================] - 1s 271us/step - loss: 1.1417e-04 - mae: 0.0072 - mse: 1.1417e-04 - val_loss: 3.3916e-04 - val_mae: 0.0113 - val_mse: 3.3916e-04\n",
      "Epoch 381/500\n",
      "2440/2440 [==============================] - 1s 264us/step - loss: 1.3496e-04 - mae: 0.0082 - mse: 1.3496e-04 - val_loss: 3.0380e-04 - val_mae: 0.0113 - val_mse: 3.0380e-04\n",
      "Epoch 382/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.2056e-04 - mae: 0.0073 - mse: 1.2056e-04 - val_loss: 2.7340e-04 - val_mae: 0.0092 - val_mse: 2.7340e-04\n",
      "Epoch 383/500\n",
      "2440/2440 [==============================] - 1s 300us/step - loss: 1.1508e-04 - mae: 0.0074 - mse: 1.1508e-04 - val_loss: 3.4217e-04 - val_mae: 0.0114 - val_mse: 3.4217e-04\n",
      "Epoch 384/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 1.1877e-04 - mae: 0.0077 - mse: 1.1877e-04 - val_loss: 3.1250e-04 - val_mae: 0.0105 - val_mse: 3.1250e-04\n",
      "Epoch 385/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.1229e-04 - mae: 0.0073 - mse: 1.1229e-04 - val_loss: 3.0224e-04 - val_mae: 0.0095 - val_mse: 3.0224e-04\n",
      "Epoch 386/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 1.0713e-04 - mae: 0.0072 - mse: 1.0713e-04 - val_loss: 3.0126e-04 - val_mae: 0.0098 - val_mse: 3.0126e-04\n",
      "Epoch 387/500\n",
      "2440/2440 [==============================] - 1s 295us/step - loss: 1.0828e-04 - mae: 0.0072 - mse: 1.0828e-04 - val_loss: 3.0368e-04 - val_mae: 0.0100 - val_mse: 3.0368e-04\n",
      "Epoch 388/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 1.0499e-04 - mae: 0.0070 - mse: 1.0499e-04 - val_loss: 2.9740e-04 - val_mae: 0.0101 - val_mse: 2.9740e-04\n",
      "Epoch 389/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 1.0578e-04 - mae: 0.0071 - mse: 1.0578e-04 - val_loss: 2.9864e-04 - val_mae: 0.0098 - val_mse: 2.9864e-04\n",
      "Epoch 390/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.0274e-04 - mae: 0.0070 - mse: 1.0274e-04 - val_loss: 3.0229e-04 - val_mae: 0.0101 - val_mse: 3.0229e-04\n",
      "Epoch 391/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.0361e-04 - mae: 0.0071 - mse: 1.0361e-04 - val_loss: 3.0646e-04 - val_mae: 0.0102 - val_mse: 3.0646e-04\n",
      "Epoch 392/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.0291e-04 - mae: 0.0070 - mse: 1.0291e-04 - val_loss: 3.0586e-04 - val_mae: 0.0101 - val_mse: 3.0586e-04\n",
      "Epoch 393/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.0135e-04 - mae: 0.0070 - mse: 1.0135e-04 - val_loss: 3.0555e-04 - val_mae: 0.0099 - val_mse: 3.0555e-04\n",
      "Epoch 394/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 9.9408e-05 - mae: 0.0069 - mse: 9.9408e-05 - val_loss: 3.0660e-04 - val_mae: 0.0099 - val_mse: 3.0660e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395/500\n",
      "2440/2440 [==============================] - 1s 263us/step - loss: 9.8032e-05 - mae: 0.0068 - mse: 9.8032e-05 - val_loss: 3.0700e-04 - val_mae: 0.0097 - val_mse: 3.0700e-04\n",
      "Epoch 396/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 9.5973e-05 - mae: 0.0067 - mse: 9.5973e-05 - val_loss: 3.0652e-04 - val_mae: 0.0095 - val_mse: 3.0652e-04\n",
      "Epoch 397/500\n",
      "2440/2440 [==============================] - 1s 272us/step - loss: 9.4194e-05 - mae: 0.0066 - mse: 9.4194e-05 - val_loss: 3.0609e-04 - val_mae: 0.0093 - val_mse: 3.0609e-04\n",
      "Epoch 398/500\n",
      "2440/2440 [==============================] - 1s 268us/step - loss: 9.2076e-05 - mae: 0.0064 - mse: 9.2076e-05 - val_loss: 3.0584e-04 - val_mae: 0.0092 - val_mse: 3.0584e-04\n",
      "Epoch 399/500\n",
      "2440/2440 [==============================] - 1s 263us/step - loss: 9.0494e-05 - mae: 0.0063 - mse: 9.0494e-05 - val_loss: 3.0591e-04 - val_mae: 0.0092 - val_mse: 3.0591e-04\n",
      "Epoch 400/500\n",
      "2440/2440 [==============================] - 1s 264us/step - loss: 8.9274e-05 - mae: 0.0063 - mse: 8.9274e-05 - val_loss: 3.0673e-04 - val_mae: 0.0094 - val_mse: 3.0673e-04\n",
      "Epoch 401/500\n",
      "2440/2440 [==============================] - 1s 267us/step - loss: 8.9045e-05 - mae: 0.0063 - mse: 8.9045e-05 - val_loss: 3.0978e-04 - val_mae: 0.0096 - val_mse: 3.0978e-04\n",
      "Epoch 402/500\n",
      "2440/2440 [==============================] - 1s 261us/step - loss: 8.8663e-05 - mae: 0.0063 - mse: 8.8663e-05 - val_loss: 3.1506e-04 - val_mae: 0.0099 - val_mse: 3.1506e-04\n",
      "Epoch 403/500\n",
      "2440/2440 [==============================] - 1s 266us/step - loss: 8.9348e-05 - mae: 0.0064 - mse: 8.9348e-05 - val_loss: 3.3252e-04 - val_mae: 0.0103 - val_mse: 3.3252e-04\n",
      "Epoch 404/500\n",
      "2440/2440 [==============================] - 1s 265us/step - loss: 8.7274e-05 - mae: 0.0063 - mse: 8.7274e-05 - val_loss: 3.3191e-04 - val_mae: 0.0108 - val_mse: 3.3191e-04\n",
      "Epoch 405/500\n",
      "2440/2440 [==============================] - 1s 262us/step - loss: 9.4593e-05 - mae: 0.0066 - mse: 9.4593e-05 - val_loss: 3.1572e-04 - val_mae: 0.0099 - val_mse: 3.1572e-04\n",
      "Epoch 406/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 1.8958e-04 - mae: 0.0095 - mse: 1.8958e-04 - val_loss: 2.6623e-04 - val_mae: 0.0089 - val_mse: 2.6623e-04\n",
      "Epoch 407/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.0841e-04 - mae: 0.0067 - mse: 1.0841e-04 - val_loss: 2.8487e-04 - val_mae: 0.0112 - val_mse: 2.8487e-04\n",
      "Epoch 408/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.1312e-04 - mae: 0.0073 - mse: 1.1312e-04 - val_loss: 3.1248e-04 - val_mae: 0.0100 - val_mse: 3.1248e-04\n",
      "Epoch 409/500\n",
      "2440/2440 [==============================] - 1s 270us/step - loss: 1.0607e-04 - mae: 0.0071 - mse: 1.0607e-04 - val_loss: 2.9887e-04 - val_mae: 0.0097 - val_mse: 2.9887e-04\n",
      "Epoch 410/500\n",
      "2440/2440 [==============================] - 1s 269us/step - loss: 1.0528e-04 - mae: 0.0071 - mse: 1.0528e-04 - val_loss: 2.9698e-04 - val_mae: 0.0093 - val_mse: 2.9698e-04\n",
      "Epoch 411/500\n",
      "2440/2440 [==============================] - 1s 266us/step - loss: 9.2853e-05 - mae: 0.0066 - mse: 9.2853e-05 - val_loss: 3.1404e-04 - val_mae: 0.0101 - val_mse: 3.1404e-04\n",
      "Epoch 412/500\n",
      "2440/2440 [==============================] - 1s 271us/step - loss: 1.0201e-04 - mae: 0.0071 - mse: 1.0201e-04 - val_loss: 3.1871e-04 - val_mae: 0.0102 - val_mse: 3.1871e-04\n",
      "Epoch 413/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 9.8833e-05 - mae: 0.0070 - mse: 9.8833e-05 - val_loss: 3.2301e-04 - val_mae: 0.0101 - val_mse: 3.2301e-04\n",
      "Epoch 414/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 1.0052e-04 - mae: 0.0071 - mse: 1.0052e-04 - val_loss: 3.3450e-04 - val_mae: 0.0107 - val_mse: 3.3450e-04\n",
      "Epoch 415/500\n",
      "2440/2440 [==============================] - 1s 266us/step - loss: 1.0187e-04 - mae: 0.0071 - mse: 1.0187e-04 - val_loss: 3.3138e-04 - val_mae: 0.0105 - val_mse: 3.3138e-04\n",
      "Epoch 416/500\n",
      "2440/2440 [==============================] - 1s 269us/step - loss: 9.8840e-05 - mae: 0.0069 - mse: 9.8840e-05 - val_loss: 3.3615e-04 - val_mae: 0.0103 - val_mse: 3.3615e-04\n",
      "Epoch 417/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 1.0092e-04 - mae: 0.0071 - mse: 1.0092e-04 - val_loss: 3.4784e-04 - val_mae: 0.0116 - val_mse: 3.4784e-04\n",
      "Epoch 418/500\n",
      "2440/2440 [==============================] - 1s 280us/step - loss: 9.9979e-05 - mae: 0.0070 - mse: 9.9979e-05 - val_loss: 3.1619e-04 - val_mae: 0.0095 - val_mse: 3.1619e-04\n",
      "Epoch 419/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 9.4722e-05 - mae: 0.0065 - mse: 9.4722e-05 - val_loss: 3.2443e-04 - val_mae: 0.0103 - val_mse: 3.2443e-04\n",
      "Epoch 420/500\n",
      "2440/2440 [==============================] - 1s 296us/step - loss: 1.0453e-04 - mae: 0.0070 - mse: 1.0453e-04 - val_loss: 3.7946e-04 - val_mae: 0.0139 - val_mse: 3.7946e-04\n",
      "Epoch 421/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.0968e-04 - mae: 0.0074 - mse: 1.0968e-04 - val_loss: 2.9541e-04 - val_mae: 0.0091 - val_mse: 2.9541e-04\n",
      "Epoch 422/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 8.6053e-05 - mae: 0.0061 - mse: 8.6053e-05 - val_loss: 3.1029e-04 - val_mae: 0.0097 - val_mse: 3.1029e-04\n",
      "Epoch 423/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 8.7981e-05 - mae: 0.0063 - mse: 8.7981e-05 - val_loss: 3.2302e-04 - val_mae: 0.0097 - val_mse: 3.2302e-04\n",
      "Epoch 424/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 9.0452e-05 - mae: 0.0065 - mse: 9.0452e-05 - val_loss: 3.1922e-04 - val_mae: 0.0097 - val_mse: 3.1922e-04\n",
      "Epoch 425/500\n",
      "2440/2440 [==============================] - 1s 277us/step - loss: 9.0669e-05 - mae: 0.0064 - mse: 9.0669e-05 - val_loss: 3.2121e-04 - val_mae: 0.0099 - val_mse: 3.2121e-04\n",
      "Epoch 426/500\n",
      "2440/2440 [==============================] - 1s 271us/step - loss: 8.6024e-05 - mae: 0.0063 - mse: 8.6024e-05 - val_loss: 3.1925e-04 - val_mae: 0.0097 - val_mse: 3.1925e-04\n",
      "Epoch 427/500\n",
      "2440/2440 [==============================] - 1s 269us/step - loss: 1.0187e-04 - mae: 0.0070 - mse: 1.0187e-04 - val_loss: 3.3655e-04 - val_mae: 0.0124 - val_mse: 3.3655e-04\n",
      "Epoch 428/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 1.0962e-04 - mae: 0.0073 - mse: 1.0962e-04 - val_loss: 3.4150e-04 - val_mae: 0.0105 - val_mse: 3.4150e-04\n",
      "Epoch 429/500\n",
      "2440/2440 [==============================] - 1s 266us/step - loss: 9.4042e-05 - mae: 0.0066 - mse: 9.4042e-05 - val_loss: 3.2095e-04 - val_mae: 0.0098 - val_mse: 3.2095e-04\n",
      "Epoch 430/500\n",
      "2440/2440 [==============================] - 1s 265us/step - loss: 8.8162e-05 - mae: 0.0064 - mse: 8.8162e-05 - val_loss: 3.3512e-04 - val_mae: 0.0097 - val_mse: 3.3512e-04\n",
      "Epoch 431/500\n",
      "2440/2440 [==============================] - 1s 275us/step - loss: 8.5365e-05 - mae: 0.0062 - mse: 8.5365e-05 - val_loss: 3.2510e-04 - val_mae: 0.0096 - val_mse: 3.2510e-04\n",
      "Epoch 432/500\n",
      "2440/2440 [==============================] - 1s 267us/step - loss: 8.5610e-05 - mae: 0.0062 - mse: 8.5610e-05 - val_loss: 3.3921e-04 - val_mae: 0.0103 - val_mse: 3.3921e-04\n",
      "Epoch 433/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 8.8959e-05 - mae: 0.0065 - mse: 8.8959e-05 - val_loss: 3.6522e-04 - val_mae: 0.0119 - val_mse: 3.6522e-04\n",
      "Epoch 434/500\n",
      "2440/2440 [==============================] - 1s 276us/step - loss: 1.0370e-04 - mae: 0.0072 - mse: 1.0370e-04 - val_loss: 3.2361e-04 - val_mae: 0.0100 - val_mse: 3.2361e-04\n",
      "Epoch 435/500\n",
      "2440/2440 [==============================] - 1s 298us/step - loss: 1.0090e-04 - mae: 0.0068 - mse: 1.0090e-04 - val_loss: 3.7519e-04 - val_mae: 0.0140 - val_mse: 3.7519e-04\n",
      "Epoch 436/500\n",
      "2440/2440 [==============================] - 1s 269us/step - loss: 1.0643e-04 - mae: 0.0072 - mse: 1.0643e-04 - val_loss: 3.6452e-04 - val_mae: 0.0113 - val_mse: 3.6452e-04\n",
      "Epoch 437/500\n",
      "2440/2440 [==============================] - 1s 273us/step - loss: 1.0926e-04 - mae: 0.0073 - mse: 1.0926e-04 - val_loss: 3.0196e-04 - val_mae: 0.0095 - val_mse: 3.0196e-04\n",
      "Epoch 438/500\n",
      "2440/2440 [==============================] - 1s 269us/step - loss: 1.0295e-04 - mae: 0.0069 - mse: 1.0295e-04 - val_loss: 3.1769e-04 - val_mae: 0.0101 - val_mse: 3.1769e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 439/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 9.2455e-05 - mae: 0.0065 - mse: 9.2455e-05 - val_loss: 3.2848e-04 - val_mae: 0.0099 - val_mse: 3.2848e-04\n",
      "Epoch 440/500\n",
      "2440/2440 [==============================] - 1s 336us/step - loss: 9.3698e-05 - mae: 0.0066 - mse: 9.3698e-05 - val_loss: 3.2057e-04 - val_mae: 0.0098 - val_mse: 3.2057e-04\n",
      "Epoch 441/500\n",
      "2440/2440 [==============================] - 1s 294us/step - loss: 8.6396e-05 - mae: 0.0064 - mse: 8.6397e-05 - val_loss: 3.6014e-04 - val_mae: 0.0106 - val_mse: 3.6014e-04\n",
      "Epoch 442/500\n",
      "2440/2440 [==============================] - 1s 396us/step - loss: 9.6028e-05 - mae: 0.0069 - mse: 9.6028e-05 - val_loss: 3.4149e-04 - val_mae: 0.0108 - val_mse: 3.4149e-04\n",
      "Epoch 443/500\n",
      "2440/2440 [==============================] - 1s 329us/step - loss: 9.7392e-05 - mae: 0.0068 - mse: 9.7392e-05 - val_loss: 3.2756e-04 - val_mae: 0.0098 - val_mse: 3.2756e-04\n",
      "Epoch 444/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 8.4516e-05 - mae: 0.0063 - mse: 8.4516e-05 - val_loss: 3.3820e-04 - val_mae: 0.0103 - val_mse: 3.3820e-04\n",
      "Epoch 445/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 8.7394e-05 - mae: 0.0065 - mse: 8.7394e-05 - val_loss: 3.4104e-04 - val_mae: 0.0098 - val_mse: 3.4104e-04\n",
      "Epoch 446/500\n",
      "2440/2440 [==============================] - 1s 305us/step - loss: 8.4401e-05 - mae: 0.0063 - mse: 8.4401e-05 - val_loss: 3.3714e-04 - val_mae: 0.0098 - val_mse: 3.3714e-04\n",
      "Epoch 447/500\n",
      "2440/2440 [==============================] - 1s 309us/step - loss: 8.3060e-05 - mae: 0.0062 - mse: 8.3060e-05 - val_loss: 3.5634e-04 - val_mae: 0.0104 - val_mse: 3.5634e-04\n",
      "Epoch 448/500\n",
      "2440/2440 [==============================] - 1s 298us/step - loss: 8.0399e-05 - mae: 0.0061 - mse: 8.0399e-05 - val_loss: 3.4436e-04 - val_mae: 0.0100 - val_mse: 3.4436e-04\n",
      "Epoch 449/500\n",
      "2440/2440 [==============================] - 1s 291us/step - loss: 8.7105e-05 - mae: 0.0064 - mse: 8.7105e-05 - val_loss: 3.2402e-04 - val_mae: 0.0097 - val_mse: 3.2402e-04\n",
      "Epoch 450/500\n",
      "2440/2440 [==============================] - 1s 304us/step - loss: 8.8344e-05 - mae: 0.0064 - mse: 8.8344e-05 - val_loss: 3.4983e-04 - val_mae: 0.0107 - val_mse: 3.4983e-04\n",
      "Epoch 451/500\n",
      "2440/2440 [==============================] - 1s 304us/step - loss: 8.0268e-05 - mae: 0.0060 - mse: 8.0268e-05 - val_loss: 3.5031e-04 - val_mae: 0.0102 - val_mse: 3.5031e-04\n",
      "Epoch 452/500\n",
      "2440/2440 [==============================] - 1s 304us/step - loss: 8.3270e-05 - mae: 0.0062 - mse: 8.3270e-05 - val_loss: 3.6342e-04 - val_mae: 0.0104 - val_mse: 3.6342e-04\n",
      "Epoch 453/500\n",
      "2440/2440 [==============================] - 1s 301us/step - loss: 8.1695e-05 - mae: 0.0062 - mse: 8.1695e-05 - val_loss: 3.4067e-04 - val_mae: 0.0099 - val_mse: 3.4067e-04\n",
      "Epoch 454/500\n",
      "2440/2440 [==============================] - 1s 286us/step - loss: 8.5494e-05 - mae: 0.0063 - mse: 8.5494e-05 - val_loss: 3.4360e-04 - val_mae: 0.0102 - val_mse: 3.4360e-04\n",
      "Epoch 455/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 8.6492e-05 - mae: 0.0064 - mse: 8.6492e-05 - val_loss: 3.7506e-04 - val_mae: 0.0108 - val_mse: 3.7506e-04\n",
      "Epoch 456/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 9.3412e-05 - mae: 0.0069 - mse: 9.3412e-05 - val_loss: 3.5383e-04 - val_mae: 0.0104 - val_mse: 3.5383e-04\n",
      "Epoch 457/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 8.5227e-05 - mae: 0.0062 - mse: 8.5227e-05 - val_loss: 3.7033e-04 - val_mae: 0.0106 - val_mse: 3.7033e-04\n",
      "Epoch 458/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 1.0048e-04 - mae: 0.0070 - mse: 1.0048e-04 - val_loss: 3.6126e-04 - val_mae: 0.0119 - val_mse: 3.6126e-04\n",
      "Epoch 459/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 9.3792e-05 - mae: 0.0067 - mse: 9.3792e-05 - val_loss: 3.7497e-04 - val_mae: 0.0109 - val_mse: 3.7497e-04\n",
      "Epoch 460/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.0258e-04 - mae: 0.0071 - mse: 1.0258e-04 - val_loss: 3.4540e-04 - val_mae: 0.0104 - val_mse: 3.4540e-04\n",
      "Epoch 461/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 8.9412e-05 - mae: 0.0065 - mse: 8.9412e-05 - val_loss: 3.7863e-04 - val_mae: 0.0109 - val_mse: 3.7863e-04\n",
      "Epoch 462/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 1.0240e-04 - mae: 0.0073 - mse: 1.0240e-04 - val_loss: 3.8303e-04 - val_mae: 0.0121 - val_mse: 3.8303e-04\n",
      "Epoch 463/500\n",
      "2440/2440 [==============================] - 1s 306us/step - loss: 1.1524e-04 - mae: 0.0077 - mse: 1.1524e-04 - val_loss: 3.7808e-04 - val_mae: 0.0113 - val_mse: 3.7808e-04\n",
      "Epoch 464/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 1.0019e-04 - mae: 0.0072 - mse: 1.0019e-04 - val_loss: 3.6873e-04 - val_mae: 0.0128 - val_mse: 3.6873e-04\n",
      "Epoch 465/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 1.0431e-04 - mae: 0.0072 - mse: 1.0431e-04 - val_loss: 3.1414e-04 - val_mae: 0.0098 - val_mse: 3.1414e-04\n",
      "Epoch 466/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 8.7681e-05 - mae: 0.0065 - mse: 8.7681e-05 - val_loss: 3.2373e-04 - val_mae: 0.0096 - val_mse: 3.2373e-04\n",
      "Epoch 467/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 8.6994e-05 - mae: 0.0065 - mse: 8.6994e-05 - val_loss: 3.2401e-04 - val_mae: 0.0095 - val_mse: 3.2401e-04\n",
      "Epoch 468/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 8.2868e-05 - mae: 0.0063 - mse: 8.2868e-05 - val_loss: 3.4815e-04 - val_mae: 0.0105 - val_mse: 3.4815e-04\n",
      "Epoch 469/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 8.2463e-05 - mae: 0.0064 - mse: 8.2463e-05 - val_loss: 3.5024e-04 - val_mae: 0.0104 - val_mse: 3.5024e-04\n",
      "Epoch 470/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 7.9581e-05 - mae: 0.0061 - mse: 7.9581e-05 - val_loss: 3.3110e-04 - val_mae: 0.0098 - val_mse: 3.3110e-04\n",
      "Epoch 471/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 8.0297e-05 - mae: 0.0061 - mse: 8.0297e-05 - val_loss: 3.5611e-04 - val_mae: 0.0103 - val_mse: 3.5611e-04\n",
      "Epoch 472/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 8.1905e-05 - mae: 0.0062 - mse: 8.1905e-05 - val_loss: 3.7079e-04 - val_mae: 0.0107 - val_mse: 3.7079e-04\n",
      "Epoch 473/500\n",
      "2440/2440 [==============================] - 1s 309us/step - loss: 8.4462e-05 - mae: 0.0062 - mse: 8.4462e-05 - val_loss: 3.6015e-04 - val_mae: 0.0110 - val_mse: 3.6015e-04\n",
      "Epoch 474/500\n",
      "2440/2440 [==============================] - 1s 295us/step - loss: 7.4708e-05 - mae: 0.0058 - mse: 7.4708e-05 - val_loss: 3.5892e-04 - val_mae: 0.0103 - val_mse: 3.5892e-04\n",
      "Epoch 475/500\n",
      "2440/2440 [==============================] - 1s 289us/step - loss: 8.7036e-05 - mae: 0.0063 - mse: 8.7036e-05 - val_loss: 3.4666e-04 - val_mae: 0.0105 - val_mse: 3.4666e-04\n",
      "Epoch 476/500\n",
      "2440/2440 [==============================] - 1s 290us/step - loss: 8.2759e-05 - mae: 0.0063 - mse: 8.2759e-05 - val_loss: 3.9316e-04 - val_mae: 0.0108 - val_mse: 3.9316e-04\n",
      "Epoch 477/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 8.1118e-05 - mae: 0.0064 - mse: 8.1118e-05 - val_loss: 3.8428e-04 - val_mae: 0.0106 - val_mse: 3.8428e-04\n",
      "Epoch 478/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 8.1839e-05 - mae: 0.0063 - mse: 8.1839e-05 - val_loss: 3.7808e-04 - val_mae: 0.0111 - val_mse: 3.7808e-04\n",
      "Epoch 479/500\n",
      "2440/2440 [==============================] - 1s 299us/step - loss: 7.6324e-05 - mae: 0.0061 - mse: 7.6324e-05 - val_loss: 3.9988e-04 - val_mae: 0.0116 - val_mse: 3.9988e-04\n",
      "Epoch 480/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 7.9956e-05 - mae: 0.0064 - mse: 7.9956e-05 - val_loss: 3.5380e-04 - val_mae: 0.0101 - val_mse: 3.5380e-04\n",
      "Epoch 481/500\n",
      "2440/2440 [==============================] - 1s 292us/step - loss: 8.6935e-05 - mae: 0.0064 - mse: 8.6935e-05 - val_loss: 3.9434e-04 - val_mae: 0.0110 - val_mse: 3.9434e-04\n",
      "Epoch 482/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 7.8094e-05 - mae: 0.0062 - mse: 7.8094e-05 - val_loss: 3.8708e-04 - val_mae: 0.0108 - val_mse: 3.8708e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 483/500\n",
      "2440/2440 [==============================] - 1s 293us/step - loss: 7.5337e-05 - mae: 0.0061 - mse: 7.5337e-05 - val_loss: 3.8486e-04 - val_mae: 0.0108 - val_mse: 3.8486e-04\n",
      "Epoch 484/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 7.6622e-05 - mae: 0.0061 - mse: 7.6622e-05 - val_loss: 4.0465e-04 - val_mae: 0.0112 - val_mse: 4.0465e-04\n",
      "Epoch 485/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 8.3597e-05 - mae: 0.0065 - mse: 8.3597e-05 - val_loss: 4.0550e-04 - val_mae: 0.0113 - val_mse: 4.0550e-04\n",
      "Epoch 486/500\n",
      "2440/2440 [==============================] - 1s 278us/step - loss: 9.0490e-05 - mae: 0.0068 - mse: 9.0490e-05 - val_loss: 4.5308e-04 - val_mae: 0.0116 - val_mse: 4.5308e-04\n",
      "Epoch 487/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 8.2534e-05 - mae: 0.0065 - mse: 8.2534e-05 - val_loss: 3.9571e-04 - val_mae: 0.0107 - val_mse: 3.9571e-04\n",
      "Epoch 488/500\n",
      "2440/2440 [==============================] - 1s 281us/step - loss: 9.3862e-05 - mae: 0.0068 - mse: 9.3862e-05 - val_loss: 3.0759e-04 - val_mae: 0.0097 - val_mse: 3.0759e-04\n",
      "Epoch 489/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 8.4868e-05 - mae: 0.0063 - mse: 8.4868e-05 - val_loss: 4.1959e-04 - val_mae: 0.0124 - val_mse: 4.1959e-04\n",
      "Epoch 490/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 7.9558e-05 - mae: 0.0063 - mse: 7.9558e-05 - val_loss: 3.8335e-04 - val_mae: 0.0107 - val_mse: 3.8335e-04\n",
      "Epoch 491/500\n",
      "2440/2440 [==============================] - 1s 279us/step - loss: 9.2131e-05 - mae: 0.0069 - mse: 9.2131e-05 - val_loss: 3.9333e-04 - val_mae: 0.0119 - val_mse: 3.9333e-04\n",
      "Epoch 492/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 8.9919e-05 - mae: 0.0067 - mse: 8.9919e-05 - val_loss: 4.1590e-04 - val_mae: 0.0117 - val_mse: 4.1590e-04\n",
      "Epoch 493/500\n",
      "2440/2440 [==============================] - 1s 321us/step - loss: 7.9306e-05 - mae: 0.0065 - mse: 7.9306e-05 - val_loss: 3.8598e-04 - val_mae: 0.0108 - val_mse: 3.8598e-04\n",
      "Epoch 494/500\n",
      "2440/2440 [==============================] - 1s 285us/step - loss: 1.0191e-04 - mae: 0.0072 - mse: 1.0191e-04 - val_loss: 3.6669e-04 - val_mae: 0.0115 - val_mse: 3.6669e-04\n",
      "Epoch 495/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 8.3860e-05 - mae: 0.0065 - mse: 8.3860e-05 - val_loss: 4.1808e-04 - val_mae: 0.0116 - val_mse: 4.1808e-04\n",
      "Epoch 496/500\n",
      "2440/2440 [==============================] - 1s 287us/step - loss: 9.1763e-05 - mae: 0.0070 - mse: 9.1763e-05 - val_loss: 3.6716e-04 - val_mae: 0.0104 - val_mse: 3.6716e-04\n",
      "Epoch 497/500\n",
      "2440/2440 [==============================] - 1s 283us/step - loss: 8.3840e-05 - mae: 0.0065 - mse: 8.3840e-05 - val_loss: 3.7311e-04 - val_mae: 0.0116 - val_mse: 3.7311e-04\n",
      "Epoch 498/500\n",
      "2440/2440 [==============================] - 1s 317us/step - loss: 7.9396e-05 - mae: 0.0063 - mse: 7.9396e-05 - val_loss: 4.1868e-04 - val_mae: 0.0119 - val_mse: 4.1868e-04\n",
      "Epoch 499/500\n",
      "2440/2440 [==============================] - 1s 284us/step - loss: 8.2509e-05 - mae: 0.0067 - mse: 8.2509e-05 - val_loss: 3.8194e-04 - val_mae: 0.0107 - val_mse: 3.8194e-04\n",
      "Epoch 500/500\n",
      "2440/2440 [==============================] - 1s 282us/step - loss: 8.6927e-05 - mae: 0.0067 - mse: 8.6927e-05 - val_loss: 4.0472e-04 - val_mae: 0.0117 - val_mse: 4.0472e-04\n"
     ]
    }
   ],
   "source": [
    "# Fit LSTM\n",
    "lstmhistory = lstm.fit(X_train_LSTM, y_train_LSTM,\n",
    "                       epochs=EPOCHS,\n",
    "                       validation_data=(X_test_LSTM, y_test_LSTM),\n",
    "                       verbose=1,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fb5a0e9b70>]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph LSTM Error, mae and mse\n",
    "plt.plot(lstmhistory.history['loss'])\n",
    "plt.plot(lstmhistory.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM Model WITHOUT Mobility Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4222 samples, validate on 1056 samples\n",
      "Epoch 1/500\n",
      "4222/4222 [==============================] - 1s 323us/step - loss: 0.0301 - mae: 0.1085 - mse: 0.0301 - val_loss: 0.0233 - val_mae: 0.0937 - val_mse: 0.0233\n",
      "Epoch 2/500\n",
      "4222/4222 [==============================] - 1s 253us/step - loss: 0.0224 - mae: 0.0810 - mse: 0.0224 - val_loss: 0.0199 - val_mae: 0.0780 - val_mse: 0.0199\n",
      "Epoch 3/500\n",
      "4222/4222 [==============================] - 1s 251us/step - loss: 0.0197 - mae: 0.0703 - mse: 0.0197 - val_loss: 0.0184 - val_mae: 0.0735 - val_mse: 0.0184\n",
      "Epoch 4/500\n",
      "4222/4222 [==============================] - 1s 246us/step - loss: 0.0183 - mae: 0.0664 - mse: 0.0183 - val_loss: 0.0170 - val_mae: 0.0720 - val_mse: 0.0170\n",
      "Epoch 5/500\n",
      "4222/4222 [==============================] - 1s 260us/step - loss: 0.0167 - mae: 0.0632 - mse: 0.0167 - val_loss: 0.0149 - val_mae: 0.0651 - val_mse: 0.0149\n",
      "Epoch 6/500\n",
      "4222/4222 [==============================] - 1s 246us/step - loss: 0.0156 - mae: 0.0620 - mse: 0.0156 - val_loss: 0.0135 - val_mae: 0.0595 - val_mse: 0.0135\n",
      "Epoch 7/500\n",
      "4222/4222 [==============================] - 1s 249us/step - loss: 0.0149 - mae: 0.0599 - mse: 0.0149 - val_loss: 0.0130 - val_mae: 0.0577 - val_mse: 0.0130\n",
      "Epoch 8/500\n",
      "4222/4222 [==============================] - 1s 283us/step - loss: 0.0147 - mae: 0.0592 - mse: 0.0147 - val_loss: 0.0126 - val_mae: 0.0579 - val_mse: 0.0126\n",
      "Epoch 9/500\n",
      "4222/4222 [==============================] - 1s 259us/step - loss: 0.0144 - mae: 0.0586 - mse: 0.0144 - val_loss: 0.0125 - val_mae: 0.0590 - val_mse: 0.0125\n",
      "Epoch 10/500\n",
      "4222/4222 [==============================] - 1s 256us/step - loss: 0.0140 - mae: 0.0578 - mse: 0.0140 - val_loss: 0.0122 - val_mae: 0.0590 - val_mse: 0.0122\n",
      "Epoch 11/500\n",
      "4222/4222 [==============================] - 1s 257us/step - loss: 0.0137 - mae: 0.0568 - mse: 0.0137 - val_loss: 0.0119 - val_mae: 0.0564 - val_mse: 0.0119\n",
      "Epoch 12/500\n",
      "4222/4222 [==============================] - 1s 261us/step - loss: 0.0134 - mae: 0.0557 - mse: 0.0134 - val_loss: 0.0115 - val_mae: 0.0528 - val_mse: 0.0115\n",
      "Epoch 13/500\n",
      "4222/4222 [==============================] - 1s 246us/step - loss: 0.0131 - mae: 0.0537 - mse: 0.0131 - val_loss: 0.0113 - val_mae: 0.0527 - val_mse: 0.0113\n",
      "Epoch 14/500\n",
      "4222/4222 [==============================] - 1s 260us/step - loss: 0.0129 - mae: 0.0536 - mse: 0.0129 - val_loss: 0.0111 - val_mae: 0.0513 - val_mse: 0.0111\n",
      "Epoch 15/500\n",
      "4222/4222 [==============================] - 1s 260us/step - loss: 0.0127 - mae: 0.0527 - mse: 0.0127 - val_loss: 0.0109 - val_mae: 0.0535 - val_mse: 0.0109\n",
      "Epoch 16/500\n",
      "4222/4222 [==============================] - 1s 255us/step - loss: 0.0126 - mae: 0.0519 - mse: 0.0126 - val_loss: 0.0107 - val_mae: 0.0512 - val_mse: 0.0107\n",
      "Epoch 17/500\n",
      "4222/4222 [==============================] - 1s 255us/step - loss: 0.0124 - mae: 0.0518 - mse: 0.0124 - val_loss: 0.0106 - val_mae: 0.0503 - val_mse: 0.0106\n",
      "Epoch 18/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0123 - mae: 0.0512 - mse: 0.0123 - val_loss: 0.0104 - val_mae: 0.0499 - val_mse: 0.0104\n",
      "Epoch 19/500\n",
      "4222/4222 [==============================] - 1s 274us/step - loss: 0.0121 - mae: 0.0507 - mse: 0.0121 - val_loss: 0.0103 - val_mae: 0.0504 - val_mse: 0.0103\n",
      "Epoch 20/500\n",
      "4222/4222 [==============================] - 1s 244us/step - loss: 0.0119 - mae: 0.0504 - mse: 0.0119 - val_loss: 0.0102 - val_mae: 0.0500 - val_mse: 0.0102\n",
      "Epoch 21/500\n",
      "4222/4222 [==============================] - 1s 248us/step - loss: 0.0118 - mae: 0.0492 - mse: 0.0118 - val_loss: 0.0101 - val_mae: 0.0495 - val_mse: 0.0101\n",
      "Epoch 22/500\n",
      "4222/4222 [==============================] - 1s 251us/step - loss: 0.0116 - mae: 0.0485 - mse: 0.0116 - val_loss: 0.0099 - val_mae: 0.0470 - val_mse: 0.0099\n",
      "Epoch 23/500\n",
      "4222/4222 [==============================] - 1s 247us/step - loss: 0.0114 - mae: 0.0486 - mse: 0.0114 - val_loss: 0.0097 - val_mae: 0.0470 - val_mse: 0.0097\n",
      "Epoch 24/500\n",
      "4222/4222 [==============================] - 1s 248us/step - loss: 0.0112 - mae: 0.0490 - mse: 0.0112 - val_loss: 0.0096 - val_mae: 0.0483 - val_mse: 0.0096\n",
      "Epoch 25/500\n",
      "4222/4222 [==============================] - 1s 271us/step - loss: 0.0111 - mae: 0.0488 - mse: 0.0111 - val_loss: 0.0094 - val_mae: 0.0465 - val_mse: 0.0094\n",
      "Epoch 26/500\n",
      "4222/4222 [==============================] - 1s 254us/step - loss: 0.0109 - mae: 0.0478 - mse: 0.0109 - val_loss: 0.0093 - val_mae: 0.0461 - val_mse: 0.0093\n",
      "Epoch 27/500\n",
      "4222/4222 [==============================] - 1s 247us/step - loss: 0.0107 - mae: 0.0472 - mse: 0.0107 - val_loss: 0.0091 - val_mae: 0.0461 - val_mse: 0.0091\n",
      "Epoch 28/500\n",
      "4222/4222 [==============================] - 1s 249us/step - loss: 0.0106 - mae: 0.0464 - mse: 0.0106 - val_loss: 0.0090 - val_mae: 0.0459 - val_mse: 0.0090\n",
      "Epoch 29/500\n",
      "4222/4222 [==============================] - 1s 259us/step - loss: 0.0103 - mae: 0.0455 - mse: 0.0103 - val_loss: 0.0088 - val_mae: 0.0455 - val_mse: 0.0088\n",
      "Epoch 30/500\n",
      "4222/4222 [==============================] - 1s 268us/step - loss: 0.0101 - mae: 0.0445 - mse: 0.0101 - val_loss: 0.0086 - val_mae: 0.0450 - val_mse: 0.0086\n",
      "Epoch 31/500\n",
      "4222/4222 [==============================] - 1s 277us/step - loss: 0.0099 - mae: 0.0441 - mse: 0.0099 - val_loss: 0.0085 - val_mae: 0.0445 - val_mse: 0.0085\n",
      "Epoch 32/500\n",
      "4222/4222 [==============================] - 1s 309us/step - loss: 0.0098 - mae: 0.0442 - mse: 0.0098 - val_loss: 0.0084 - val_mae: 0.0441 - val_mse: 0.0084\n",
      "Epoch 33/500\n",
      "4222/4222 [==============================] - 1s 297us/step - loss: 0.0096 - mae: 0.0441 - mse: 0.0096 - val_loss: 0.0083 - val_mae: 0.0436 - val_mse: 0.0083\n",
      "Epoch 34/500\n",
      "4222/4222 [==============================] - 1s 278us/step - loss: 0.0095 - mae: 0.0440 - mse: 0.0095 - val_loss: 0.0082 - val_mae: 0.0438 - val_mse: 0.0082\n",
      "Epoch 35/500\n",
      "4222/4222 [==============================] - 1s 278us/step - loss: 0.0093 - mae: 0.0439 - mse: 0.0093 - val_loss: 0.0082 - val_mae: 0.0446 - val_mse: 0.0082\n",
      "Epoch 36/500\n",
      "4222/4222 [==============================] - 1s 296us/step - loss: 0.0091 - mae: 0.0436 - mse: 0.0091 - val_loss: 0.0082 - val_mae: 0.0452 - val_mse: 0.0082\n",
      "Epoch 37/500\n",
      "4222/4222 [==============================] - 1s 310us/step - loss: 0.0089 - mae: 0.0432 - mse: 0.0089 - val_loss: 0.0082 - val_mae: 0.0466 - val_mse: 0.0082\n",
      "Epoch 38/500\n",
      "4222/4222 [==============================] - 1s 323us/step - loss: 0.0086 - mae: 0.0426 - mse: 0.0086 - val_loss: 0.0083 - val_mae: 0.0473 - val_mse: 0.0083\n",
      "Epoch 39/500\n",
      "4222/4222 [==============================] - 1s 304us/step - loss: 0.0082 - mae: 0.0422 - mse: 0.0082 - val_loss: 0.0083 - val_mae: 0.0475 - val_mse: 0.0083\n",
      "Epoch 40/500\n",
      "4222/4222 [==============================] - 1s 322us/step - loss: 0.0080 - mae: 0.0416 - mse: 0.0080 - val_loss: 0.0082 - val_mae: 0.0468 - val_mse: 0.0082\n",
      "Epoch 41/500\n",
      "4222/4222 [==============================] - 1s 302us/step - loss: 0.0077 - mae: 0.0406 - mse: 0.0077 - val_loss: 0.0079 - val_mae: 0.0443 - val_mse: 0.0079\n",
      "Epoch 42/500\n",
      "4222/4222 [==============================] - 1s 292us/step - loss: 0.0073 - mae: 0.0395 - mse: 0.0073 - val_loss: 0.0076 - val_mae: 0.0421 - val_mse: 0.0076\n",
      "Epoch 43/500\n",
      "4222/4222 [==============================] - 1s 311us/step - loss: 0.0069 - mae: 0.0378 - mse: 0.0069 - val_loss: 0.0072 - val_mae: 0.0405 - val_mse: 0.0072\n",
      "Epoch 44/500\n",
      "4222/4222 [==============================] - 1s 325us/step - loss: 0.0065 - mae: 0.0367 - mse: 0.0065 - val_loss: 0.0072 - val_mae: 0.0399 - val_mse: 0.0072\n",
      "Epoch 45/500\n",
      "4222/4222 [==============================] - 1s 305us/step - loss: 0.0063 - mae: 0.0359 - mse: 0.0063 - val_loss: 0.0071 - val_mae: 0.0390 - val_mse: 0.0071\n",
      "Epoch 46/500\n",
      "4222/4222 [==============================] - 1s 291us/step - loss: 0.0060 - mae: 0.0351 - mse: 0.0060 - val_loss: 0.0072 - val_mae: 0.0392 - val_mse: 0.0072\n",
      "Epoch 47/500\n",
      "4222/4222 [==============================] - 1s 287us/step - loss: 0.0057 - mae: 0.0343 - mse: 0.0057 - val_loss: 0.0075 - val_mae: 0.0404 - val_mse: 0.0075\n",
      "Epoch 48/500\n",
      "4222/4222 [==============================] - 1s 304us/step - loss: 0.0056 - mae: 0.0340 - mse: 0.0056 - val_loss: 0.0073 - val_mae: 0.0393 - val_mse: 0.0073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "4222/4222 [==============================] - 1s 286us/step - loss: 0.0052 - mae: 0.0329 - mse: 0.0052 - val_loss: 0.0080 - val_mae: 0.0418 - val_mse: 0.0080\n",
      "Epoch 50/500\n",
      "4222/4222 [==============================] - 1s 273us/step - loss: 0.0052 - mae: 0.0329 - mse: 0.0052 - val_loss: 0.0074 - val_mae: 0.0399 - val_mse: 0.0074\n",
      "Epoch 51/500\n",
      "4222/4222 [==============================] - 1s 274us/step - loss: 0.0048 - mae: 0.0317 - mse: 0.0048 - val_loss: 0.0079 - val_mae: 0.0411 - val_mse: 0.0079\n",
      "Epoch 52/500\n",
      "4222/4222 [==============================] - 1s 283us/step - loss: 0.0047 - mae: 0.0313 - mse: 0.0047 - val_loss: 0.0077 - val_mae: 0.0400 - val_mse: 0.0077\n",
      "Epoch 53/500\n",
      "4222/4222 [==============================] - 1s 309us/step - loss: 0.0044 - mae: 0.0304 - mse: 0.0044 - val_loss: 0.0076 - val_mae: 0.0394 - val_mse: 0.0076\n",
      "Epoch 54/500\n",
      "4222/4222 [==============================] - 1s 279us/step - loss: 0.0042 - mae: 0.0297 - mse: 0.0042 - val_loss: 0.0073 - val_mae: 0.0383 - val_mse: 0.0073\n",
      "Epoch 55/500\n",
      "4222/4222 [==============================] - 1s 319us/step - loss: 0.0041 - mae: 0.0294 - mse: 0.0041 - val_loss: 0.0066 - val_mae: 0.0358 - val_mse: 0.0066\n",
      "Epoch 56/500\n",
      "4222/4222 [==============================] - 1s 311us/step - loss: 0.0041 - mae: 0.0295 - mse: 0.0041 - val_loss: 0.0053 - val_mae: 0.0321 - val_mse: 0.0053\n",
      "Epoch 57/500\n",
      "4222/4222 [==============================] - 1s 295us/step - loss: 0.0042 - mae: 0.0302 - mse: 0.0042 - val_loss: 0.0047 - val_mae: 0.0336 - val_mse: 0.0047\n",
      "Epoch 58/500\n",
      "4222/4222 [==============================] - 1s 285us/step - loss: 0.0050 - mae: 0.0334 - mse: 0.0050 - val_loss: 0.0049 - val_mae: 0.0333 - val_mse: 0.0049\n",
      "Epoch 59/500\n",
      "4222/4222 [==============================] - 1s 273us/step - loss: 0.0040 - mae: 0.0290 - mse: 0.0040 - val_loss: 0.0047 - val_mae: 0.0339 - val_mse: 0.0047\n",
      "Epoch 60/500\n",
      "4222/4222 [==============================] - 1s 281us/step - loss: 0.0035 - mae: 0.0274 - mse: 0.0035 - val_loss: 0.0049 - val_mae: 0.0349 - val_mse: 0.0049\n",
      "Epoch 61/500\n",
      "4222/4222 [==============================] - 1s 287us/step - loss: 0.0034 - mae: 0.0272 - mse: 0.0034 - val_loss: 0.0049 - val_mae: 0.0346 - val_mse: 0.0049\n",
      "Epoch 62/500\n",
      "4222/4222 [==============================] - 1s 274us/step - loss: 0.0033 - mae: 0.0267 - mse: 0.0033 - val_loss: 0.0048 - val_mae: 0.0340 - val_mse: 0.0048\n",
      "Epoch 63/500\n",
      "4222/4222 [==============================] - 1s 279us/step - loss: 0.0033 - mae: 0.0263 - mse: 0.0033 - val_loss: 0.0046 - val_mae: 0.0333 - val_mse: 0.0046\n",
      "Epoch 64/500\n",
      "4222/4222 [==============================] - 1s 279us/step - loss: 0.0032 - mae: 0.0258 - mse: 0.0032 - val_loss: 0.0045 - val_mae: 0.0328 - val_mse: 0.0045\n",
      "Epoch 65/500\n",
      "4222/4222 [==============================] - 1s 295us/step - loss: 0.0031 - mae: 0.0253 - mse: 0.0031 - val_loss: 0.0044 - val_mae: 0.0321 - val_mse: 0.0044\n",
      "Epoch 66/500\n",
      "4222/4222 [==============================] - 1s 297us/step - loss: 0.0031 - mae: 0.0250 - mse: 0.0031 - val_loss: 0.0043 - val_mae: 0.0315 - val_mse: 0.0043\n",
      "Epoch 67/500\n",
      "4222/4222 [==============================] - 1s 304us/step - loss: 0.0030 - mae: 0.0245 - mse: 0.0030 - val_loss: 0.0041 - val_mae: 0.0302 - val_mse: 0.0041\n",
      "Epoch 68/500\n",
      "4222/4222 [==============================] - 1s 307us/step - loss: 0.0029 - mae: 0.0243 - mse: 0.0029 - val_loss: 0.0040 - val_mae: 0.0293 - val_mse: 0.0040\n",
      "Epoch 69/500\n",
      "4222/4222 [==============================] - 1s 279us/step - loss: 0.0029 - mae: 0.0238 - mse: 0.0029 - val_loss: 0.0037 - val_mae: 0.0275 - val_mse: 0.0037\n",
      "Epoch 70/500\n",
      "4222/4222 [==============================] - 1s 281us/step - loss: 0.0029 - mae: 0.0243 - mse: 0.0029 - val_loss: 0.0038 - val_mae: 0.0274 - val_mse: 0.0038\n",
      "Epoch 71/500\n",
      "4222/4222 [==============================] - 1s 298us/step - loss: 0.0027 - mae: 0.0233 - mse: 0.0027 - val_loss: 0.0033 - val_mae: 0.0261 - val_mse: 0.0033\n",
      "Epoch 72/500\n",
      "4222/4222 [==============================] - 1s 299us/step - loss: 0.0029 - mae: 0.0242 - mse: 0.0029 - val_loss: 0.0036 - val_mae: 0.0267 - val_mse: 0.0036\n",
      "Epoch 73/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0027 - mae: 0.0228 - mse: 0.0027 - val_loss: 0.0030 - val_mae: 0.0259 - val_mse: 0.0030\n",
      "Epoch 74/500\n",
      "4222/4222 [==============================] - 1s 291us/step - loss: 0.0027 - mae: 0.0237 - mse: 0.0027 - val_loss: 0.0034 - val_mae: 0.0267 - val_mse: 0.0034\n",
      "Epoch 75/500\n",
      "4222/4222 [==============================] - 1s 287us/step - loss: 0.0026 - mae: 0.0223 - mse: 0.0026 - val_loss: 0.0029 - val_mae: 0.0261 - val_mse: 0.0029\n",
      "Epoch 76/500\n",
      "4222/4222 [==============================] - 1s 284us/step - loss: 0.0026 - mae: 0.0229 - mse: 0.0026 - val_loss: 0.0033 - val_mae: 0.0265 - val_mse: 0.0033\n",
      "Epoch 77/500\n",
      "4222/4222 [==============================] - 1s 295us/step - loss: 0.0025 - mae: 0.0218 - mse: 0.0025 - val_loss: 0.0029 - val_mae: 0.0257 - val_mse: 0.0029\n",
      "Epoch 78/500\n",
      "4222/4222 [==============================] - 1s 303us/step - loss: 0.0025 - mae: 0.0224 - mse: 0.0025 - val_loss: 0.0032 - val_mae: 0.0262 - val_mse: 0.0032\n",
      "Epoch 79/500\n",
      "4222/4222 [==============================] - 1s 283us/step - loss: 0.0024 - mae: 0.0213 - mse: 0.0024 - val_loss: 0.0028 - val_mae: 0.0252 - val_mse: 0.0028\n",
      "Epoch 80/500\n",
      "4222/4222 [==============================] - 1s 288us/step - loss: 0.0024 - mae: 0.0217 - mse: 0.0024 - val_loss: 0.0031 - val_mae: 0.0255 - val_mse: 0.0031\n",
      "Epoch 81/500\n",
      "4222/4222 [==============================] - 1s 302us/step - loss: 0.0023 - mae: 0.0207 - mse: 0.0023 - val_loss: 0.0028 - val_mae: 0.0247 - val_mse: 0.0028\n",
      "Epoch 82/500\n",
      "4222/4222 [==============================] - 1s 300us/step - loss: 0.0023 - mae: 0.0210 - mse: 0.0023 - val_loss: 0.0030 - val_mae: 0.0247 - val_mse: 0.0030\n",
      "Epoch 83/500\n",
      "4222/4222 [==============================] - 1s 296us/step - loss: 0.0022 - mae: 0.0202 - mse: 0.0022 - val_loss: 0.0027 - val_mae: 0.0249 - val_mse: 0.0027\n",
      "Epoch 84/500\n",
      "4222/4222 [==============================] - 1s 290us/step - loss: 0.0022 - mae: 0.0205 - mse: 0.0022 - val_loss: 0.0029 - val_mae: 0.0250 - val_mse: 0.0029\n",
      "Epoch 85/500\n",
      "4222/4222 [==============================] - 1s 291us/step - loss: 0.0021 - mae: 0.0198 - mse: 0.0021 - val_loss: 0.0028 - val_mae: 0.0254 - val_mse: 0.0028\n",
      "Epoch 86/500\n",
      "4222/4222 [==============================] - 1s 301us/step - loss: 0.0021 - mae: 0.0202 - mse: 0.0021 - val_loss: 0.0029 - val_mae: 0.0257 - val_mse: 0.0029\n",
      "Epoch 87/500\n",
      "4222/4222 [==============================] - 1s 275us/step - loss: 0.0020 - mae: 0.0196 - mse: 0.0020 - val_loss: 0.0028 - val_mae: 0.0252 - val_mse: 0.0028\n",
      "Epoch 88/500\n",
      "4222/4222 [==============================] - 1s 280us/step - loss: 0.0020 - mae: 0.0198 - mse: 0.0020 - val_loss: 0.0029 - val_mae: 0.0256 - val_mse: 0.0029\n",
      "Epoch 89/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0020 - mae: 0.0194 - mse: 0.0020 - val_loss: 0.0028 - val_mae: 0.0252 - val_mse: 0.0028\n",
      "Epoch 90/500\n",
      "4222/4222 [==============================] - 1s 284us/step - loss: 0.0020 - mae: 0.0194 - mse: 0.0020 - val_loss: 0.0030 - val_mae: 0.0252 - val_mse: 0.0030\n",
      "Epoch 91/500\n",
      "4222/4222 [==============================] - 1s 299us/step - loss: 0.0019 - mae: 0.0190 - mse: 0.0019 - val_loss: 0.0029 - val_mae: 0.0250 - val_mse: 0.0029\n",
      "Epoch 92/500\n",
      "4222/4222 [==============================] - 1s 286us/step - loss: 0.0020 - mae: 0.0191 - mse: 0.0020 - val_loss: 0.0030 - val_mae: 0.0249 - val_mse: 0.0030\n",
      "Epoch 93/500\n",
      "4222/4222 [==============================] - 1s 348us/step - loss: 0.0019 - mae: 0.0188 - mse: 0.0019 - val_loss: 0.0029 - val_mae: 0.0246 - val_mse: 0.0029\n",
      "Epoch 94/500\n",
      "4222/4222 [==============================] - 1s 281us/step - loss: 0.0019 - mae: 0.0190 - mse: 0.0019 - val_loss: 0.0030 - val_mae: 0.0245 - val_mse: 0.0030\n",
      "Epoch 95/500\n",
      "4222/4222 [==============================] - 2s 387us/step - loss: 0.0019 - mae: 0.0188 - mse: 0.0019 - val_loss: 0.0029 - val_mae: 0.0243 - val_mse: 0.0029\n",
      "Epoch 96/500\n",
      "4222/4222 [==============================] - 2s 433us/step - loss: 0.0019 - mae: 0.0189 - mse: 0.0019 - val_loss: 0.0030 - val_mae: 0.0244 - val_mse: 0.0030\n",
      "Epoch 97/500\n",
      "4222/4222 [==============================] - 3s 615us/step - loss: 0.0019 - mae: 0.0185 - mse: 0.0019 - val_loss: 0.0029 - val_mae: 0.0241 - val_mse: 0.0029\n",
      "Epoch 98/500\n",
      "4222/4222 [==============================] - 2s 513us/step - loss: 0.0019 - mae: 0.0187 - mse: 0.0019 - val_loss: 0.0030 - val_mae: 0.0244 - val_mse: 0.0030\n",
      "Epoch 99/500\n",
      "4222/4222 [==============================] - 2s 461us/step - loss: 0.0018 - mae: 0.0184 - mse: 0.0018 - val_loss: 0.0029 - val_mae: 0.0240 - val_mse: 0.0029\n",
      "Epoch 100/500\n",
      "4222/4222 [==============================] - 2s 382us/step - loss: 0.0018 - mae: 0.0186 - mse: 0.0018 - val_loss: 0.0031 - val_mae: 0.0246 - val_mse: 0.0031\n",
      "Epoch 101/500\n",
      "4222/4222 [==============================] - 1s 350us/step - loss: 0.0018 - mae: 0.0181 - mse: 0.0018 - val_loss: 0.0030 - val_mae: 0.0241 - val_mse: 0.0030\n",
      "Epoch 102/500\n",
      "4222/4222 [==============================] - 2s 358us/step - loss: 0.0018 - mae: 0.0188 - mse: 0.0018 - val_loss: 0.0031 - val_mae: 0.0245 - val_mse: 0.0031\n",
      "Epoch 103/500\n",
      "4222/4222 [==============================] - 2s 421us/step - loss: 0.0018 - mae: 0.0181 - mse: 0.0018 - val_loss: 0.0030 - val_mae: 0.0242 - val_mse: 0.0030\n",
      "Epoch 104/500\n",
      "4222/4222 [==============================] - 1s 350us/step - loss: 0.0018 - mae: 0.0183 - mse: 0.0018 - val_loss: 0.0030 - val_mae: 0.0245 - val_mse: 0.0030\n",
      "Epoch 105/500\n",
      "4222/4222 [==============================] - 2s 437us/step - loss: 0.0018 - mae: 0.0182 - mse: 0.0018 - val_loss: 0.0030 - val_mae: 0.0239 - val_mse: 0.0030\n",
      "Epoch 106/500\n",
      "4222/4222 [==============================] - 1s 314us/step - loss: 0.0018 - mae: 0.0187 - mse: 0.0018 - val_loss: 0.0030 - val_mae: 0.0233 - val_mse: 0.0030\n",
      "Epoch 107/500\n",
      "4222/4222 [==============================] - 1s 305us/step - loss: 0.0017 - mae: 0.0179 - mse: 0.0017 - val_loss: 0.0030 - val_mae: 0.0227 - val_mse: 0.0030\n",
      "Epoch 108/500\n",
      "4222/4222 [==============================] - 1s 291us/step - loss: 0.0017 - mae: 0.0176 - mse: 0.0017 - val_loss: 0.0031 - val_mae: 0.0230 - val_mse: 0.0031\n",
      "Epoch 109/500\n",
      "4222/4222 [==============================] - 1s 302us/step - loss: 0.0015 - mae: 0.0173 - mse: 0.0015 - val_loss: 0.0030 - val_mae: 0.0228 - val_mse: 0.0030\n",
      "Epoch 110/500\n",
      "4222/4222 [==============================] - 1s 298us/step - loss: 0.0020 - mae: 0.0194 - mse: 0.0020 - val_loss: 0.0033 - val_mae: 0.0241 - val_mse: 0.0033\n",
      "Epoch 111/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0017 - mae: 0.0182 - mse: 0.0017 - val_loss: 0.0031 - val_mae: 0.0238 - val_mse: 0.0031\n",
      "Epoch 112/500\n",
      "4222/4222 [==============================] - 1s 284us/step - loss: 0.0016 - mae: 0.0174 - mse: 0.0016 - val_loss: 0.0031 - val_mae: 0.0234 - val_mse: 0.0031\n",
      "Epoch 113/500\n",
      "4222/4222 [==============================] - 1s 281us/step - loss: 0.0015 - mae: 0.0170 - mse: 0.0015 - val_loss: 0.0031 - val_mae: 0.0234 - val_mse: 0.0031\n",
      "Epoch 114/500\n",
      "4222/4222 [==============================] - 1s 282us/step - loss: 0.0019 - mae: 0.0192 - mse: 0.0019 - val_loss: 0.0031 - val_mae: 0.0224 - val_mse: 0.0031\n",
      "Epoch 115/500\n",
      "4222/4222 [==============================] - 1s 290us/step - loss: 0.0017 - mae: 0.0181 - mse: 0.0017 - val_loss: 0.0030 - val_mae: 0.0222 - val_mse: 0.0030\n",
      "Epoch 116/500\n",
      "4222/4222 [==============================] - 1s 293us/step - loss: 0.0020 - mae: 0.0187 - mse: 0.0020 - val_loss: 0.0026 - val_mae: 0.0228 - val_mse: 0.0026\n",
      "Epoch 117/500\n",
      "4222/4222 [==============================] - 1s 292us/step - loss: 0.0019 - mae: 0.0185 - mse: 0.0019 - val_loss: 0.0030 - val_mae: 0.0223 - val_mse: 0.0030\n",
      "Epoch 118/500\n",
      "4222/4222 [==============================] - 1s 287us/step - loss: 0.0014 - mae: 0.0169 - mse: 0.0014 - val_loss: 0.0028 - val_mae: 0.0225 - val_mse: 0.0028\n",
      "Epoch 119/500\n",
      "4222/4222 [==============================] - 1s 306us/step - loss: 0.0020 - mae: 0.0189 - mse: 0.0020 - val_loss: 0.0030 - val_mae: 0.0216 - val_mse: 0.0030\n",
      "Epoch 120/500\n",
      "4222/4222 [==============================] - 1s 286us/step - loss: 0.0014 - mae: 0.0164 - mse: 0.0014 - val_loss: 0.0030 - val_mae: 0.0227 - val_mse: 0.0030\n",
      "Epoch 121/500\n",
      "4222/4222 [==============================] - 1s 302us/step - loss: 0.0019 - mae: 0.0186 - mse: 0.0019 - val_loss: 0.0030 - val_mae: 0.0226 - val_mse: 0.0030\n",
      "Epoch 122/500\n",
      "4222/4222 [==============================] - 1s 282us/step - loss: 0.0017 - mae: 0.0189 - mse: 0.0017 - val_loss: 0.0032 - val_mae: 0.0222 - val_mse: 0.0032\n",
      "Epoch 123/500\n",
      "4222/4222 [==============================] - 1s 284us/step - loss: 0.0018 - mae: 0.0182 - mse: 0.0018 - val_loss: 0.0026 - val_mae: 0.0210 - val_mse: 0.0026\n",
      "Epoch 124/500\n",
      "4222/4222 [==============================] - 1s 337us/step - loss: 0.0015 - mae: 0.0163 - mse: 0.0015 - val_loss: 0.0030 - val_mae: 0.0218 - val_mse: 0.0030\n",
      "Epoch 125/500\n",
      "4222/4222 [==============================] - 1s 300us/step - loss: 0.0015 - mae: 0.0161 - mse: 0.0015 - val_loss: 0.0029 - val_mae: 0.0223 - val_mse: 0.0029\n",
      "Epoch 126/500\n",
      "4222/4222 [==============================] - 1s 301us/step - loss: 0.0015 - mae: 0.0167 - mse: 0.0015 - val_loss: 0.0031 - val_mae: 0.0222 - val_mse: 0.0031\n",
      "Epoch 127/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0016 - mae: 0.0166 - mse: 0.0016 - val_loss: 0.0029 - val_mae: 0.0223 - val_mse: 0.0029\n",
      "Epoch 128/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0014 - mae: 0.0161 - mse: 0.0014 - val_loss: 0.0031 - val_mae: 0.0224 - val_mse: 0.0031\n",
      "Epoch 129/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0015 - mae: 0.0161 - mse: 0.0015 - val_loss: 0.0031 - val_mae: 0.0226 - val_mse: 0.0031\n",
      "Epoch 130/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0013 - mae: 0.0158 - mse: 0.0013 - val_loss: 0.0031 - val_mae: 0.0219 - val_mse: 0.0031\n",
      "Epoch 131/500\n",
      "4222/4222 [==============================] - 1s 296us/step - loss: 0.0020 - mae: 0.0189 - mse: 0.0020 - val_loss: 0.0032 - val_mae: 0.0214 - val_mse: 0.0032\n",
      "Epoch 132/500\n",
      "4222/4222 [==============================] - 1s 305us/step - loss: 0.0013 - mae: 0.0161 - mse: 0.0013 - val_loss: 0.0030 - val_mae: 0.0218 - val_mse: 0.0030\n",
      "Epoch 133/500\n",
      "4222/4222 [==============================] - 1s 306us/step - loss: 0.0020 - mae: 0.0192 - mse: 0.0020 - val_loss: 0.0031 - val_mae: 0.0233 - val_mse: 0.0031\n",
      "Epoch 134/500\n",
      "4222/4222 [==============================] - 1s 290us/step - loss: 0.0015 - mae: 0.0173 - mse: 0.0015 - val_loss: 0.0028 - val_mae: 0.0210 - val_mse: 0.0028\n",
      "Epoch 135/500\n",
      "4222/4222 [==============================] - 1s 287us/step - loss: 0.0026 - mae: 0.0226 - mse: 0.0026 - val_loss: 0.0026 - val_mae: 0.0236 - val_mse: 0.0026\n",
      "Epoch 136/500\n",
      "4222/4222 [==============================] - 1s 290us/step - loss: 0.0017 - mae: 0.0186 - mse: 0.0017 - val_loss: 0.0025 - val_mae: 0.0205 - val_mse: 0.0025\n",
      "Epoch 137/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0014 - mae: 0.0159 - mse: 0.0014 - val_loss: 0.0027 - val_mae: 0.0211 - val_mse: 0.0027\n",
      "Epoch 138/500\n",
      "4222/4222 [==============================] - 1s 296us/step - loss: 0.0014 - mae: 0.0157 - mse: 0.0014 - val_loss: 0.0028 - val_mae: 0.0206 - val_mse: 0.0028\n",
      "Epoch 139/500\n",
      "4222/4222 [==============================] - 1s 301us/step - loss: 0.0012 - mae: 0.0148 - mse: 0.0012 - val_loss: 0.0027 - val_mae: 0.0204 - val_mse: 0.0027\n",
      "Epoch 140/500\n",
      "4222/4222 [==============================] - 1s 291us/step - loss: 0.0018 - mae: 0.0174 - mse: 0.0018 - val_loss: 0.0031 - val_mae: 0.0203 - val_mse: 0.0031\n",
      "Epoch 141/500\n",
      "4222/4222 [==============================] - 1s 294us/step - loss: 0.0013 - mae: 0.0155 - mse: 0.0013 - val_loss: 0.0026 - val_mae: 0.0200 - val_mse: 0.0026\n",
      "Epoch 142/500\n",
      "4222/4222 [==============================] - 1s 291us/step - loss: 0.0020 - mae: 0.0187 - mse: 0.0020 - val_loss: 0.0032 - val_mae: 0.0204 - val_mse: 0.0032\n",
      "Epoch 143/500\n",
      "4222/4222 [==============================] - 1s 289us/step - loss: 0.0013 - mae: 0.0153 - mse: 0.0013 - val_loss: 0.0027 - val_mae: 0.0205 - val_mse: 0.0027\n",
      "Epoch 144/500\n",
      "4222/4222 [==============================] - 1s 307us/step - loss: 0.0019 - mae: 0.0181 - mse: 0.0019 - val_loss: 0.0030 - val_mae: 0.0205 - val_mse: 0.0030\n",
      "Epoch 145/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4222/4222 [==============================] - 1s 283us/step - loss: 0.0013 - mae: 0.0152 - mse: 0.0013 - val_loss: 0.0026 - val_mae: 0.0199 - val_mse: 0.0026\n",
      "Epoch 146/500\n",
      "4222/4222 [==============================] - 1s 286us/step - loss: 0.0020 - mae: 0.0189 - mse: 0.0020 - val_loss: 0.0032 - val_mae: 0.0205 - val_mse: 0.0032\n",
      "Epoch 147/500\n",
      "4222/4222 [==============================] - 1s 291us/step - loss: 0.0012 - mae: 0.0151 - mse: 0.0012 - val_loss: 0.0026 - val_mae: 0.0205 - val_mse: 0.0026\n",
      "Epoch 148/500\n",
      "4222/4222 [==============================] - 1s 333us/step - loss: 0.0019 - mae: 0.0184 - mse: 0.0019 - val_loss: 0.0027 - val_mae: 0.0201 - val_mse: 0.0027\n",
      "Epoch 149/500\n",
      "4222/4222 [==============================] - 1s 285us/step - loss: 0.0013 - mae: 0.0149 - mse: 0.0013 - val_loss: 0.0026 - val_mae: 0.0209 - val_mse: 0.0026\n",
      "Epoch 150/500\n",
      "4222/4222 [==============================] - 1s 299us/step - loss: 0.0019 - mae: 0.0187 - mse: 0.0019 - val_loss: 0.0032 - val_mae: 0.0206 - val_mse: 0.0032\n",
      "Epoch 151/500\n",
      "4222/4222 [==============================] - 1s 279us/step - loss: 0.0013 - mae: 0.0152 - mse: 0.0013 - val_loss: 0.0024 - val_mae: 0.0198 - val_mse: 0.0024\n",
      "Epoch 152/500\n",
      "4222/4222 [==============================] - 1s 301us/step - loss: 0.0020 - mae: 0.0188 - mse: 0.0020 - val_loss: 0.0029 - val_mae: 0.0204 - val_mse: 0.0029\n",
      "Epoch 153/500\n",
      "4222/4222 [==============================] - 1s 275us/step - loss: 0.0012 - mae: 0.0143 - mse: 0.0012 - val_loss: 0.0027 - val_mae: 0.0211 - val_mse: 0.0027\n",
      "Epoch 154/500\n",
      "4222/4222 [==============================] - 1s 312us/step - loss: 0.0019 - mae: 0.0183 - mse: 0.0019 - val_loss: 0.0028 - val_mae: 0.0205 - val_mse: 0.0028\n",
      "Epoch 155/500\n",
      "4222/4222 [==============================] - 1s 301us/step - loss: 0.0022 - mae: 0.0200 - mse: 0.0022 - val_loss: 0.0025 - val_mae: 0.0205 - val_mse: 0.0025\n",
      "Epoch 156/500\n",
      "4222/4222 [==============================] - 1s 326us/step - loss: 0.0026 - mae: 0.0221 - mse: 0.0026 - val_loss: 0.0024 - val_mae: 0.0206 - val_mse: 0.0024\n",
      "Epoch 157/500\n",
      "4222/4222 [==============================] - 1s 330us/step - loss: 0.0015 - mae: 0.0166 - mse: 0.0015 - val_loss: 0.0026 - val_mae: 0.0202 - val_mse: 0.0026\n",
      "Epoch 158/500\n",
      "4222/4222 [==============================] - 1s 306us/step - loss: 0.0019 - mae: 0.0178 - mse: 0.0019 - val_loss: 0.0030 - val_mae: 0.0208 - val_mse: 0.0030\n",
      "Epoch 159/500\n",
      "4222/4222 [==============================] - 1s 325us/step - loss: 0.0013 - mae: 0.0152 - mse: 0.0013 - val_loss: 0.0025 - val_mae: 0.0201 - val_mse: 0.0025\n",
      "Epoch 160/500\n",
      "4222/4222 [==============================] - 1s 299us/step - loss: 0.0019 - mae: 0.0179 - mse: 0.0019 - val_loss: 0.0031 - val_mae: 0.0209 - val_mse: 0.0031\n",
      "Epoch 161/500\n",
      "4222/4222 [==============================] - 1s 331us/step - loss: 0.0013 - mae: 0.0152 - mse: 0.0013 - val_loss: 0.0024 - val_mae: 0.0194 - val_mse: 0.0024\n",
      "Epoch 162/500\n",
      "4222/4222 [==============================] - 1s 313us/step - loss: 0.0020 - mae: 0.0187 - mse: 0.0020 - val_loss: 0.0028 - val_mae: 0.0199 - val_mse: 0.0028\n",
      "Epoch 163/500\n",
      "4222/4222 [==============================] - 1s 298us/step - loss: 0.0013 - mae: 0.0151 - mse: 0.0013 - val_loss: 0.0022 - val_mae: 0.0190 - val_mse: 0.0022\n",
      "Epoch 164/500\n",
      "4222/4222 [==============================] - 1s 302us/step - loss: 0.0019 - mae: 0.0180 - mse: 0.0019 - val_loss: 0.0024 - val_mae: 0.0195 - val_mse: 0.0024\n",
      "Epoch 165/500\n",
      "4222/4222 [==============================] - 1s 297us/step - loss: 0.0012 - mae: 0.0147 - mse: 0.0012 - val_loss: 0.0024 - val_mae: 0.0190 - val_mse: 0.0024\n",
      "Epoch 166/500\n",
      "4222/4222 [==============================] - 1s 292us/step - loss: 0.0019 - mae: 0.0184 - mse: 0.0019 - val_loss: 0.0026 - val_mae: 0.0196 - val_mse: 0.0026\n",
      "Epoch 167/500\n",
      "4222/4222 [==============================] - 1s 309us/step - loss: 0.0013 - mae: 0.0151 - mse: 0.0013 - val_loss: 0.0020 - val_mae: 0.0186 - val_mse: 0.0020\n",
      "Epoch 168/500\n",
      "4222/4222 [==============================] - 1s 298us/step - loss: 0.0018 - mae: 0.0178 - mse: 0.0018 - val_loss: 0.0024 - val_mae: 0.0199 - val_mse: 0.0024\n",
      "Epoch 169/500\n",
      "4222/4222 [==============================] - 1s 307us/step - loss: 0.0013 - mae: 0.0153 - mse: 0.0013 - val_loss: 0.0023 - val_mae: 0.0191 - val_mse: 0.0023\n",
      "Epoch 170/500\n",
      "4222/4222 [==============================] - 1s 300us/step - loss: 0.0019 - mae: 0.0186 - mse: 0.0019 - val_loss: 0.0027 - val_mae: 0.0201 - val_mse: 0.0027\n",
      "Epoch 171/500\n",
      "4222/4222 [==============================] - 1s 318us/step - loss: 0.0013 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0020 - val_mae: 0.0188 - val_mse: 0.0020\n",
      "Epoch 172/500\n",
      "4222/4222 [==============================] - 1s 341us/step - loss: 0.0018 - mae: 0.0177 - mse: 0.0018 - val_loss: 0.0023 - val_mae: 0.0193 - val_mse: 0.0023\n",
      "Epoch 173/500\n",
      "4222/4222 [==============================] - 1s 323us/step - loss: 0.0012 - mae: 0.0149 - mse: 0.0012 - val_loss: 0.0023 - val_mae: 0.0190 - val_mse: 0.0023\n",
      "Epoch 174/500\n",
      "3680/4222 [=========================>....] - ETA: 0s - loss: 0.0021 - mae: 0.0195 - mse: 0.0021"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-8da166ed9ccf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                        \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_LSTM_nomob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_LSTM_nomob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                        \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                        shuffle=False)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "X_LSTM_nomob, y_LSTM_nomob = get_LSTM_states_dataset(state_cases_df, n_steps=N_STEPS)\n",
    "X_train_LSTM_nomob, X_test_LSTM_nomob, y_train_LSTM_nomob, y_test_LSTM_nomob = train_test_split(X_LSTM_nomob, y_LSTM_nomob, test_size=SPLIT, shuffle=False)\n",
    "\n",
    "# Set up model\n",
    "n_lstm_features = X_train_LSTM_nomob.shape[2]\n",
    "lstm_nomob = get_lstm(N_STEPS, n_lstm_features)\n",
    "\n",
    "# Fit LSTM\n",
    "lstm_nomob_history = lstm_nomob.fit(X_train_LSTM_nomob, y_train_LSTM_nomob,\n",
    "                                     epochs=EPOCHS,\n",
    "                                     validation_data=(X_test_LSTM_nomob, y_test_LSTM_nomob),\n",
    "                                     verbose=1,\n",
    "                                     shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_nomob_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-157-4ecad2a12268>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Graph LSTM Error, mae and mse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_nomob_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_nomob_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lstm_nomob_history' is not defined"
     ]
    }
   ],
   "source": [
    "# Graph LSTM Error, mae and mse\n",
    "plt.plot(lstm_nomob_history.history['loss'])\n",
    "plt.plot(lstm_nomob_history.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STACKED LSTM Model with Mobility Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Stacked LSTM, Boo's model\n",
    "def get_stacked_lstm(n_steps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "    model.add(LSTM(128, input_shape=(n_steps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = 'adam', loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "n_lstm_features = X_train_LSTM.shape[2]\n",
    "slstm = get_stacked_lstm(N_STEPS, n_lstm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2440 samples, validate on 610 samples\n",
      "Epoch 1/200\n",
      "2440/2440 [==============================] - 2s 785us/step - loss: 0.0174 - mae: 0.0798 - mse: 0.0174 - val_loss: 0.0022 - val_mae: 0.0349 - val_mse: 0.0022\n",
      "Epoch 2/200\n",
      "2440/2440 [==============================] - 1s 543us/step - loss: 0.0022 - mae: 0.0343 - mse: 0.0022 - val_loss: 0.0024 - val_mae: 0.0412 - val_mse: 0.0024\n",
      "Epoch 3/200\n",
      "2440/2440 [==============================] - 1s 549us/step - loss: 0.0017 - mae: 0.0310 - mse: 0.0017 - val_loss: 0.0015 - val_mae: 0.0299 - val_mse: 0.0015\n",
      "Epoch 4/200\n",
      "2440/2440 [==============================] - 1s 562us/step - loss: 0.0015 - mae: 0.0279 - mse: 0.0015 - val_loss: 0.0013 - val_mae: 0.0278 - val_mse: 0.0013\n",
      "Epoch 5/200\n",
      "2440/2440 [==============================] - 1s 537us/step - loss: 0.0014 - mae: 0.0272 - mse: 0.0014 - val_loss: 0.0015 - val_mae: 0.0328 - val_mse: 0.0015\n",
      "Epoch 6/200\n",
      "2440/2440 [==============================] - 1s 538us/step - loss: 0.0013 - mae: 0.0256 - mse: 0.0013 - val_loss: 0.0014 - val_mae: 0.0322 - val_mse: 0.0014\n",
      "Epoch 7/200\n",
      "2440/2440 [==============================] - 1s 536us/step - loss: 0.0012 - mae: 0.0245 - mse: 0.0012 - val_loss: 0.0013 - val_mae: 0.0297 - val_mse: 0.0013\n",
      "Epoch 8/200\n",
      "2440/2440 [==============================] - 1s 554us/step - loss: 0.0011 - mae: 0.0238 - mse: 0.0011 - val_loss: 0.0011 - val_mae: 0.0262 - val_mse: 0.0011\n",
      "Epoch 9/200\n",
      "2440/2440 [==============================] - 1s 539us/step - loss: 0.0011 - mae: 0.0234 - mse: 0.0011 - val_loss: 8.5337e-04 - val_mae: 0.0215 - val_mse: 8.5337e-04\n",
      "Epoch 10/200\n",
      "2440/2440 [==============================] - 1s 549us/step - loss: 0.0010 - mae: 0.0230 - mse: 0.0010 - val_loss: 7.2569e-04 - val_mae: 0.0169 - val_mse: 7.2569e-04\n",
      "Epoch 11/200\n",
      "2440/2440 [==============================] - ETA: 0s - loss: 9.9261e-04 - mae: 0.0230 - mse: 9.9261e-0 - 1s 544us/step - loss: 9.9252e-04 - mae: 0.0230 - mse: 9.9252e-04 - val_loss: 7.3569e-04 - val_mae: 0.0186 - val_mse: 7.3569e-04\n",
      "Epoch 12/200\n",
      "2440/2440 [==============================] - 1s 544us/step - loss: 9.9194e-04 - mae: 0.0238 - mse: 9.9194e-04 - val_loss: 7.7677e-04 - val_mae: 0.0206 - val_mse: 7.7677e-04\n",
      "Epoch 13/200\n",
      "2440/2440 [==============================] - 1s 543us/step - loss: 9.3702e-04 - mae: 0.0234 - mse: 9.3702e-04 - val_loss: 6.2246e-04 - val_mae: 0.0161 - val_mse: 6.2246e-04\n",
      "Epoch 14/200\n",
      "2440/2440 [==============================] - 1s 559us/step - loss: 7.8055e-04 - mae: 0.0204 - mse: 7.8055e-04 - val_loss: 6.0066e-04 - val_mae: 0.0174 - val_mse: 6.0066e-04\n",
      "Epoch 15/200\n",
      "2440/2440 [==============================] - 1s 547us/step - loss: 7.0310e-04 - mae: 0.0192 - mse: 7.0310e-04 - val_loss: 5.5662e-04 - val_mae: 0.0166 - val_mse: 5.5662e-04\n",
      "Epoch 16/200\n",
      "2440/2440 [==============================] - 1s 535us/step - loss: 6.5138e-04 - mae: 0.0183 - mse: 6.5138e-04 - val_loss: 5.2970e-04 - val_mae: 0.0164 - val_mse: 5.2970e-04\n",
      "Epoch 17/200\n",
      "2440/2440 [==============================] - 1s 545us/step - loss: 6.0327e-04 - mae: 0.0175 - mse: 6.0327e-04 - val_loss: 5.0735e-04 - val_mae: 0.0162 - val_mse: 5.0735e-04\n",
      "Epoch 18/200\n",
      "2440/2440 [==============================] - 1s 540us/step - loss: 5.6004e-04 - mae: 0.0167 - mse: 5.6004e-04 - val_loss: 4.8954e-04 - val_mae: 0.0161 - val_mse: 4.8954e-04\n",
      "Epoch 19/200\n",
      "2440/2440 [==============================] - 1s 575us/step - loss: 5.2100e-04 - mae: 0.0160 - mse: 5.2100e-04 - val_loss: 4.7384e-04 - val_mae: 0.0160 - val_mse: 4.7384e-04\n",
      "Epoch 20/200\n",
      "2440/2440 [==============================] - 1s 553us/step - loss: 4.8647e-04 - mae: 0.0154 - mse: 4.8647e-04 - val_loss: 4.5874e-04 - val_mae: 0.0159 - val_mse: 4.5874e-04\n",
      "Epoch 21/200\n",
      "2440/2440 [==============================] - 1s 541us/step - loss: 4.5666e-04 - mae: 0.0148 - mse: 4.5666e-04 - val_loss: 4.4327e-04 - val_mae: 0.0157 - val_mse: 4.4327e-04\n",
      "Epoch 22/200\n",
      "2440/2440 [==============================] - 1s 608us/step - loss: 4.3159e-04 - mae: 0.0144 - mse: 4.3159e-04 - val_loss: 4.2706e-04 - val_mae: 0.0154 - val_mse: 4.2706e-04\n",
      "Epoch 23/200\n",
      "2440/2440 [==============================] - 2s 622us/step - loss: 4.1115e-04 - mae: 0.0140 - mse: 4.1115e-04 - val_loss: 4.1012e-04 - val_mae: 0.0150 - val_mse: 4.1012e-04\n",
      "Epoch 24/200\n",
      "2440/2440 [==============================] - 1s 609us/step - loss: 3.9503e-04 - mae: 0.0138 - mse: 3.9503e-04 - val_loss: 3.9268e-04 - val_mae: 0.0146 - val_mse: 3.9268e-04\n",
      "Epoch 25/200\n",
      "2440/2440 [==============================] - 2s 630us/step - loss: 3.8265e-04 - mae: 0.0136 - mse: 3.8265e-04 - val_loss: 3.7516e-04 - val_mae: 0.0141 - val_mse: 3.7516e-04\n",
      "Epoch 26/200\n",
      "2440/2440 [==============================] - 1s 614us/step - loss: 3.7304e-04 - mae: 0.0135 - mse: 3.7304e-04 - val_loss: 3.5832e-04 - val_mae: 0.0136 - val_mse: 3.5832e-04\n",
      "Epoch 27/200\n",
      "2440/2440 [==============================] - 2s 652us/step - loss: 3.6480e-04 - mae: 0.0134 - mse: 3.6480e-04 - val_loss: 3.4311e-04 - val_mae: 0.0131 - val_mse: 3.4311e-04\n",
      "Epoch 28/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 3.5625e-04 - mae: 0.0133 - mse: 3.5625e-04 - val_loss: 3.3030e-04 - val_mae: 0.0126 - val_mse: 3.3030e-04\n",
      "Epoch 29/200\n",
      "2440/2440 [==============================] - 2s 625us/step - loss: 3.4595e-04 - mae: 0.0131 - mse: 3.4595e-04 - val_loss: 3.1994e-04 - val_mae: 0.0122 - val_mse: 3.1994e-04\n",
      "Epoch 30/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 3.3348e-04 - mae: 0.0129 - mse: 3.3348e-04 - val_loss: 3.1119e-04 - val_mae: 0.0118 - val_mse: 3.1119e-04\n",
      "Epoch 31/200\n",
      "2440/2440 [==============================] - 2s 641us/step - loss: 3.1954e-04 - mae: 0.0126 - mse: 3.1954e-04 - val_loss: 3.0309e-04 - val_mae: 0.0115 - val_mse: 3.0309e-04\n",
      "Epoch 32/200\n",
      "2440/2440 [==============================] - 1s 606us/step - loss: 3.0543e-04 - mae: 0.0123 - mse: 3.0543e-04 - val_loss: 2.9521e-04 - val_mae: 0.0111 - val_mse: 2.9521e-04\n",
      "Epoch 33/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 2.9220e-04 - mae: 0.0120 - mse: 2.9220e-04 - val_loss: 2.8765e-04 - val_mae: 0.0108 - val_mse: 2.8765e-04\n",
      "Epoch 34/200\n",
      "2440/2440 [==============================] - 2s 635us/step - loss: 2.8037e-04 - mae: 0.0117 - mse: 2.8037e-04 - val_loss: 2.8070e-04 - val_mae: 0.0105 - val_mse: 2.8070e-04\n",
      "Epoch 35/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 2.7004e-04 - mae: 0.0114 - mse: 2.7004e-04 - val_loss: 2.7459e-04 - val_mae: 0.0102 - val_mse: 2.7459e-04\n",
      "Epoch 36/200\n",
      "2440/2440 [==============================] - 1s 608us/step - loss: 2.6110e-04 - mae: 0.0112 - mse: 2.6110e-04 - val_loss: 2.6937e-04 - val_mae: 0.0100 - val_mse: 2.6937e-04\n",
      "Epoch 37/200\n",
      "2440/2440 [==============================] - 1s 611us/step - loss: 2.5337e-04 - mae: 0.0110 - mse: 2.5337e-04 - val_loss: 2.6503e-04 - val_mae: 0.0099 - val_mse: 2.6503e-04\n",
      "Epoch 38/200\n",
      "2440/2440 [==============================] - 1s 609us/step - loss: 2.4665e-04 - mae: 0.0109 - mse: 2.4665e-04 - val_loss: 2.6148e-04 - val_mae: 0.0098 - val_mse: 2.6148e-04\n",
      "Epoch 39/200\n",
      "2440/2440 [==============================] - 2s 635us/step - loss: 2.4079e-04 - mae: 0.0107 - mse: 2.4079e-04 - val_loss: 2.5861e-04 - val_mae: 0.0097 - val_mse: 2.5861e-04\n",
      "Epoch 40/200\n",
      "2440/2440 [==============================] - 1s 602us/step - loss: 2.3566e-04 - mae: 0.0106 - mse: 2.3566e-04 - val_loss: 2.5635e-04 - val_mae: 0.0097 - val_mse: 2.5635e-04\n",
      "Epoch 41/200\n",
      "2440/2440 [==============================] - 1s 607us/step - loss: 2.3115e-04 - mae: 0.0105 - mse: 2.3115e-04 - val_loss: 2.5459e-04 - val_mae: 0.0096 - val_mse: 2.5459e-04\n",
      "Epoch 42/200\n",
      "2440/2440 [==============================] - 2s 616us/step - loss: 2.2720e-04 - mae: 0.0104 - mse: 2.2720e-04 - val_loss: 2.5326e-04 - val_mae: 0.0097 - val_mse: 2.5326e-04\n",
      "Epoch 43/200\n",
      "2440/2440 [==============================] - 2s 678us/step - loss: 2.2372e-04 - mae: 0.0103 - mse: 2.2372e-04 - val_loss: 2.5230e-04 - val_mae: 0.0097 - val_mse: 2.5230e-04\n",
      "Epoch 44/200\n",
      "2440/2440 [==============================] - 2s 625us/step - loss: 2.2066e-04 - mae: 0.0102 - mse: 2.2066e-04 - val_loss: 2.5162e-04 - val_mae: 0.0097 - val_mse: 2.5162e-04\n",
      "Epoch 45/200\n",
      "2440/2440 [==============================] - 1s 611us/step - loss: 2.1795e-04 - mae: 0.0102 - mse: 2.1795e-04 - val_loss: 2.5118e-04 - val_mae: 0.0098 - val_mse: 2.5118e-04\n",
      "Epoch 46/200\n",
      "2440/2440 [==============================] - 2s 636us/step - loss: 2.1554e-04 - mae: 0.0101 - mse: 2.1554e-04 - val_loss: 2.5092e-04 - val_mae: 0.0098 - val_mse: 2.5092e-04\n",
      "Epoch 47/200\n",
      "2440/2440 [==============================] - 2s 635us/step - loss: 2.1337e-04 - mae: 0.0100 - mse: 2.1337e-04 - val_loss: 2.5079e-04 - val_mae: 0.0099 - val_mse: 2.5079e-04\n",
      "Epoch 48/200\n",
      "2440/2440 [==============================] - 2s 670us/step - loss: 2.1138e-04 - mae: 0.0100 - mse: 2.1138e-04 - val_loss: 2.5073e-04 - val_mae: 0.0099 - val_mse: 2.5073e-04\n",
      "Epoch 49/200\n",
      "2440/2440 [==============================] - 2s 618us/step - loss: 2.0952e-04 - mae: 0.0099 - mse: 2.0952e-04 - val_loss: 2.5070e-04 - val_mae: 0.0100 - val_mse: 2.5070e-04\n",
      "Epoch 50/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 2.0773e-04 - mae: 0.0099 - mse: 2.0773e-04 - val_loss: 2.5066e-04 - val_mae: 0.0100 - val_mse: 2.5066e-04\n",
      "Epoch 51/200\n",
      "2440/2440 [==============================] - 1s 606us/step - loss: 2.0598e-04 - mae: 0.0098 - mse: 2.0598e-04 - val_loss: 2.5055e-04 - val_mae: 0.0101 - val_mse: 2.5055e-04\n",
      "Epoch 52/200\n",
      "2440/2440 [==============================] - 2s 631us/step - loss: 2.0423e-04 - mae: 0.0097 - mse: 2.0423e-04 - val_loss: 2.5035e-04 - val_mae: 0.0101 - val_mse: 2.5035e-04\n",
      "Epoch 53/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 2.0244e-04 - mae: 0.0097 - mse: 2.0244e-04 - val_loss: 2.5000e-04 - val_mae: 0.0101 - val_mse: 2.5000e-04\n",
      "Epoch 54/200\n",
      "2440/2440 [==============================] - 1s 607us/step - loss: 2.0061e-04 - mae: 0.0096 - mse: 2.0061e-04 - val_loss: 2.4946e-04 - val_mae: 0.0101 - val_mse: 2.4946e-04\n",
      "Epoch 55/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 1.9871e-04 - mae: 0.0096 - mse: 1.9871e-04 - val_loss: 2.4869e-04 - val_mae: 0.0101 - val_mse: 2.4869e-04\n",
      "Epoch 56/200\n",
      "2440/2440 [==============================] - 2s 616us/step - loss: 1.9673e-04 - mae: 0.0095 - mse: 1.9673e-04 - val_loss: 2.4762e-04 - val_mae: 0.0101 - val_mse: 2.4762e-04\n",
      "Epoch 57/200\n",
      "2440/2440 [==============================] - 2s 633us/step - loss: 1.9470e-04 - mae: 0.0094 - mse: 1.9470e-04 - val_loss: 2.4620e-04 - val_mae: 0.0101 - val_mse: 2.4620e-04\n",
      "Epoch 58/200\n",
      "2440/2440 [==============================] - 2s 678us/step - loss: 1.9261e-04 - mae: 0.0094 - mse: 1.9261e-04 - val_loss: 2.4438e-04 - val_mae: 0.0100 - val_mse: 2.4438e-04\n",
      "Epoch 59/200\n",
      "2440/2440 [==============================] - 1s 614us/step - loss: 1.9050e-04 - mae: 0.0093 - mse: 1.9050e-04 - val_loss: 2.4211e-04 - val_mae: 0.0099 - val_mse: 2.4211e-04\n",
      "Epoch 60/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 1.8841e-04 - mae: 0.0092 - mse: 1.8841e-04 - val_loss: 2.3939e-04 - val_mae: 0.0097 - val_mse: 2.3939e-04\n",
      "Epoch 61/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 1.8639e-04 - mae: 0.0092 - mse: 1.8639e-04 - val_loss: 2.3623e-04 - val_mae: 0.0096 - val_mse: 2.3623e-04\n",
      "Epoch 62/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 1.8449e-04 - mae: 0.0091 - mse: 1.8449e-04 - val_loss: 2.3272e-04 - val_mae: 0.0094 - val_mse: 2.3272e-04\n",
      "Epoch 63/200\n",
      "2440/2440 [==============================] - 2s 635us/step - loss: 1.8276e-04 - mae: 0.0090 - mse: 1.8276e-04 - val_loss: 2.2899e-04 - val_mae: 0.0091 - val_mse: 2.2899e-04\n",
      "Epoch 64/200\n",
      "2440/2440 [==============================] - 1s 609us/step - loss: 1.8124e-04 - mae: 0.0090 - mse: 1.8124e-04 - val_loss: 2.2522e-04 - val_mae: 0.0089 - val_mse: 2.2522e-04\n",
      "Epoch 65/200\n",
      "2440/2440 [==============================] - 1s 606us/step - loss: 1.7999e-04 - mae: 0.0089 - mse: 1.7999e-04 - val_loss: 2.2162e-04 - val_mae: 0.0086 - val_mse: 2.2162e-04\n",
      "Epoch 66/200\n",
      "2440/2440 [==============================] - 2s 626us/step - loss: 1.7905e-04 - mae: 0.0089 - mse: 1.7905e-04 - val_loss: 2.1843e-04 - val_mae: 0.0084 - val_mse: 2.1843e-04\n",
      "Epoch 67/200\n",
      "2440/2440 [==============================] - 1s 607us/step - loss: 1.7847e-04 - mae: 0.0088 - mse: 1.7847e-04 - val_loss: 2.1586e-04 - val_mae: 0.0082 - val_mse: 2.1586e-04\n",
      "Epoch 68/200\n",
      "2440/2440 [==============================] - 2s 673us/step - loss: 1.7838e-04 - mae: 0.0088 - mse: 1.7838e-04 - val_loss: 2.1413e-04 - val_mae: 0.0080 - val_mse: 2.1413e-04\n",
      "Epoch 69/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 1.7894e-04 - mae: 0.0089 - mse: 1.7894e-04 - val_loss: 2.1339e-04 - val_mae: 0.0079 - val_mse: 2.1339e-04\n",
      "Epoch 70/200\n",
      "2440/2440 [==============================] - 1s 611us/step - loss: 1.8041e-04 - mae: 0.0089 - mse: 1.8041e-04 - val_loss: 2.1370e-04 - val_mae: 0.0078 - val_mse: 2.1370e-04\n",
      "Epoch 71/200\n",
      "2440/2440 [==============================] - 1s 610us/step - loss: 1.8294e-04 - mae: 0.0090 - mse: 1.8294e-04 - val_loss: 2.1498e-04 - val_mae: 0.0079 - val_mse: 2.1498e-04\n",
      "Epoch 72/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 1.8638e-04 - mae: 0.0092 - mse: 1.8638e-04 - val_loss: 2.1666e-04 - val_mae: 0.0080 - val_mse: 2.1666e-04\n",
      "Epoch 73/200\n",
      "2440/2440 [==============================] - 2s 657us/step - loss: 1.8971e-04 - mae: 0.0093 - mse: 1.8971e-04 - val_loss: 2.1766e-04 - val_mae: 0.0081 - val_mse: 2.1766e-04\n",
      "Epoch 74/200\n",
      "2440/2440 [==============================] - 2s 641us/step - loss: 1.9119e-04 - mae: 0.0094 - mse: 1.9119e-04 - val_loss: 2.1721e-04 - val_mae: 0.0081 - val_mse: 2.1721e-04\n",
      "Epoch 75/200\n",
      "2440/2440 [==============================] - 2s 618us/step - loss: 1.8978e-04 - mae: 0.0093 - mse: 1.8978e-04 - val_loss: 2.1578e-04 - val_mae: 0.0081 - val_mse: 2.1578e-04\n",
      "Epoch 76/200\n",
      "2440/2440 [==============================] - 2s 618us/step - loss: 1.8657e-04 - mae: 0.0092 - mse: 1.8657e-04 - val_loss: 2.1437e-04 - val_mae: 0.0080 - val_mse: 2.1437e-04\n",
      "Epoch 77/200\n",
      "2440/2440 [==============================] - 2s 624us/step - loss: 1.8357e-04 - mae: 0.0091 - mse: 1.8357e-04 - val_loss: 2.1346e-04 - val_mae: 0.0079 - val_mse: 2.1346e-04\n",
      "Epoch 78/200\n",
      "2440/2440 [==============================] - 2s 638us/step - loss: 1.8185e-04 - mae: 0.0090 - mse: 1.8185e-04 - val_loss: 2.1322e-04 - val_mae: 0.0079 - val_mse: 2.1322e-04\n",
      "Epoch 79/200\n",
      "2440/2440 [==============================] - 2s 646us/step - loss: 1.8152e-04 - mae: 0.0090 - mse: 1.8152e-04 - val_loss: 2.1360e-04 - val_mae: 0.0079 - val_mse: 2.1360e-04\n",
      "Epoch 80/200\n",
      "2440/2440 [==============================] - 2s 647us/step - loss: 1.8224e-04 - mae: 0.0090 - mse: 1.8224e-04 - val_loss: 2.1431e-04 - val_mae: 0.0079 - val_mse: 2.1431e-04\n",
      "Epoch 81/200\n",
      "2440/2440 [==============================] - 2s 639us/step - loss: 1.8334e-04 - mae: 0.0091 - mse: 1.8334e-04 - val_loss: 2.1479e-04 - val_mae: 0.0080 - val_mse: 2.1479e-04\n",
      "Epoch 82/200\n",
      "2440/2440 [==============================] - 2s 647us/step - loss: 1.8392e-04 - mae: 0.0091 - mse: 1.8392e-04 - val_loss: 2.1460e-04 - val_mae: 0.0080 - val_mse: 2.1460e-04\n",
      "Epoch 83/200\n",
      "2440/2440 [==============================] - 2s 616us/step - loss: 1.8335e-04 - mae: 0.0091 - mse: 1.8335e-04 - val_loss: 2.1380e-04 - val_mae: 0.0080 - val_mse: 2.1380e-04\n",
      "Epoch 84/200\n",
      "2440/2440 [==============================] - 2s 634us/step - loss: 1.8180e-04 - mae: 0.0090 - mse: 1.8180e-04 - val_loss: 2.1284e-04 - val_mae: 0.0079 - val_mse: 2.1284e-04\n",
      "Epoch 85/200\n",
      "2440/2440 [==============================] - 2s 653us/step - loss: 1.8002e-04 - mae: 0.0089 - mse: 1.8002e-04 - val_loss: 2.1212e-04 - val_mae: 0.0079 - val_mse: 2.1212e-04\n",
      "Epoch 86/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 1.7869e-04 - mae: 0.0089 - mse: 1.7869e-04 - val_loss: 2.1185e-04 - val_mae: 0.0078 - val_mse: 2.1185e-04\n",
      "Epoch 87/200\n",
      "2440/2440 [==============================] - 2s 647us/step - loss: 1.7810e-04 - mae: 0.0089 - mse: 1.7810e-04 - val_loss: 2.1204e-04 - val_mae: 0.0079 - val_mse: 2.1204e-04\n",
      "Epoch 88/200\n",
      "2440/2440 [==============================] - 2s 616us/step - loss: 1.7819e-04 - mae: 0.0089 - mse: 1.7819e-04 - val_loss: 2.1255e-04 - val_mae: 0.0079 - val_mse: 2.1255e-04\n",
      "Epoch 89/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2440/2440 [==============================] - 1s 597us/step - loss: 1.7867e-04 - mae: 0.0089 - mse: 1.7867e-04 - val_loss: 2.1307e-04 - val_mae: 0.0079 - val_mse: 2.1307e-04\n",
      "Epoch 90/200\n",
      "2440/2440 [==============================] - 1s 596us/step - loss: 1.7908e-04 - mae: 0.0089 - mse: 1.7908e-04 - val_loss: 2.1328e-04 - val_mae: 0.0080 - val_mse: 2.1328e-04\n",
      "Epoch 91/200\n",
      "2440/2440 [==============================] - 1s 611us/step - loss: 1.7892e-04 - mae: 0.0089 - mse: 1.7892e-04 - val_loss: 2.1301e-04 - val_mae: 0.0080 - val_mse: 2.1301e-04\n",
      "Epoch 92/200\n",
      "2440/2440 [==============================] - 1s 609us/step - loss: 1.7805e-04 - mae: 0.0089 - mse: 1.7805e-04 - val_loss: 2.1243e-04 - val_mae: 0.0080 - val_mse: 2.1243e-04\n",
      "Epoch 93/200\n",
      "2440/2440 [==============================] - 2s 625us/step - loss: 1.7671e-04 - mae: 0.0088 - mse: 1.7671e-04 - val_loss: 2.1182e-04 - val_mae: 0.0080 - val_mse: 2.1182e-04\n",
      "Epoch 94/200\n",
      "2440/2440 [==============================] - 1s 611us/step - loss: 1.7536e-04 - mae: 0.0087 - mse: 1.7536e-04 - val_loss: 2.1142e-04 - val_mae: 0.0080 - val_mse: 2.1142e-04\n",
      "Epoch 95/200\n",
      "2440/2440 [==============================] - 2s 619us/step - loss: 1.7433e-04 - mae: 0.0087 - mse: 1.7433e-04 - val_loss: 2.1140e-04 - val_mae: 0.0080 - val_mse: 2.1140e-04\n",
      "Epoch 96/200\n",
      "2440/2440 [==============================] - 2s 631us/step - loss: 1.7377e-04 - mae: 0.0086 - mse: 1.7377e-04 - val_loss: 2.1183e-04 - val_mae: 0.0080 - val_mse: 2.1183e-04\n",
      "Epoch 97/200\n",
      "2440/2440 [==============================] - 2s 646us/step - loss: 1.7371e-04 - mae: 0.0086 - mse: 1.7371e-04 - val_loss: 2.1269e-04 - val_mae: 0.0081 - val_mse: 2.1269e-04\n",
      "Epoch 98/200\n",
      "2440/2440 [==============================] - 2s 653us/step - loss: 1.7404e-04 - mae: 0.0087 - mse: 1.7404e-04 - val_loss: 2.1380e-04 - val_mae: 0.0082 - val_mse: 2.1380e-04\n",
      "Epoch 99/200\n",
      "2440/2440 [==============================] - 2s 630us/step - loss: 1.7453e-04 - mae: 0.0087 - mse: 1.7453e-04 - val_loss: 2.1481e-04 - val_mae: 0.0083 - val_mse: 2.1481e-04\n",
      "Epoch 100/200\n",
      "2440/2440 [==============================] - 2s 623us/step - loss: 1.7473e-04 - mae: 0.0087 - mse: 1.7473e-04 - val_loss: 2.1530e-04 - val_mae: 0.0084 - val_mse: 2.1530e-04\n",
      "Epoch 101/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 1.7421e-04 - mae: 0.0086 - mse: 1.7421e-04 - val_loss: 2.1500e-04 - val_mae: 0.0084 - val_mse: 2.1500e-04\n",
      "Epoch 102/200\n",
      "2440/2440 [==============================] - 2s 650us/step - loss: 1.7286e-04 - mae: 0.0086 - mse: 1.7286e-04 - val_loss: 2.1392e-04 - val_mae: 0.0084 - val_mse: 2.1392e-04\n",
      "Epoch 103/200\n",
      "2440/2440 [==============================] - 2s 646us/step - loss: 1.7108e-04 - mae: 0.0085 - mse: 1.7108e-04 - val_loss: 2.1227e-04 - val_mae: 0.0083 - val_mse: 2.1227e-04\n",
      "Epoch 104/200\n",
      "2440/2440 [==============================] - 2s 615us/step - loss: 1.6947e-04 - mae: 0.0084 - mse: 1.6947e-04 - val_loss: 2.1041e-04 - val_mae: 0.0081 - val_mse: 2.1041e-04\n",
      "Epoch 105/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 1.6856e-04 - mae: 0.0084 - mse: 1.6856e-04 - val_loss: 2.0880e-04 - val_mae: 0.0079 - val_mse: 2.0880e-04\n",
      "Epoch 106/200\n",
      "2440/2440 [==============================] - 1s 610us/step - loss: 1.6847e-04 - mae: 0.0084 - mse: 1.6847e-04 - val_loss: 2.0778e-04 - val_mae: 0.0078 - val_mse: 2.0778e-04\n",
      "Epoch 107/200\n",
      "2440/2440 [==============================] - 2s 639us/step - loss: 1.6853e-04 - mae: 0.0084 - mse: 1.6853e-04 - val_loss: 2.0715e-04 - val_mae: 0.0077 - val_mse: 2.0715e-04\n",
      "Epoch 108/200\n",
      "2440/2440 [==============================] - 2s 642us/step - loss: 1.6781e-04 - mae: 0.0084 - mse: 1.6781e-04 - val_loss: 2.0682e-04 - val_mae: 0.0077 - val_mse: 2.0682e-04\n",
      "Epoch 109/200\n",
      "2440/2440 [==============================] - 2s 623us/step - loss: 1.6663e-04 - mae: 0.0084 - mse: 1.6663e-04 - val_loss: 2.0771e-04 - val_mae: 0.0078 - val_mse: 2.0771e-04\n",
      "Epoch 110/200\n",
      "2440/2440 [==============================] - 2s 636us/step - loss: 1.6652e-04 - mae: 0.0083 - mse: 1.6652e-04 - val_loss: 2.1069e-04 - val_mae: 0.0080 - val_mse: 2.1069e-04\n",
      "Epoch 111/200\n",
      "2440/2440 [==============================] - 2s 635us/step - loss: 1.6856e-04 - mae: 0.0084 - mse: 1.6856e-04 - val_loss: 2.1570e-04 - val_mae: 0.0084 - val_mse: 2.1570e-04\n",
      "Epoch 112/200\n",
      "2440/2440 [==============================] - 2s 702us/step - loss: 1.7247e-04 - mae: 0.0086 - mse: 1.7247e-04 - val_loss: 2.2105e-04 - val_mae: 0.0088 - val_mse: 2.2105e-04\n",
      "Epoch 113/200\n",
      "2440/2440 [==============================] - 2s 672us/step - loss: 1.7593e-04 - mae: 0.0087 - mse: 1.7593e-04 - val_loss: 2.2329e-04 - val_mae: 0.0090 - val_mse: 2.2329e-04\n",
      "Epoch 114/200\n",
      "2440/2440 [==============================] - 2s 660us/step - loss: 1.7576e-04 - mae: 0.0087 - mse: 1.7576e-04 - val_loss: 2.2167e-04 - val_mae: 0.0090 - val_mse: 2.2167e-04\n",
      "Epoch 115/200\n",
      "2440/2440 [==============================] - 2s 678us/step - loss: 1.7411e-04 - mae: 0.0087 - mse: 1.7411e-04 - val_loss: 2.2041e-04 - val_mae: 0.0090 - val_mse: 2.2041e-04\n",
      "Epoch 116/200\n",
      "2440/2440 [==============================] - 2s 631us/step - loss: 1.7623e-04 - mae: 0.0089 - mse: 1.7623e-04 - val_loss: 2.2307e-04 - val_mae: 0.0091 - val_mse: 2.2307e-04\n",
      "Epoch 117/200\n",
      "2440/2440 [==============================] - 2s 633us/step - loss: 1.7719e-04 - mae: 0.0090 - mse: 1.7719e-04 - val_loss: 2.2640e-04 - val_mae: 0.0094 - val_mse: 2.2640e-04\n",
      "Epoch 118/200\n",
      "2440/2440 [==============================] - 1s 611us/step - loss: 1.7411e-04 - mae: 0.0089 - mse: 1.7411e-04 - val_loss: 2.3245e-04 - val_mae: 0.0098 - val_mse: 2.3245e-04\n",
      "Epoch 119/200\n",
      "2440/2440 [==============================] - 2s 618us/step - loss: 1.7459e-04 - mae: 0.0088 - mse: 1.7459e-04 - val_loss: 2.4088e-04 - val_mae: 0.0102 - val_mse: 2.4088e-04\n",
      "Epoch 120/200\n",
      "2440/2440 [==============================] - 1s 597us/step - loss: 1.7730e-04 - mae: 0.0089 - mse: 1.7730e-04 - val_loss: 2.4940e-04 - val_mae: 0.0107 - val_mse: 2.4940e-04\n",
      "Epoch 121/200\n",
      "2440/2440 [==============================] - 1s 602us/step - loss: 1.7991e-04 - mae: 0.0090 - mse: 1.7991e-04 - val_loss: 2.5886e-04 - val_mae: 0.0111 - val_mse: 2.5886e-04\n",
      "Epoch 122/200\n",
      "2440/2440 [==============================] - 2s 676us/step - loss: 1.8254e-04 - mae: 0.0091 - mse: 1.8254e-04 - val_loss: 2.7037e-04 - val_mae: 0.0116 - val_mse: 2.7037e-04\n",
      "Epoch 123/200\n",
      "2440/2440 [==============================] - 2s 726us/step - loss: 1.8600e-04 - mae: 0.0092 - mse: 1.8600e-04 - val_loss: 2.8423e-04 - val_mae: 0.0122 - val_mse: 2.8423e-04\n",
      "Epoch 124/200\n",
      "2440/2440 [==============================] - 2s 669us/step - loss: 1.9109e-04 - mae: 0.0094 - mse: 1.9109e-04 - val_loss: 2.9919e-04 - val_mae: 0.0128 - val_mse: 2.9919e-04\n",
      "Epoch 125/200\n",
      "2440/2440 [==============================] - 2s 709us/step - loss: 1.9792e-04 - mae: 0.0096 - mse: 1.9792e-04 - val_loss: 3.1152e-04 - val_mae: 0.0132 - val_mse: 3.1152e-04\n",
      "Epoch 126/200\n",
      "2440/2440 [==============================] - 2s 696us/step - loss: 2.0729e-04 - mae: 0.0100 - mse: 2.0729e-04 - val_loss: 3.1875e-04 - val_mae: 0.0134 - val_mse: 3.1875e-04\n",
      "Epoch 127/200\n",
      "2440/2440 [==============================] - 2s 710us/step - loss: 2.1927e-04 - mae: 0.0104 - mse: 2.1927e-04 - val_loss: 3.0646e-04 - val_mae: 0.0130 - val_mse: 3.0646e-04\n",
      "Epoch 128/200\n",
      "2440/2440 [==============================] - 2s 617us/step - loss: 2.2328e-04 - mae: 0.0106 - mse: 2.2328e-04 - val_loss: 2.6319e-04 - val_mae: 0.0114 - val_mse: 2.6319e-04\n",
      "Epoch 129/200\n",
      "2440/2440 [==============================] - 2s 675us/step - loss: 2.0703e-04 - mae: 0.0102 - mse: 2.0703e-04 - val_loss: 2.2257e-04 - val_mae: 0.0091 - val_mse: 2.2257e-04\n",
      "Epoch 130/200\n",
      "2440/2440 [==============================] - 2s 621us/step - loss: 1.8852e-04 - mae: 0.0096 - mse: 1.8852e-04 - val_loss: 2.0893e-04 - val_mae: 0.0078 - val_mse: 2.0893e-04\n",
      "Epoch 131/200\n",
      "2440/2440 [==============================] - 2s 627us/step - loss: 1.7974e-04 - mae: 0.0092 - mse: 1.7974e-04 - val_loss: 2.0693e-04 - val_mae: 0.0076 - val_mse: 2.0693e-04\n",
      "Epoch 132/200\n",
      "2440/2440 [==============================] - 2s 680us/step - loss: 1.7811e-04 - mae: 0.0091 - mse: 1.7811e-04 - val_loss: 2.0691e-04 - val_mae: 0.0076 - val_mse: 2.0691e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/200\n",
      "2440/2440 [==============================] - 2s 656us/step - loss: 1.7691e-04 - mae: 0.0090 - mse: 1.7691e-04 - val_loss: 2.0746e-04 - val_mae: 0.0076 - val_mse: 2.0746e-04\n",
      "Epoch 134/200\n",
      "2440/2440 [==============================] - 2s 682us/step - loss: 1.7496e-04 - mae: 0.0090 - mse: 1.7496e-04 - val_loss: 2.0820e-04 - val_mae: 0.0076 - val_mse: 2.0820e-04\n",
      "Epoch 135/200\n",
      "2440/2440 [==============================] - 1s 600us/step - loss: 1.7299e-04 - mae: 0.0089 - mse: 1.7299e-04 - val_loss: 2.0894e-04 - val_mae: 0.0076 - val_mse: 2.0894e-04\n",
      "Epoch 136/200\n",
      "2440/2440 [==============================] - 1s 603us/step - loss: 1.7137e-04 - mae: 0.0088 - mse: 1.7137e-04 - val_loss: 2.0958e-04 - val_mae: 0.0077 - val_mse: 2.0958e-04\n",
      "Epoch 137/200\n",
      "2440/2440 [==============================] - 1s 596us/step - loss: 1.7013e-04 - mae: 0.0088 - mse: 1.7013e-04 - val_loss: 2.1011e-04 - val_mae: 0.0077 - val_mse: 2.1011e-04\n",
      "Epoch 138/200\n",
      "2440/2440 [==============================] - 1s 593us/step - loss: 1.6916e-04 - mae: 0.0087 - mse: 1.6916e-04 - val_loss: 2.1055e-04 - val_mae: 0.0077 - val_mse: 2.1055e-04\n",
      "Epoch 139/200\n",
      "2440/2440 [==============================] - 2s 633us/step - loss: 1.6840e-04 - mae: 0.0087 - mse: 1.6840e-04 - val_loss: 2.1091e-04 - val_mae: 0.0077 - val_mse: 2.1091e-04\n",
      "Epoch 140/200\n",
      "2440/2440 [==============================] - 1s 590us/step - loss: 1.6782e-04 - mae: 0.0086 - mse: 1.6782e-04 - val_loss: 2.1120e-04 - val_mae: 0.0077 - val_mse: 2.1120e-04\n",
      "Epoch 141/200\n",
      "2440/2440 [==============================] - 2s 712us/step - loss: 1.6739e-04 - mae: 0.0086 - mse: 1.6739e-04 - val_loss: 2.1145e-04 - val_mae: 0.0078 - val_mse: 2.1145e-04\n",
      "Epoch 142/200\n",
      "2440/2440 [==============================] - 2s 615us/step - loss: 1.6710e-04 - mae: 0.0086 - mse: 1.6710e-04 - val_loss: 2.1167e-04 - val_mae: 0.0078 - val_mse: 2.1167e-04\n",
      "Epoch 143/200\n",
      "2440/2440 [==============================] - 2s 630us/step - loss: 1.6690e-04 - mae: 0.0086 - mse: 1.6690e-04 - val_loss: 2.1190e-04 - val_mae: 0.0078 - val_mse: 2.1190e-04\n",
      "Epoch 144/200\n",
      "2440/2440 [==============================] - 2s 655us/step - loss: 1.6674e-04 - mae: 0.0086 - mse: 1.6674e-04 - val_loss: 2.1215e-04 - val_mae: 0.0078 - val_mse: 2.1215e-04\n",
      "Epoch 145/200\n",
      "2440/2440 [==============================] - 2s 625us/step - loss: 1.6657e-04 - mae: 0.0086 - mse: 1.6657e-04 - val_loss: 2.1240e-04 - val_mae: 0.0078 - val_mse: 2.1240e-04\n",
      "Epoch 146/200\n",
      "2440/2440 [==============================] - 1s 601us/step - loss: 1.6639e-04 - mae: 0.0086 - mse: 1.6639e-04 - val_loss: 2.1265e-04 - val_mae: 0.0078 - val_mse: 2.1265e-04\n",
      "Epoch 147/200\n",
      "2440/2440 [==============================] - 1s 589us/step - loss: 1.6621e-04 - mae: 0.0086 - mse: 1.6621e-04 - val_loss: 2.1286e-04 - val_mae: 0.0078 - val_mse: 2.1286e-04\n",
      "Epoch 148/200\n",
      "2440/2440 [==============================] - 2s 641us/step - loss: 1.6605e-04 - mae: 0.0086 - mse: 1.6605e-04 - val_loss: 2.1302e-04 - val_mae: 0.0078 - val_mse: 2.1302e-04\n",
      "Epoch 149/200\n",
      "2440/2440 [==============================] - 2s 676us/step - loss: 1.6593e-04 - mae: 0.0085 - mse: 1.6593e-04 - val_loss: 2.1315e-04 - val_mae: 0.0078 - val_mse: 2.1315e-04\n",
      "Epoch 150/200\n",
      "2440/2440 [==============================] - 2s 625us/step - loss: 1.6583e-04 - mae: 0.0085 - mse: 1.6583e-04 - val_loss: 2.1324e-04 - val_mae: 0.0078 - val_mse: 2.1324e-04\n",
      "Epoch 151/200\n",
      "2440/2440 [==============================] - 2s 644us/step - loss: 1.6575e-04 - mae: 0.0085 - mse: 1.6575e-04 - val_loss: 2.1333e-04 - val_mae: 0.0078 - val_mse: 2.1333e-04\n",
      "Epoch 152/200\n",
      "2440/2440 [==============================] - 2s 716us/step - loss: 1.6565e-04 - mae: 0.0085 - mse: 1.6565e-04 - val_loss: 2.1349e-04 - val_mae: 0.0078 - val_mse: 2.1349e-04\n",
      "Epoch 153/200\n",
      "2440/2440 [==============================] - 2s 642us/step - loss: 1.6551e-04 - mae: 0.0085 - mse: 1.6551e-04 - val_loss: 2.1378e-04 - val_mae: 0.0078 - val_mse: 2.1378e-04\n",
      "Epoch 154/200\n",
      "2440/2440 [==============================] - 2s 634us/step - loss: 1.6534e-04 - mae: 0.0085 - mse: 1.6534e-04 - val_loss: 2.1421e-04 - val_mae: 0.0078 - val_mse: 2.1421e-04\n",
      "Epoch 155/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 1.6517e-04 - mae: 0.0085 - mse: 1.6517e-04 - val_loss: 2.1467e-04 - val_mae: 0.0078 - val_mse: 2.1467e-04\n",
      "Epoch 156/200\n",
      "2440/2440 [==============================] - 1s 609us/step - loss: 1.6502e-04 - mae: 0.0085 - mse: 1.6502e-04 - val_loss: 2.1503e-04 - val_mae: 0.0078 - val_mse: 2.1503e-04\n",
      "Epoch 157/200\n",
      "2440/2440 [==============================] - 2s 667us/step - loss: 1.6492e-04 - mae: 0.0085 - mse: 1.6492e-04 - val_loss: 2.1518e-04 - val_mae: 0.0078 - val_mse: 2.1518e-04\n",
      "Epoch 158/200\n",
      "2440/2440 [==============================] - 2s 621us/step - loss: 1.6487e-04 - mae: 0.0085 - mse: 1.6487e-04 - val_loss: 2.1517e-04 - val_mae: 0.0078 - val_mse: 2.1517e-04\n",
      "Epoch 159/200\n",
      "2440/2440 [==============================] - 1s 606us/step - loss: 1.6486e-04 - mae: 0.0085 - mse: 1.6486e-04 - val_loss: 2.1514e-04 - val_mae: 0.0078 - val_mse: 2.1514e-04\n",
      "Epoch 160/200\n",
      "2440/2440 [==============================] - 1s 606us/step - loss: 1.6484e-04 - mae: 0.0085 - mse: 1.6484e-04 - val_loss: 2.1534e-04 - val_mae: 0.0078 - val_mse: 2.1534e-04\n",
      "Epoch 161/200\n",
      "2440/2440 [==============================] - 2s 643us/step - loss: 1.6474e-04 - mae: 0.0085 - mse: 1.6474e-04 - val_loss: 2.1604e-04 - val_mae: 0.0078 - val_mse: 2.1604e-04\n",
      "Epoch 162/200\n",
      "2440/2440 [==============================] - 2s 624us/step - loss: 1.6447e-04 - mae: 0.0084 - mse: 1.6447e-04 - val_loss: 2.1726e-04 - val_mae: 0.0079 - val_mse: 2.1726e-04\n",
      "Epoch 163/200\n",
      "2440/2440 [==============================] - 2s 617us/step - loss: 1.6408e-04 - mae: 0.0084 - mse: 1.6408e-04 - val_loss: 2.1852e-04 - val_mae: 0.0079 - val_mse: 2.1852e-04\n",
      "Epoch 164/200\n",
      "2440/2440 [==============================] - 2s 620us/step - loss: 1.6368e-04 - mae: 0.0084 - mse: 1.6368e-04 - val_loss: 2.1930e-04 - val_mae: 0.0080 - val_mse: 2.1930e-04\n",
      "Epoch 165/200\n",
      "2440/2440 [==============================] - 2s 621us/step - loss: 1.6326e-04 - mae: 0.0084 - mse: 1.6326e-04 - val_loss: 2.1939e-04 - val_mae: 0.0079 - val_mse: 2.1939e-04\n",
      "Epoch 166/200\n",
      "2440/2440 [==============================] - 2s 626us/step - loss: 1.6278e-04 - mae: 0.0084 - mse: 1.6278e-04 - val_loss: 2.1893e-04 - val_mae: 0.0079 - val_mse: 2.1893e-04\n",
      "Epoch 167/200\n",
      "2440/2440 [==============================] - 2s 636us/step - loss: 1.6226e-04 - mae: 0.0084 - mse: 1.6226e-04 - val_loss: 2.1820e-04 - val_mae: 0.0079 - val_mse: 2.1820e-04\n",
      "Epoch 168/200\n",
      "2440/2440 [==============================] - 2s 647us/step - loss: 1.6182e-04 - mae: 0.0083 - mse: 1.6182e-04 - val_loss: 2.1763e-04 - val_mae: 0.0079 - val_mse: 2.1763e-04\n",
      "Epoch 169/200\n",
      "2440/2440 [==============================] - 2s 649us/step - loss: 1.6152e-04 - mae: 0.0083 - mse: 1.6152e-04 - val_loss: 2.1761e-04 - val_mae: 0.0079 - val_mse: 2.1761e-04\n",
      "Epoch 170/200\n",
      "2440/2440 [==============================] - 2s 670us/step - loss: 1.6135e-04 - mae: 0.0083 - mse: 1.6135e-04 - val_loss: 2.1814e-04 - val_mae: 0.0080 - val_mse: 2.1814e-04\n",
      "Epoch 171/200\n",
      "2440/2440 [==============================] - 2s 657us/step - loss: 1.6123e-04 - mae: 0.0083 - mse: 1.6123e-04 - val_loss: 2.1871e-04 - val_mae: 0.0080 - val_mse: 2.1871e-04\n",
      "Epoch 172/200\n",
      "2440/2440 [==============================] - 2s 631us/step - loss: 1.6115e-04 - mae: 0.0083 - mse: 1.6115e-04 - val_loss: 2.1888e-04 - val_mae: 0.0080 - val_mse: 2.1888e-04\n",
      "Epoch 173/200\n",
      "2440/2440 [==============================] - 1s 601us/step - loss: 1.6109e-04 - mae: 0.0083 - mse: 1.6109e-04 - val_loss: 2.1869e-04 - val_mae: 0.0080 - val_mse: 2.1869e-04\n",
      "Epoch 174/200\n",
      "2440/2440 [==============================] - 2s 617us/step - loss: 1.6099e-04 - mae: 0.0083 - mse: 1.6099e-04 - val_loss: 2.1846e-04 - val_mae: 0.0080 - val_mse: 2.1846e-04\n",
      "Epoch 175/200\n",
      "2440/2440 [==============================] - 2s 643us/step - loss: 1.6082e-04 - mae: 0.0083 - mse: 1.6082e-04 - val_loss: 2.1849e-04 - val_mae: 0.0080 - val_mse: 2.1849e-04\n",
      "Epoch 176/200\n",
      "2440/2440 [==============================] - 2s 671us/step - loss: 1.6058e-04 - mae: 0.0083 - mse: 1.6058e-04 - val_loss: 2.1890e-04 - val_mae: 0.0080 - val_mse: 2.1890e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/200\n",
      "2440/2440 [==============================] - 1s 610us/step - loss: 1.6032e-04 - mae: 0.0083 - mse: 1.6032e-04 - val_loss: 2.1951e-04 - val_mae: 0.0080 - val_mse: 2.1951e-04\n",
      "Epoch 178/200\n",
      "2440/2440 [==============================] - 1s 609us/step - loss: 1.6007e-04 - mae: 0.0083 - mse: 1.6007e-04 - val_loss: 2.1976e-04 - val_mae: 0.0080 - val_mse: 2.1976e-04\n",
      "Epoch 179/200\n",
      "2440/2440 [==============================] - 1s 609us/step - loss: 1.5979e-04 - mae: 0.0082 - mse: 1.5979e-04 - val_loss: 2.1935e-04 - val_mae: 0.0080 - val_mse: 2.1935e-04\n",
      "Epoch 180/200\n",
      "2440/2440 [==============================] - 2s 674us/step - loss: 1.5937e-04 - mae: 0.0082 - mse: 1.5937e-04 - val_loss: 2.1880e-04 - val_mae: 0.0080 - val_mse: 2.1880e-04\n",
      "Epoch 181/200\n",
      "2440/2440 [==============================] - 1s 602us/step - loss: 1.5886e-04 - mae: 0.0082 - mse: 1.5886e-04 - val_loss: 2.1872e-04 - val_mae: 0.0080 - val_mse: 2.1872e-04\n",
      "Epoch 182/200\n",
      "2440/2440 [==============================] - 1s 599us/step - loss: 1.5835e-04 - mae: 0.0082 - mse: 1.5835e-04 - val_loss: 2.1894e-04 - val_mae: 0.0080 - val_mse: 2.1894e-04\n",
      "Epoch 183/200\n",
      "2440/2440 [==============================] - 1s 597us/step - loss: 1.5799e-04 - mae: 0.0082 - mse: 1.5799e-04 - val_loss: 2.1887e-04 - val_mae: 0.0080 - val_mse: 2.1887e-04\n",
      "Epoch 184/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 1.5791e-04 - mae: 0.0081 - mse: 1.5791e-04 - val_loss: 2.1831e-04 - val_mae: 0.0080 - val_mse: 2.1831e-04\n",
      "Epoch 185/200\n",
      "2440/2440 [==============================] - 2s 636us/step - loss: 1.5799e-04 - mae: 0.0081 - mse: 1.5799e-04 - val_loss: 2.1777e-04 - val_mae: 0.0080 - val_mse: 2.1777e-04\n",
      "Epoch 186/200\n",
      "2440/2440 [==============================] - 1s 613us/step - loss: 1.5804e-04 - mae: 0.0081 - mse: 1.5804e-04 - val_loss: 2.1785e-04 - val_mae: 0.0080 - val_mse: 2.1785e-04\n",
      "Epoch 187/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 1.5803e-04 - mae: 0.0081 - mse: 1.5803e-04 - val_loss: 2.1876e-04 - val_mae: 0.0080 - val_mse: 2.1876e-04\n",
      "Epoch 188/200\n",
      "2440/2440 [==============================] - 1s 606us/step - loss: 1.5813e-04 - mae: 0.0081 - mse: 1.5813e-04 - val_loss: 2.2036e-04 - val_mae: 0.0081 - val_mse: 2.2036e-04\n",
      "Epoch 189/200\n",
      "2440/2440 [==============================] - 1s 604us/step - loss: 1.5844e-04 - mae: 0.0082 - mse: 1.5844e-04 - val_loss: 2.2215e-04 - val_mae: 0.0082 - val_mse: 2.2215e-04\n",
      "Epoch 190/200\n",
      "2440/2440 [==============================] - 2s 624us/step - loss: 1.5886e-04 - mae: 0.0082 - mse: 1.5886e-04 - val_loss: 2.2380e-04 - val_mae: 0.0083 - val_mse: 2.2380e-04\n",
      "Epoch 191/200\n",
      "2440/2440 [==============================] - 1s 605us/step - loss: 1.5935e-04 - mae: 0.0082 - mse: 1.5935e-04 - val_loss: 2.2547e-04 - val_mae: 0.0084 - val_mse: 2.2547e-04\n",
      "Epoch 192/200\n",
      "2440/2440 [==============================] - 1s 598us/step - loss: 1.5991e-04 - mae: 0.0082 - mse: 1.5991e-04 - val_loss: 2.2753e-04 - val_mae: 0.0085 - val_mse: 2.2753e-04\n",
      "Epoch 193/200\n",
      "2440/2440 [==============================] - 1s 602us/step - loss: 1.6039e-04 - mae: 0.0083 - mse: 1.6039e-04 - val_loss: 2.3033e-04 - val_mae: 0.0086 - val_mse: 2.3033e-04\n",
      "Epoch 194/200\n",
      "2440/2440 [==============================] - 1s 600us/step - loss: 1.6081e-04 - mae: 0.0083 - mse: 1.6081e-04 - val_loss: 2.3420e-04 - val_mae: 0.0088 - val_mse: 2.3420e-04\n",
      "Epoch 195/200\n",
      "2440/2440 [==============================] - 2s 625us/step - loss: 1.6132e-04 - mae: 0.0083 - mse: 1.6132e-04 - val_loss: 2.3922e-04 - val_mae: 0.0091 - val_mse: 2.3922e-04\n",
      "Epoch 196/200\n",
      "2440/2440 [==============================] - 2s 626us/step - loss: 1.6221e-04 - mae: 0.0084 - mse: 1.6221e-04 - val_loss: 2.4487e-04 - val_mae: 0.0093 - val_mse: 2.4487e-04\n",
      "Epoch 197/200\n",
      "2440/2440 [==============================] - 2s 651us/step - loss: 1.6351e-04 - mae: 0.0084 - mse: 1.6351e-04 - val_loss: 2.5076e-04 - val_mae: 0.0096 - val_mse: 2.5076e-04\n",
      "Epoch 198/200\n",
      "2440/2440 [==============================] - 1s 600us/step - loss: 1.6487e-04 - mae: 0.0085 - mse: 1.6487e-04 - val_loss: 2.5756e-04 - val_mae: 0.0100 - val_mse: 2.5756e-04\n",
      "Epoch 199/200\n",
      "2440/2440 [==============================] - 1s 601us/step - loss: 1.6605e-04 - mae: 0.0085 - mse: 1.6605e-04 - val_loss: 2.6632e-04 - val_mae: 0.0104 - val_mse: 2.6632e-04\n",
      "Epoch 200/200\n",
      "2440/2440 [==============================] - 1s 604us/step - loss: 1.6750e-04 - mae: 0.0086 - mse: 1.6750e-04 - val_loss: 2.7706e-04 - val_mae: 0.0109 - val_mse: 2.7706e-04\n"
     ]
    }
   ],
   "source": [
    "# Use same data from earlier to fit the model\n",
    "slstmhistory = slstm.fit(X_train_LSTM, y_train_LSTM,\n",
    "                       epochs=EPOCHS,\n",
    "                       validation_data=(X_test_LSTM, y_test_LSTM),\n",
    "                       verbose=1,\n",
    "                       shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stacked LSTM Model WITHOUT Mobility Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4222 samples, validate on 1056 samples\n",
      "Epoch 1/200\n",
      "4222/4222 [==============================] - 1s 328us/step - loss: 0.0335 - mae: 0.1160 - mse: 0.0335 - val_loss: 0.0243 - val_mae: 0.0976 - val_mse: 0.0243\n",
      "Epoch 2/200\n",
      "4222/4222 [==============================] - 1s 248us/step - loss: 0.0229 - mae: 0.0836 - mse: 0.0229 - val_loss: 0.0203 - val_mae: 0.0802 - val_mse: 0.0203\n",
      "Epoch 3/200\n",
      "4222/4222 [==============================] - 1s 243us/step - loss: 0.0197 - mae: 0.0712 - mse: 0.0197 - val_loss: 0.0183 - val_mae: 0.0719 - val_mse: 0.0183\n",
      "Epoch 4/200\n",
      "4222/4222 [==============================] - 1s 243us/step - loss: 0.0183 - mae: 0.0670 - mse: 0.0183 - val_loss: 0.0166 - val_mae: 0.0696 - val_mse: 0.0166\n",
      "Epoch 5/200\n",
      "4222/4222 [==============================] - 1s 250us/step - loss: 0.0168 - mae: 0.0640 - mse: 0.0168 - val_loss: 0.0150 - val_mae: 0.0691 - val_mse: 0.0150\n",
      "Epoch 6/200\n",
      "4222/4222 [==============================] - 1s 262us/step - loss: 0.0158 - mae: 0.0616 - mse: 0.0158 - val_loss: 0.0139 - val_mae: 0.0635 - val_mse: 0.0139\n",
      "Epoch 7/200\n",
      "4222/4222 [==============================] - 1s 283us/step - loss: 0.0152 - mae: 0.0600 - mse: 0.0152 - val_loss: 0.0134 - val_mae: 0.0610 - val_mse: 0.0134\n",
      "Epoch 8/200\n",
      "4222/4222 [==============================] - 1s 244us/step - loss: 0.0149 - mae: 0.0595 - mse: 0.0149 - val_loss: 0.0131 - val_mae: 0.0615 - val_mse: 0.0131\n",
      "Epoch 9/200\n",
      "4222/4222 [==============================] - 1s 250us/step - loss: 0.0147 - mae: 0.0592 - mse: 0.0147 - val_loss: 0.0129 - val_mae: 0.0630 - val_mse: 0.0129\n",
      "Epoch 10/200\n",
      "4222/4222 [==============================] - 1s 254us/step - loss: 0.0144 - mae: 0.0586 - mse: 0.0144 - val_loss: 0.0128 - val_mae: 0.0632 - val_mse: 0.0128\n",
      "Epoch 11/200\n",
      "4222/4222 [==============================] - 1s 266us/step - loss: 0.0141 - mae: 0.0578 - mse: 0.0141 - val_loss: 0.0126 - val_mae: 0.0625 - val_mse: 0.0126\n",
      "Epoch 12/200\n",
      "4222/4222 [==============================] - 1s 240us/step - loss: 0.0139 - mae: 0.0571 - mse: 0.0139 - val_loss: 0.0124 - val_mae: 0.0619 - val_mse: 0.0124\n",
      "Epoch 13/200\n",
      "4222/4222 [==============================] - 1s 253us/step - loss: 0.0137 - mae: 0.0566 - mse: 0.0137 - val_loss: 0.0122 - val_mae: 0.0611 - val_mse: 0.0122\n",
      "Epoch 14/200\n",
      "4222/4222 [==============================] - 1s 273us/step - loss: 0.0135 - mae: 0.0561 - mse: 0.0135 - val_loss: 0.0119 - val_mae: 0.0594 - val_mse: 0.0119\n",
      "Epoch 15/200\n",
      "4222/4222 [==============================] - 1s 256us/step - loss: 0.0133 - mae: 0.0553 - mse: 0.0133 - val_loss: 0.0115 - val_mae: 0.0563 - val_mse: 0.0115\n",
      "Epoch 16/200\n",
      "4222/4222 [==============================] - 1s 245us/step - loss: 0.0130 - mae: 0.0544 - mse: 0.0130 - val_loss: 0.0112 - val_mae: 0.0533 - val_mse: 0.0112\n",
      "Epoch 17/200\n",
      "4222/4222 [==============================] - 1s 249us/step - loss: 0.0128 - mae: 0.0533 - mse: 0.0128 - val_loss: 0.0110 - val_mae: 0.0514 - val_mse: 0.0110\n",
      "Epoch 18/200\n",
      "4222/4222 [==============================] - 1s 264us/step - loss: 0.0126 - mae: 0.0524 - mse: 0.0126 - val_loss: 0.0108 - val_mae: 0.0505 - val_mse: 0.0108\n",
      "Epoch 19/200\n",
      "4222/4222 [==============================] - 1s 262us/step - loss: 0.0124 - mae: 0.0515 - mse: 0.0124 - val_loss: 0.0106 - val_mae: 0.0503 - val_mse: 0.0106\n",
      "Epoch 20/200\n",
      "4222/4222 [==============================] - 1s 248us/step - loss: 0.0122 - mae: 0.0509 - mse: 0.0122 - val_loss: 0.0104 - val_mae: 0.0509 - val_mse: 0.0104\n",
      "Epoch 21/200\n",
      "4222/4222 [==============================] - 1s 259us/step - loss: 0.0119 - mae: 0.0503 - mse: 0.0119 - val_loss: 0.0103 - val_mae: 0.0517 - val_mse: 0.0103\n",
      "Epoch 22/200\n",
      "4222/4222 [==============================] - 1s 244us/step - loss: 0.0117 - mae: 0.0496 - mse: 0.0117 - val_loss: 0.0101 - val_mae: 0.0516 - val_mse: 0.0101\n",
      "Epoch 23/200\n",
      "4222/4222 [==============================] - 1s 247us/step - loss: 0.0115 - mae: 0.0487 - mse: 0.0115 - val_loss: 0.0099 - val_mae: 0.0504 - val_mse: 0.0099\n",
      "Epoch 24/200\n",
      "4222/4222 [==============================] - 1s 245us/step - loss: 0.0113 - mae: 0.0479 - mse: 0.0113 - val_loss: 0.0096 - val_mae: 0.0490 - val_mse: 0.0096\n",
      "Epoch 25/200\n",
      "4222/4222 [==============================] - 1s 243us/step - loss: 0.0110 - mae: 0.0471 - mse: 0.0110 - val_loss: 0.0094 - val_mae: 0.0476 - val_mse: 0.0094\n",
      "Epoch 26/200\n",
      "4222/4222 [==============================] - 1s 245us/step - loss: 0.0107 - mae: 0.0463 - mse: 0.0107 - val_loss: 0.0091 - val_mae: 0.0466 - val_mse: 0.0091\n",
      "Epoch 27/200\n",
      "4222/4222 [==============================] - 1s 251us/step - loss: 0.0105 - mae: 0.0453 - mse: 0.0105 - val_loss: 0.0089 - val_mae: 0.0464 - val_mse: 0.0089\n",
      "Epoch 28/200\n",
      "4222/4222 [==============================] - 1s 280us/step - loss: 0.0103 - mae: 0.0449 - mse: 0.0103 - val_loss: 0.0088 - val_mae: 0.0463 - val_mse: 0.0088\n",
      "Epoch 29/200\n",
      "4222/4222 [==============================] - 1s 269us/step - loss: 0.0102 - mae: 0.0448 - mse: 0.0102 - val_loss: 0.0086 - val_mae: 0.0461 - val_mse: 0.0086\n",
      "Epoch 30/200\n",
      "4222/4222 [==============================] - 1s 274us/step - loss: 0.0100 - mae: 0.0447 - mse: 0.0100 - val_loss: 0.0085 - val_mae: 0.0459 - val_mse: 0.0085\n",
      "Epoch 31/200\n",
      "4222/4222 [==============================] - 1s 274us/step - loss: 0.0099 - mae: 0.0445 - mse: 0.0099 - val_loss: 0.0085 - val_mae: 0.0457 - val_mse: 0.0085\n",
      "Epoch 32/200\n",
      "4222/4222 [==============================] - 1s 273us/step - loss: 0.0098 - mae: 0.0442 - mse: 0.0098 - val_loss: 0.0084 - val_mae: 0.0455 - val_mse: 0.0084\n",
      "Epoch 33/200\n",
      "4222/4222 [==============================] - 1s 283us/step - loss: 0.0096 - mae: 0.0441 - mse: 0.0096 - val_loss: 0.0083 - val_mae: 0.0453 - val_mse: 0.0083\n",
      "Epoch 34/200\n",
      "4222/4222 [==============================] - 1s 286us/step - loss: 0.0095 - mae: 0.0439 - mse: 0.0095 - val_loss: 0.0081 - val_mae: 0.0450 - val_mse: 0.0081\n",
      "Epoch 35/200\n",
      "4222/4222 [==============================] - 1s 287us/step - loss: 0.0094 - mae: 0.0437 - mse: 0.0094 - val_loss: 0.0080 - val_mae: 0.0446 - val_mse: 0.0080\n",
      "Epoch 36/200\n",
      "4222/4222 [==============================] - 1s 273us/step - loss: 0.0092 - mae: 0.0434 - mse: 0.0092 - val_loss: 0.0079 - val_mae: 0.0440 - val_mse: 0.0079\n",
      "Epoch 37/200\n",
      "4222/4222 [==============================] - 1s 273us/step - loss: 0.0090 - mae: 0.0429 - mse: 0.0090 - val_loss: 0.0079 - val_mae: 0.0456 - val_mse: 0.0079\n",
      "Epoch 38/200\n",
      "4222/4222 [==============================] - 1s 292us/step - loss: 0.0086 - mae: 0.0425 - mse: 0.0086 - val_loss: 0.0078 - val_mae: 0.0484 - val_mse: 0.0078\n",
      "Epoch 39/200\n",
      "4222/4222 [==============================] - 1s 288us/step - loss: 0.0083 - mae: 0.0421 - mse: 0.0083 - val_loss: 0.0078 - val_mae: 0.0489 - val_mse: 0.0078\n",
      "Epoch 40/200\n",
      "4222/4222 [==============================] - 1s 320us/step - loss: 0.0079 - mae: 0.0411 - mse: 0.0079 - val_loss: 0.0077 - val_mae: 0.0461 - val_mse: 0.0077\n",
      "Epoch 41/200\n",
      "4222/4222 [==============================] - 1s 298us/step - loss: 0.0076 - mae: 0.0400 - mse: 0.0076 - val_loss: 0.0074 - val_mae: 0.0448 - val_mse: 0.0074\n",
      "Epoch 42/200\n",
      "4222/4222 [==============================] - 1s 276us/step - loss: 0.0073 - mae: 0.0392 - mse: 0.0073 - val_loss: 0.0072 - val_mae: 0.0444 - val_mse: 0.0072\n",
      "Epoch 43/200\n",
      "4222/4222 [==============================] - 1s 275us/step - loss: 0.0070 - mae: 0.0383 - mse: 0.0070 - val_loss: 0.0070 - val_mae: 0.0435 - val_mse: 0.0070\n",
      "Epoch 44/200\n",
      "4222/4222 [==============================] - 1s 288us/step - loss: 0.0067 - mae: 0.0378 - mse: 0.0067 - val_loss: 0.0067 - val_mae: 0.0416 - val_mse: 0.0067\n",
      "Epoch 45/200\n",
      "4222/4222 [==============================] - 1s 278us/step - loss: 0.0063 - mae: 0.0370 - mse: 0.0063 - val_loss: 0.0063 - val_mae: 0.0392 - val_mse: 0.0063\n",
      "Epoch 46/200\n",
      "4222/4222 [==============================] - 1s 285us/step - loss: 0.0061 - mae: 0.0362 - mse: 0.0061 - val_loss: 0.0062 - val_mae: 0.0377 - val_mse: 0.0062\n",
      "Epoch 47/200\n",
      "4222/4222 [==============================] - 1s 282us/step - loss: 0.0057 - mae: 0.0346 - mse: 0.0057 - val_loss: 0.0059 - val_mae: 0.0380 - val_mse: 0.0059\n",
      "Epoch 48/200\n",
      "4222/4222 [==============================] - 1s 288us/step - loss: 0.0055 - mae: 0.0342 - mse: 0.0055 - val_loss: 0.0058 - val_mae: 0.0373 - val_mse: 0.0058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200\n",
      "4222/4222 [==============================] - 1s 286us/step - loss: 0.0051 - mae: 0.0328 - mse: 0.0051 - val_loss: 0.0059 - val_mae: 0.0398 - val_mse: 0.0059\n",
      "Epoch 50/200\n",
      "1088/4222 [======>.......................] - ETA: 0s - loss: 0.0069 - mae: 0.0346 - mse: 0.0069    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-159-d1be7cc70567>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                      \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_LSTM_nomob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_LSTM_nomob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                      \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                                      shuffle=False)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\andy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reuse No-mobility Data\n",
    "# X_LSTM_nomob, y_LSTM_nomob = get_LSTM_states_dataset(state_cases_df, n_steps=N_STEPS)\n",
    "# X_train_LSTM_nomob, X_test_LSTM_nomob, y_train_LSTM_nomob, y_test_LSTM_nomob = train_test_split(X_LSTM_nomob, y_LSTM_nomob, test_size=SPLIT, shuffle=False)\n",
    "\n",
    "# Set up model\n",
    "n_lstm_features = X_train_LSTM_nomob.shape[2]\n",
    "slstm_nomob = get_lstm(N_STEPS, n_lstm_features)\n",
    "\n",
    "# Fit LSTM\n",
    "slstm_nomob_history = slstm_nomob.fit(X_train_LSTM_nomob, y_train_LSTM_nomob,\n",
    "                                     epochs=EPOCHS,\n",
    "                                     validation_data=(X_test_LSTM_nomob, y_test_LSTM_nomob),\n",
    "                                     verbose=1,\n",
    "                                     shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN Model with Mobility Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up CNN, Boo's model\n",
    "def get_CNN_model(n_steps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, \n",
    "                     activation='relu', \n",
    "                     input_shape=(n_steps, n_features)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "n_CNN_features = X_train_CNN.shape[2]\n",
    "cnn = get_CNN_model(N_STEPS, n_CNN_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2440 samples, validate on 610 samples\n",
      "Epoch 1/200\n",
      "2440/2440 [==============================] - 0s 108us/step - loss: 0.0267 - mae: 0.1135 - mse: 0.0267 - val_loss: 0.0038 - val_mae: 0.0474 - val_mse: 0.0038\n",
      "Epoch 2/200\n",
      "2440/2440 [==============================] - 0s 65us/step - loss: 0.0034 - mae: 0.0444 - mse: 0.0034 - val_loss: 0.0024 - val_mae: 0.0364 - val_mse: 0.0024\n",
      "Epoch 3/200\n",
      "2440/2440 [==============================] - 0s 59us/step - loss: 0.0024 - mae: 0.0375 - mse: 0.0024 - val_loss: 0.0020 - val_mae: 0.0317 - val_mse: 0.0020\n",
      "Epoch 4/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 0.0020 - mae: 0.0337 - mse: 0.0020 - val_loss: 0.0018 - val_mae: 0.0304 - val_mse: 0.0018\n",
      "Epoch 5/200\n",
      "2440/2440 [==============================] - 0s 66us/step - loss: 0.0016 - mae: 0.0303 - mse: 0.0016 - val_loss: 0.0016 - val_mae: 0.0290 - val_mse: 0.0016\n",
      "Epoch 6/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 0.0015 - mae: 0.0286 - mse: 0.0015 - val_loss: 0.0021 - val_mae: 0.0386 - val_mse: 0.0021\n",
      "Epoch 7/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 0.0014 - mae: 0.0277 - mse: 0.0014 - val_loss: 0.0014 - val_mae: 0.0279 - val_mse: 0.0014\n",
      "Epoch 8/200\n",
      "2440/2440 [==============================] - 0s 62us/step - loss: 0.0013 - mae: 0.0268 - mse: 0.0013 - val_loss: 0.0012 - val_mae: 0.0250 - val_mse: 0.0012\n",
      "Epoch 9/200\n",
      "2440/2440 [==============================] - 0s 66us/step - loss: 0.0012 - mae: 0.0258 - mse: 0.0012 - val_loss: 0.0012 - val_mae: 0.0251 - val_mse: 0.0012\n",
      "Epoch 10/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 0.0011 - mae: 0.0244 - mse: 0.0011 - val_loss: 0.0011 - val_mae: 0.0244 - val_mse: 0.0011\n",
      "Epoch 11/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 0.0010 - mae: 0.0233 - mse: 0.0010 - val_loss: 0.0011 - val_mae: 0.0248 - val_mse: 0.0011\n",
      "Epoch 12/200\n",
      "2440/2440 [==============================] - 0s 59us/step - loss: 9.4084e-04 - mae: 0.0223 - mse: 9.4084e-04 - val_loss: 0.0011 - val_mae: 0.0244 - val_mse: 0.0011\n",
      "Epoch 13/200\n",
      "2440/2440 [==============================] - 0s 61us/step - loss: 8.8684e-04 - mae: 0.0216 - mse: 8.8684e-04 - val_loss: 0.0011 - val_mae: 0.0248 - val_mse: 0.0011\n",
      "Epoch 14/200\n",
      "2440/2440 [==============================] - 0s 63us/step - loss: 8.4703e-04 - mae: 0.0211 - mse: 8.4703e-04 - val_loss: 0.0011 - val_mae: 0.0245 - val_mse: 0.0011\n",
      "Epoch 15/200\n",
      "2440/2440 [==============================] - 0s 61us/step - loss: 8.0035e-04 - mae: 0.0205 - mse: 8.0035e-04 - val_loss: 0.0011 - val_mae: 0.0243 - val_mse: 0.0011\n",
      "Epoch 16/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 7.6493e-04 - mae: 0.0200 - mse: 7.6493e-04 - val_loss: 0.0010 - val_mae: 0.0242 - val_mse: 0.0010\n",
      "Epoch 17/200\n",
      "2440/2440 [==============================] - 0s 65us/step - loss: 7.3219e-04 - mae: 0.0195 - mse: 7.3219e-04 - val_loss: 0.0010 - val_mae: 0.0245 - val_mse: 0.0010\n",
      "Epoch 18/200\n",
      "2440/2440 [==============================] - 0s 61us/step - loss: 7.0408e-04 - mae: 0.0191 - mse: 7.0408e-04 - val_loss: 0.0010 - val_mae: 0.0244 - val_mse: 0.0010\n",
      "Epoch 19/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 6.7420e-04 - mae: 0.0187 - mse: 6.7420e-04 - val_loss: 0.0011 - val_mae: 0.0262 - val_mse: 0.0011\n",
      "Epoch 20/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 6.3678e-04 - mae: 0.0181 - mse: 6.3678e-04 - val_loss: 0.0011 - val_mae: 0.0259 - val_mse: 0.0011\n",
      "Epoch 21/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 6.1025e-04 - mae: 0.0176 - mse: 6.1025e-04 - val_loss: 0.0011 - val_mae: 0.0263 - val_mse: 0.0011\n",
      "Epoch 22/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 5.8902e-04 - mae: 0.0173 - mse: 5.8902e-04 - val_loss: 0.0011 - val_mae: 0.0262 - val_mse: 0.0011\n",
      "Epoch 23/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 5.7471e-04 - mae: 0.0171 - mse: 5.7471e-04 - val_loss: 0.0011 - val_mae: 0.0260 - val_mse: 0.0011\n",
      "Epoch 24/200\n",
      "2440/2440 [==============================] - 0s 76us/step - loss: 5.5774e-04 - mae: 0.0169 - mse: 5.5774e-04 - val_loss: 0.0010 - val_mae: 0.0255 - val_mse: 0.0010\n",
      "Epoch 25/200\n",
      "2440/2440 [==============================] - 0s 63us/step - loss: 5.4313e-04 - mae: 0.0166 - mse: 5.4313e-04 - val_loss: 0.0010 - val_mae: 0.0252 - val_mse: 0.0010\n",
      "Epoch 26/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 5.3249e-04 - mae: 0.0165 - mse: 5.3249e-04 - val_loss: 0.0010 - val_mae: 0.0259 - val_mse: 0.0010\n",
      "Epoch 27/200\n",
      "2440/2440 [==============================] - 0s 65us/step - loss: 5.2420e-04 - mae: 0.0165 - mse: 5.2420e-04 - val_loss: 0.0010 - val_mae: 0.0253 - val_mse: 0.0010\n",
      "Epoch 28/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 5.0742e-04 - mae: 0.0162 - mse: 5.0742e-04 - val_loss: 0.0010 - val_mae: 0.0253 - val_mse: 0.0010\n",
      "Epoch 29/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 4.9815e-04 - mae: 0.0161 - mse: 4.9815e-04 - val_loss: 0.0010 - val_mae: 0.0254 - val_mse: 0.0010\n",
      "Epoch 30/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 4.8366e-04 - mae: 0.0158 - mse: 4.8366e-04 - val_loss: 9.4475e-04 - val_mae: 0.0243 - val_mse: 9.4475e-04\n",
      "Epoch 31/200\n",
      "2440/2440 [==============================] - 0s 51us/step - loss: 4.6283e-04 - mae: 0.0154 - mse: 4.6283e-04 - val_loss: 9.3035e-04 - val_mae: 0.0241 - val_mse: 9.3035e-04\n",
      "Epoch 32/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 4.4971e-04 - mae: 0.0151 - mse: 4.4971e-04 - val_loss: 8.9420e-04 - val_mae: 0.0233 - val_mse: 8.9420e-04\n",
      "Epoch 33/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 4.3644e-04 - mae: 0.0148 - mse: 4.3644e-04 - val_loss: 8.4224e-04 - val_mae: 0.0224 - val_mse: 8.4224e-04\n",
      "Epoch 34/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 4.2672e-04 - mae: 0.0146 - mse: 4.2672e-04 - val_loss: 8.7650e-04 - val_mae: 0.0230 - val_mse: 8.7650e-04\n",
      "Epoch 35/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 4.2419e-04 - mae: 0.0146 - mse: 4.2419e-04 - val_loss: 8.4441e-04 - val_mae: 0.0223 - val_mse: 8.4441e-04\n",
      "Epoch 36/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 4.0706e-04 - mae: 0.0143 - mse: 4.0706e-04 - val_loss: 8.5440e-04 - val_mae: 0.0226 - val_mse: 8.5440e-04\n",
      "Epoch 37/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 4.0627e-04 - mae: 0.0144 - mse: 4.0627e-04 - val_loss: 8.3399e-04 - val_mae: 0.0221 - val_mse: 8.3399e-04\n",
      "Epoch 38/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 3.9579e-04 - mae: 0.0142 - mse: 3.9579e-04 - val_loss: 7.8542e-04 - val_mae: 0.0206 - val_mse: 7.8542e-04\n",
      "Epoch 39/200\n",
      "2440/2440 [==============================] - 0s 51us/step - loss: 3.9944e-04 - mae: 0.0142 - mse: 3.9944e-04 - val_loss: 8.3385e-04 - val_mae: 0.0220 - val_mse: 8.3385e-04\n",
      "Epoch 40/200\n",
      "2440/2440 [==============================] - 0s 50us/step - loss: 3.7382e-04 - mae: 0.0139 - mse: 3.7382e-04 - val_loss: 8.3130e-04 - val_mae: 0.0222 - val_mse: 8.3130e-04\n",
      "Epoch 41/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 3.7333e-04 - mae: 0.0139 - mse: 3.7333e-04 - val_loss: 8.3000e-04 - val_mae: 0.0218 - val_mse: 8.3000e-04\n",
      "Epoch 42/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 3.6585e-04 - mae: 0.0137 - mse: 3.6585e-04 - val_loss: 8.1186e-04 - val_mae: 0.0214 - val_mse: 8.1186e-04\n",
      "Epoch 43/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 3.5910e-04 - mae: 0.0136 - mse: 3.5910e-04 - val_loss: 8.0275e-04 - val_mae: 0.0212 - val_mse: 8.0275e-04\n",
      "Epoch 44/200\n",
      "2440/2440 [==============================] - 0s 60us/step - loss: 3.5779e-04 - mae: 0.0136 - mse: 3.5779e-04 - val_loss: 8.0697e-04 - val_mae: 0.0216 - val_mse: 8.0697e-04\n",
      "Epoch 45/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 3.5519e-04 - mae: 0.0136 - mse: 3.5519e-04 - val_loss: 8.1694e-04 - val_mae: 0.0216 - val_mse: 8.1694e-04\n",
      "Epoch 46/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 3.5244e-04 - mae: 0.0136 - mse: 3.5244e-04 - val_loss: 8.1217e-04 - val_mae: 0.0213 - val_mse: 8.1217e-04\n",
      "Epoch 47/200\n",
      "2440/2440 [==============================] - 0s 49us/step - loss: 3.4364e-04 - mae: 0.0134 - mse: 3.4364e-04 - val_loss: 7.9681e-04 - val_mae: 0.0212 - val_mse: 7.9681e-04\n",
      "Epoch 48/200\n",
      "2440/2440 [==============================] - 0s 51us/step - loss: 3.2977e-04 - mae: 0.0131 - mse: 3.2977e-04 - val_loss: 7.7412e-04 - val_mae: 0.0206 - val_mse: 7.7412e-04\n",
      "Epoch 49/200\n",
      "2440/2440 [==============================] - 0s 63us/step - loss: 3.3023e-04 - mae: 0.0131 - mse: 3.3023e-04 - val_loss: 8.2141e-04 - val_mae: 0.0216 - val_mse: 8.2141e-04\n",
      "Epoch 50/200\n",
      "2440/2440 [==============================] - 0s 49us/step - loss: 3.2016e-04 - mae: 0.0129 - mse: 3.2016e-04 - val_loss: 7.8168e-04 - val_mae: 0.0206 - val_mse: 7.8168e-04\n",
      "Epoch 51/200\n",
      "2440/2440 [==============================] - 0s 50us/step - loss: 3.1616e-04 - mae: 0.0126 - mse: 3.1616e-04 - val_loss: 7.4501e-04 - val_mae: 0.0194 - val_mse: 7.4501e-04\n",
      "Epoch 52/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 3.0873e-04 - mae: 0.0124 - mse: 3.0873e-04 - val_loss: 7.7168e-04 - val_mae: 0.0199 - val_mse: 7.7168e-04\n",
      "Epoch 53/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 3.0468e-04 - mae: 0.0124 - mse: 3.0468e-04 - val_loss: 7.5457e-04 - val_mae: 0.0193 - val_mse: 7.5457e-04\n",
      "Epoch 54/200\n",
      "2440/2440 [==============================] - 0s 51us/step - loss: 2.8914e-04 - mae: 0.0119 - mse: 2.8914e-04 - val_loss: 7.3500e-04 - val_mae: 0.0189 - val_mse: 7.3500e-04\n",
      "Epoch 55/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 2.9267e-04 - mae: 0.0120 - mse: 2.9267e-04 - val_loss: 7.2984e-04 - val_mae: 0.0182 - val_mse: 7.2984e-04\n",
      "Epoch 56/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.8550e-04 - mae: 0.0118 - mse: 2.8550e-04 - val_loss: 7.3713e-04 - val_mae: 0.0186 - val_mse: 7.3713e-04\n",
      "Epoch 57/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.8631e-04 - mae: 0.0119 - mse: 2.8631e-04 - val_loss: 7.8664e-04 - val_mae: 0.0203 - val_mse: 7.8664e-04\n",
      "Epoch 58/200\n",
      "2440/2440 [==============================] - 0s 62us/step - loss: 2.7877e-04 - mae: 0.0118 - mse: 2.7877e-04 - val_loss: 7.7783e-04 - val_mae: 0.0199 - val_mse: 7.7783e-04\n",
      "Epoch 59/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 2.7946e-04 - mae: 0.0119 - mse: 2.7946e-04 - val_loss: 7.5885e-04 - val_mae: 0.0189 - val_mse: 7.5885e-04\n",
      "Epoch 60/200\n",
      "2440/2440 [==============================] - 0s 84us/step - loss: 2.7370e-04 - mae: 0.0117 - mse: 2.7370e-04 - val_loss: 7.6886e-04 - val_mae: 0.0193 - val_mse: 7.6886e-04\n",
      "Epoch 61/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 2.6798e-04 - mae: 0.0114 - mse: 2.6798e-04 - val_loss: 7.5597e-04 - val_mae: 0.0188 - val_mse: 7.5597e-04\n",
      "Epoch 62/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 2.7310e-04 - mae: 0.0117 - mse: 2.7310e-04 - val_loss: 7.6212e-04 - val_mae: 0.0189 - val_mse: 7.6212e-04\n",
      "Epoch 63/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 2.6606e-04 - mae: 0.0115 - mse: 2.6606e-04 - val_loss: 7.6325e-04 - val_mae: 0.0184 - val_mse: 7.6325e-04\n",
      "Epoch 64/200\n",
      "2440/2440 [==============================] - 0s 67us/step - loss: 2.5862e-04 - mae: 0.0114 - mse: 2.5862e-04 - val_loss: 7.5988e-04 - val_mae: 0.0182 - val_mse: 7.5988e-04\n",
      "Epoch 65/200\n",
      "2440/2440 [==============================] - 0s 62us/step - loss: 2.5659e-04 - mae: 0.0113 - mse: 2.5659e-04 - val_loss: 7.6556e-04 - val_mae: 0.0185 - val_mse: 7.6556e-04\n",
      "Epoch 66/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.5704e-04 - mae: 0.0114 - mse: 2.5704e-04 - val_loss: 8.1102e-04 - val_mae: 0.0194 - val_mse: 8.1102e-04\n",
      "Epoch 67/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.5968e-04 - mae: 0.0116 - mse: 2.5968e-04 - val_loss: 7.8391e-04 - val_mae: 0.0186 - val_mse: 7.8391e-04\n",
      "Epoch 68/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.6710e-04 - mae: 0.0119 - mse: 2.6710e-04 - val_loss: 8.8375e-04 - val_mae: 0.0205 - val_mse: 8.8375e-04\n",
      "Epoch 69/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.9693e-04 - mae: 0.0129 - mse: 2.9693e-04 - val_loss: 8.6822e-04 - val_mae: 0.0199 - val_mse: 8.6822e-04\n",
      "Epoch 70/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.9695e-04 - mae: 0.0126 - mse: 2.9695e-04 - val_loss: 9.1900e-04 - val_mae: 0.0199 - val_mse: 9.1900e-04\n",
      "Epoch 71/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 3.2127e-04 - mae: 0.0134 - mse: 3.2127e-04 - val_loss: 8.6895e-04 - val_mae: 0.0191 - val_mse: 8.6895e-04\n",
      "Epoch 72/200\n",
      "2440/2440 [==============================] - 0s 70us/step - loss: 3.0102e-04 - mae: 0.0130 - mse: 3.0102e-04 - val_loss: 9.4502e-04 - val_mae: 0.0204 - val_mse: 9.4502e-04\n",
      "Epoch 73/200\n",
      "2440/2440 [==============================] - 0s 65us/step - loss: 3.3820e-04 - mae: 0.0142 - mse: 3.3820e-04 - val_loss: 8.5479e-04 - val_mae: 0.0201 - val_mse: 8.5479e-04\n",
      "Epoch 74/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 3.4536e-04 - mae: 0.0144 - mse: 3.4536e-04 - val_loss: 9.2662e-04 - val_mae: 0.0210 - val_mse: 9.2662e-04\n",
      "Epoch 75/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 3.0426e-04 - mae: 0.0133 - mse: 3.0426e-04 - val_loss: 9.4682e-04 - val_mae: 0.0214 - val_mse: 9.4682e-04\n",
      "Epoch 76/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 3.7767e-04 - mae: 0.0148 - mse: 3.7767e-04 - val_loss: 9.1638e-04 - val_mae: 0.0213 - val_mse: 9.1638e-04\n",
      "Epoch 77/200\n",
      "2440/2440 [==============================] - 0s 62us/step - loss: 3.4355e-04 - mae: 0.0138 - mse: 3.4355e-04 - val_loss: 8.7488e-04 - val_mae: 0.0202 - val_mse: 8.7488e-04\n",
      "Epoch 78/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 3.0467e-04 - mae: 0.0128 - mse: 3.0467e-04 - val_loss: 9.7714e-04 - val_mae: 0.0214 - val_mse: 9.7714e-04\n",
      "Epoch 79/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 3.5543e-04 - mae: 0.0138 - mse: 3.5543e-04 - val_loss: 9.4860e-04 - val_mae: 0.0216 - val_mse: 9.4860e-04\n",
      "Epoch 80/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 4.1092e-04 - mae: 0.0149 - mse: 4.1092e-04 - val_loss: 8.3177e-04 - val_mae: 0.0212 - val_mse: 8.3177e-04\n",
      "Epoch 81/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 3.9630e-04 - mae: 0.0147 - mse: 3.9630e-04 - val_loss: 7.8114e-04 - val_mae: 0.0200 - val_mse: 7.8114e-04\n",
      "Epoch 82/200\n",
      "2440/2440 [==============================] - 0s 63us/step - loss: 2.8087e-04 - mae: 0.0123 - mse: 2.8087e-04 - val_loss: 8.6724e-04 - val_mae: 0.0197 - val_mse: 8.6724e-04\n",
      "Epoch 83/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 2.4842e-04 - mae: 0.0113 - mse: 2.4842e-04 - val_loss: 8.1754e-04 - val_mae: 0.0201 - val_mse: 8.1754e-04\n",
      "Epoch 84/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 2.2779e-04 - mae: 0.0109 - mse: 2.2779e-04 - val_loss: 8.4810e-04 - val_mae: 0.0203 - val_mse: 8.4810e-04\n",
      "Epoch 85/200\n",
      "2440/2440 [==============================] - 0s 60us/step - loss: 2.2221e-04 - mae: 0.0109 - mse: 2.2221e-04 - val_loss: 8.3266e-04 - val_mae: 0.0204 - val_mse: 8.3266e-04\n",
      "Epoch 86/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.1705e-04 - mae: 0.0107 - mse: 2.1705e-04 - val_loss: 8.5085e-04 - val_mae: 0.0206 - val_mse: 8.5085e-04\n",
      "Epoch 87/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.1684e-04 - mae: 0.0108 - mse: 2.1684e-04 - val_loss: 8.4865e-04 - val_mae: 0.0207 - val_mse: 8.4865e-04\n",
      "Epoch 88/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 2.1162e-04 - mae: 0.0105 - mse: 2.1162e-04 - val_loss: 8.4454e-04 - val_mae: 0.0206 - val_mse: 8.4454e-04\n",
      "Epoch 89/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 2.0815e-04 - mae: 0.0104 - mse: 2.0815e-04 - val_loss: 8.4074e-04 - val_mae: 0.0209 - val_mse: 8.4074e-04\n",
      "Epoch 90/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2440/2440 [==============================] - 0s 57us/step - loss: 2.1775e-04 - mae: 0.0106 - mse: 2.1775e-04 - val_loss: 8.6006e-04 - val_mae: 0.0211 - val_mse: 8.6006e-04\n",
      "Epoch 91/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.0336e-04 - mae: 0.0103 - mse: 2.0336e-04 - val_loss: 8.6102e-04 - val_mae: 0.0211 - val_mse: 8.6102e-04\n",
      "Epoch 92/200\n",
      "2440/2440 [==============================] - 0s 62us/step - loss: 1.9837e-04 - mae: 0.0102 - mse: 1.9837e-04 - val_loss: 8.9843e-04 - val_mae: 0.0214 - val_mse: 8.9843e-04\n",
      "Epoch 93/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 2.0191e-04 - mae: 0.0106 - mse: 2.0191e-04 - val_loss: 9.3765e-04 - val_mae: 0.0216 - val_mse: 9.3765e-04\n",
      "Epoch 94/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.2001e-04 - mae: 0.0111 - mse: 2.2001e-04 - val_loss: 9.5735e-04 - val_mae: 0.0214 - val_mse: 9.5735e-04\n",
      "Epoch 95/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.3568e-04 - mae: 0.0115 - mse: 2.3568e-04 - val_loss: 9.5373e-04 - val_mae: 0.0208 - val_mse: 9.5373e-04\n",
      "Epoch 96/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.2753e-04 - mae: 0.0111 - mse: 2.2753e-04 - val_loss: 8.9649e-04 - val_mae: 0.0203 - val_mse: 8.9649e-04\n",
      "Epoch 97/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.1908e-04 - mae: 0.0107 - mse: 2.1908e-04 - val_loss: 8.8824e-04 - val_mae: 0.0205 - val_mse: 8.8824e-04\n",
      "Epoch 98/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.2011e-04 - mae: 0.0108 - mse: 2.2011e-04 - val_loss: 9.1316e-04 - val_mae: 0.0212 - val_mse: 9.1316e-04\n",
      "Epoch 99/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.7516e-04 - mae: 0.0117 - mse: 2.7516e-04 - val_loss: 9.2384e-04 - val_mae: 0.0226 - val_mse: 9.2384e-04\n",
      "Epoch 100/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 3.4489e-04 - mae: 0.0131 - mse: 3.4489e-04 - val_loss: 0.0010 - val_mae: 0.0248 - val_mse: 0.0010\n",
      "Epoch 101/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 3.4406e-04 - mae: 0.0129 - mse: 3.4406e-04 - val_loss: 8.7981e-04 - val_mae: 0.0213 - val_mse: 8.7981e-04\n",
      "Epoch 102/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 2.6769e-04 - mae: 0.0116 - mse: 2.6769e-04 - val_loss: 8.3324e-04 - val_mae: 0.0212 - val_mse: 8.3324e-04\n",
      "Epoch 103/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 2.5229e-04 - mae: 0.0116 - mse: 2.5229e-04 - val_loss: 9.0333e-04 - val_mae: 0.0227 - val_mse: 9.0333e-04\n",
      "Epoch 104/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.3513e-04 - mae: 0.0113 - mse: 2.3513e-04 - val_loss: 9.0073e-04 - val_mae: 0.0224 - val_mse: 9.0073e-04\n",
      "Epoch 105/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.3112e-04 - mae: 0.0111 - mse: 2.3112e-04 - val_loss: 9.2169e-04 - val_mae: 0.0229 - val_mse: 9.2169e-04\n",
      "Epoch 106/200\n",
      "2440/2440 [==============================] - 0s 59us/step - loss: 2.5486e-04 - mae: 0.0114 - mse: 2.5486e-04 - val_loss: 0.0011 - val_mae: 0.0258 - val_mse: 0.0011\n",
      "Epoch 107/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 2.4815e-04 - mae: 0.0112 - mse: 2.4815e-04 - val_loss: 9.2668e-04 - val_mae: 0.0225 - val_mse: 9.2668e-04\n",
      "Epoch 108/200\n",
      "2440/2440 [==============================] - 0s 60us/step - loss: 2.3799e-04 - mae: 0.0110 - mse: 2.3799e-04 - val_loss: 9.5905e-04 - val_mae: 0.0235 - val_mse: 9.5905e-04\n",
      "Epoch 109/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.3933e-04 - mae: 0.0109 - mse: 2.3933e-04 - val_loss: 9.3617e-04 - val_mae: 0.0228 - val_mse: 9.3617e-04\n",
      "Epoch 110/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.6650e-04 - mae: 0.0116 - mse: 2.6650e-04 - val_loss: 0.0011 - val_mae: 0.0255 - val_mse: 0.0011\n",
      "Epoch 111/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.6363e-04 - mae: 0.0115 - mse: 2.6363e-04 - val_loss: 0.0010 - val_mae: 0.0246 - val_mse: 0.0010\n",
      "Epoch 112/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.9574e-04 - mae: 0.0123 - mse: 2.9574e-04 - val_loss: 0.0012 - val_mae: 0.0274 - val_mse: 0.0012\n",
      "Epoch 113/200\n",
      "2440/2440 [==============================] - 0s 59us/step - loss: 3.8895e-04 - mae: 0.0140 - mse: 3.8895e-04 - val_loss: 0.0011 - val_mae: 0.0253 - val_mse: 0.0011\n",
      "Epoch 114/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 4.4144e-04 - mae: 0.0150 - mse: 4.4144e-04 - val_loss: 0.0010 - val_mae: 0.0237 - val_mse: 0.0010\n",
      "Epoch 115/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 6.0990e-04 - mae: 0.0178 - mse: 6.0990e-04 - val_loss: 0.0011 - val_mae: 0.0220 - val_mse: 0.0011\n",
      "Epoch 116/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 6.8770e-04 - mae: 0.0182 - mse: 6.8770e-04 - val_loss: 0.0012 - val_mae: 0.0227 - val_mse: 0.0012\n",
      "Epoch 117/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 5.7256e-04 - mae: 0.0170 - mse: 5.7256e-04 - val_loss: 0.0013 - val_mae: 0.0251 - val_mse: 0.0013\n",
      "Epoch 118/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 5.4921e-04 - mae: 0.0164 - mse: 5.4921e-04 - val_loss: 0.0014 - val_mae: 0.0246 - val_mse: 0.0014\n",
      "Epoch 119/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 5.0497e-04 - mae: 0.0151 - mse: 5.0497e-04 - val_loss: 0.0015 - val_mae: 0.0251 - val_mse: 0.0015\n",
      "Epoch 120/200\n",
      "2440/2440 [==============================] - 0s 64us/step - loss: 5.0077e-04 - mae: 0.0148 - mse: 5.0077e-04 - val_loss: 0.0015 - val_mae: 0.0260 - val_mse: 0.0015\n",
      "Epoch 121/200\n",
      "2440/2440 [==============================] - 0s 65us/step - loss: 4.7977e-04 - mae: 0.0147 - mse: 4.7977e-04 - val_loss: 0.0017 - val_mae: 0.0273 - val_mse: 0.0017\n",
      "Epoch 122/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 5.1515e-04 - mae: 0.0152 - mse: 5.1515e-04 - val_loss: 0.0021 - val_mae: 0.0303 - val_mse: 0.0021\n",
      "Epoch 123/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 6.1368e-04 - mae: 0.0165 - mse: 6.1368e-04 - val_loss: 0.0022 - val_mae: 0.0305 - val_mse: 0.0022\n",
      "Epoch 124/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 7.5485e-04 - mae: 0.0181 - mse: 7.5485e-04 - val_loss: 0.0021 - val_mae: 0.0295 - val_mse: 0.0021\n",
      "Epoch 125/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 9.1566e-04 - mae: 0.0198 - mse: 9.1566e-04 - val_loss: 0.0020 - val_mae: 0.0291 - val_mse: 0.0020\n",
      "Epoch 126/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 0.0010 - mae: 0.0208 - mse: 0.0010 - val_loss: 0.0013 - val_mae: 0.0247 - val_mse: 0.0013\n",
      "Epoch 127/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 9.6475e-04 - mae: 0.0203 - mse: 9.6475e-04 - val_loss: 9.3671e-04 - val_mae: 0.0205 - val_mse: 9.3671e-04\n",
      "Epoch 128/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 7.3801e-04 - mae: 0.0179 - mse: 7.3801e-04 - val_loss: 8.3754e-04 - val_mae: 0.0193 - val_mse: 8.3754e-04\n",
      "Epoch 129/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 5.2297e-04 - mae: 0.0152 - mse: 5.2297e-04 - val_loss: 8.2588e-04 - val_mae: 0.0189 - val_mse: 8.2588e-04\n",
      "Epoch 130/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 4.2914e-04 - mae: 0.0138 - mse: 4.2914e-04 - val_loss: 7.7192e-04 - val_mae: 0.0185 - val_mse: 7.7192e-04\n",
      "Epoch 131/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 3.7042e-04 - mae: 0.0129 - mse: 3.7042e-04 - val_loss: 7.9256e-04 - val_mae: 0.0190 - val_mse: 7.9256e-04\n",
      "Epoch 132/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 3.3153e-04 - mae: 0.0122 - mse: 3.3153e-04 - val_loss: 7.5317e-04 - val_mae: 0.0184 - val_mse: 7.5317e-04\n",
      "Epoch 133/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 3.0101e-04 - mae: 0.0116 - mse: 3.0101e-04 - val_loss: 7.7992e-04 - val_mae: 0.0189 - val_mse: 7.7992e-04\n",
      "Epoch 134/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 2.7890e-04 - mae: 0.0112 - mse: 2.7890e-04 - val_loss: 7.6441e-04 - val_mae: 0.0190 - val_mse: 7.6441e-04\n",
      "Epoch 135/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2440/2440 [==============================] - 0s 61us/step - loss: 2.6295e-04 - mae: 0.0109 - mse: 2.6295e-04 - val_loss: 7.8051e-04 - val_mae: 0.0194 - val_mse: 7.8051e-04\n",
      "Epoch 136/200\n",
      "2440/2440 [==============================] - 0s 59us/step - loss: 2.5333e-04 - mae: 0.0107 - mse: 2.5333e-04 - val_loss: 7.9060e-04 - val_mae: 0.0196 - val_mse: 7.9060e-04\n",
      "Epoch 137/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 2.4360e-04 - mae: 0.0105 - mse: 2.4360e-04 - val_loss: 8.0171e-04 - val_mae: 0.0197 - val_mse: 8.0171e-04\n",
      "Epoch 138/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.3937e-04 - mae: 0.0104 - mse: 2.3937e-04 - val_loss: 8.1011e-04 - val_mae: 0.0197 - val_mse: 8.1011e-04\n",
      "Epoch 139/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.3558e-04 - mae: 0.0103 - mse: 2.3558e-04 - val_loss: 8.2549e-04 - val_mae: 0.0199 - val_mse: 8.2549e-04\n",
      "Epoch 140/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.2913e-04 - mae: 0.0102 - mse: 2.2913e-04 - val_loss: 8.3040e-04 - val_mae: 0.0198 - val_mse: 8.3040e-04\n",
      "Epoch 141/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 2.3044e-04 - mae: 0.0103 - mse: 2.3044e-04 - val_loss: 8.6694e-04 - val_mae: 0.0196 - val_mse: 8.6694e-04\n",
      "Epoch 142/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.3233e-04 - mae: 0.0103 - mse: 2.3233e-04 - val_loss: 8.8761e-04 - val_mae: 0.0203 - val_mse: 8.8761e-04\n",
      "Epoch 143/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 2.2777e-04 - mae: 0.0102 - mse: 2.2777e-04 - val_loss: 9.0471e-04 - val_mae: 0.0204 - val_mse: 9.0471e-04\n",
      "Epoch 144/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.2666e-04 - mae: 0.0101 - mse: 2.2666e-04 - val_loss: 9.4678e-04 - val_mae: 0.0206 - val_mse: 9.4678e-04\n",
      "Epoch 145/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.3043e-04 - mae: 0.0103 - mse: 2.3043e-04 - val_loss: 9.4324e-04 - val_mae: 0.0209 - val_mse: 9.4324e-04\n",
      "Epoch 146/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.3084e-04 - mae: 0.0103 - mse: 2.3084e-04 - val_loss: 9.7733e-04 - val_mae: 0.0206 - val_mse: 9.7733e-04\n",
      "Epoch 147/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.3475e-04 - mae: 0.0103 - mse: 2.3475e-04 - val_loss: 9.9134e-04 - val_mae: 0.0207 - val_mse: 9.9135e-04\n",
      "Epoch 148/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.3670e-04 - mae: 0.0103 - mse: 2.3670e-04 - val_loss: 0.0010 - val_mae: 0.0210 - val_mse: 0.0010\n",
      "Epoch 149/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 2.4025e-04 - mae: 0.0105 - mse: 2.4025e-04 - val_loss: 0.0011 - val_mae: 0.0210 - val_mse: 0.0011\n",
      "Epoch 150/200\n",
      "2440/2440 [==============================] - 0s 60us/step - loss: 2.4389e-04 - mae: 0.0106 - mse: 2.4389e-04 - val_loss: 0.0010 - val_mae: 0.0209 - val_mse: 0.0010\n",
      "Epoch 151/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.4474e-04 - mae: 0.0106 - mse: 2.4474e-04 - val_loss: 0.0011 - val_mae: 0.0212 - val_mse: 0.0011\n",
      "Epoch 152/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.5459e-04 - mae: 0.0107 - mse: 2.5459e-04 - val_loss: 0.0011 - val_mae: 0.0213 - val_mse: 0.0011\n",
      "Epoch 153/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.5889e-04 - mae: 0.0108 - mse: 2.5889e-04 - val_loss: 0.0011 - val_mae: 0.0215 - val_mse: 0.0011\n",
      "Epoch 154/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.7106e-04 - mae: 0.0110 - mse: 2.7106e-04 - val_loss: 0.0011 - val_mae: 0.0218 - val_mse: 0.0011\n",
      "Epoch 155/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 2.8430e-04 - mae: 0.0112 - mse: 2.8430e-04 - val_loss: 0.0012 - val_mae: 0.0227 - val_mse: 0.0012\n",
      "Epoch 156/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.9535e-04 - mae: 0.0115 - mse: 2.9535e-04 - val_loss: 0.0012 - val_mae: 0.0226 - val_mse: 0.0012\n",
      "Epoch 157/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 3.1312e-04 - mae: 0.0118 - mse: 3.1312e-04 - val_loss: 0.0012 - val_mae: 0.0235 - val_mse: 0.0012\n",
      "Epoch 158/200\n",
      "2440/2440 [==============================] - 0s 59us/step - loss: 3.4111e-04 - mae: 0.0122 - mse: 3.4111e-04 - val_loss: 0.0012 - val_mae: 0.0230 - val_mse: 0.0012\n",
      "Epoch 159/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 3.6231e-04 - mae: 0.0127 - mse: 3.6231e-04 - val_loss: 0.0013 - val_mae: 0.0238 - val_mse: 0.0013\n",
      "Epoch 160/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 4.2365e-04 - mae: 0.0137 - mse: 4.2365e-04 - val_loss: 0.0012 - val_mae: 0.0235 - val_mse: 0.0012\n",
      "Epoch 161/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 4.7174e-04 - mae: 0.0144 - mse: 4.7174e-04 - val_loss: 0.0011 - val_mae: 0.0224 - val_mse: 0.0011\n",
      "Epoch 162/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 4.9117e-04 - mae: 0.0150 - mse: 4.9117e-04 - val_loss: 0.0011 - val_mae: 0.0224 - val_mse: 0.0011\n",
      "Epoch 163/200\n",
      "2440/2440 [==============================] - 0s 60us/step - loss: 5.3913e-04 - mae: 0.0157 - mse: 5.3913e-04 - val_loss: 9.5241e-04 - val_mae: 0.0222 - val_mse: 9.5241e-04\n",
      "Epoch 164/200\n",
      "2440/2440 [==============================] - 0s 62us/step - loss: 5.3461e-04 - mae: 0.0156 - mse: 5.3461e-04 - val_loss: 8.9685e-04 - val_mae: 0.0209 - val_mse: 8.9685e-04\n",
      "Epoch 165/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 4.7561e-04 - mae: 0.0147 - mse: 4.7561e-04 - val_loss: 9.6787e-04 - val_mae: 0.0199 - val_mse: 9.6787e-04\n",
      "Epoch 166/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 4.3481e-04 - mae: 0.0143 - mse: 4.3481e-04 - val_loss: 0.0010 - val_mae: 0.0189 - val_mse: 0.0010\n",
      "Epoch 167/200\n",
      "2440/2440 [==============================] - 0s 59us/step - loss: 4.2368e-04 - mae: 0.0142 - mse: 4.2368e-04 - val_loss: 0.0010 - val_mae: 0.0186 - val_mse: 0.0010\n",
      "Epoch 168/200\n",
      "2440/2440 [==============================] - 0s 61us/step - loss: 3.9789e-04 - mae: 0.0138 - mse: 3.9789e-04 - val_loss: 9.8909e-04 - val_mae: 0.0182 - val_mse: 9.8909e-04\n",
      "Epoch 169/200\n",
      "2440/2440 [==============================] - 0s 61us/step - loss: 3.9173e-04 - mae: 0.0136 - mse: 3.9173e-04 - val_loss: 9.2152e-04 - val_mae: 0.0175 - val_mse: 9.2152e-04\n",
      "Epoch 170/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 3.6072e-04 - mae: 0.0130 - mse: 3.6072e-04 - val_loss: 8.5602e-04 - val_mae: 0.0168 - val_mse: 8.5602e-04\n",
      "Epoch 171/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 3.3614e-04 - mae: 0.0125 - mse: 3.3614e-04 - val_loss: 8.3807e-04 - val_mae: 0.0166 - val_mse: 8.3807e-04\n",
      "Epoch 172/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 2.9581e-04 - mae: 0.0117 - mse: 2.9581e-04 - val_loss: 8.0513e-04 - val_mae: 0.0163 - val_mse: 8.0513e-04\n",
      "Epoch 173/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 2.7408e-04 - mae: 0.0113 - mse: 2.7408e-04 - val_loss: 7.8354e-04 - val_mae: 0.0163 - val_mse: 7.8354e-04\n",
      "Epoch 174/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.5227e-04 - mae: 0.0108 - mse: 2.5227e-04 - val_loss: 7.6903e-04 - val_mae: 0.0162 - val_mse: 7.6903e-04\n",
      "Epoch 175/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 2.2914e-04 - mae: 0.0103 - mse: 2.2914e-04 - val_loss: 7.5292e-04 - val_mae: 0.0162 - val_mse: 7.5292e-04\n",
      "Epoch 176/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 2.1341e-04 - mae: 0.0099 - mse: 2.1341e-04 - val_loss: 7.4790e-04 - val_mae: 0.0161 - val_mse: 7.4790e-04\n",
      "Epoch 177/200\n",
      "2440/2440 [==============================] - 0s 53us/step - loss: 1.9729e-04 - mae: 0.0095 - mse: 1.9729e-04 - val_loss: 7.3509e-04 - val_mae: 0.0161 - val_mse: 7.3509e-04\n",
      "Epoch 178/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.8408e-04 - mae: 0.0092 - mse: 1.8408e-04 - val_loss: 7.3708e-04 - val_mae: 0.0162 - val_mse: 7.3708e-04\n",
      "Epoch 179/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 1.7284e-04 - mae: 0.0089 - mse: 1.7284e-04 - val_loss: 7.3218e-04 - val_mae: 0.0163 - val_mse: 7.3218e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/200\n",
      "2440/2440 [==============================] - 0s 59us/step - loss: 1.6284e-04 - mae: 0.0087 - mse: 1.6284e-04 - val_loss: 7.2596e-04 - val_mae: 0.0163 - val_mse: 7.2596e-04\n",
      "Epoch 181/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 1.5364e-04 - mae: 0.0084 - mse: 1.5364e-04 - val_loss: 7.1443e-04 - val_mae: 0.0162 - val_mse: 7.1443e-04\n",
      "Epoch 182/200\n",
      "2440/2440 [==============================] - 0s 50us/step - loss: 1.4952e-04 - mae: 0.0083 - mse: 1.4952e-04 - val_loss: 7.1289e-04 - val_mae: 0.0163 - val_mse: 7.1289e-04\n",
      "Epoch 183/200\n",
      "2440/2440 [==============================] - 0s 52us/step - loss: 1.4290e-04 - mae: 0.0081 - mse: 1.4290e-04 - val_loss: 7.1534e-04 - val_mae: 0.0163 - val_mse: 7.1534e-04\n",
      "Epoch 184/200\n",
      "2440/2440 [==============================] - 0s 50us/step - loss: 1.3574e-04 - mae: 0.0079 - mse: 1.3574e-04 - val_loss: 7.1468e-04 - val_mae: 0.0163 - val_mse: 7.1468e-04\n",
      "Epoch 185/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.3248e-04 - mae: 0.0078 - mse: 1.3248e-04 - val_loss: 7.0923e-04 - val_mae: 0.0163 - val_mse: 7.0923e-04\n",
      "Epoch 186/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.2584e-04 - mae: 0.0075 - mse: 1.2584e-04 - val_loss: 7.1209e-04 - val_mae: 0.0164 - val_mse: 7.1209e-04\n",
      "Epoch 187/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 1.2555e-04 - mae: 0.0075 - mse: 1.2555e-04 - val_loss: 7.1400e-04 - val_mae: 0.0165 - val_mse: 7.1400e-04\n",
      "Epoch 188/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 1.2155e-04 - mae: 0.0074 - mse: 1.2155e-04 - val_loss: 7.0811e-04 - val_mae: 0.0164 - val_mse: 7.0811e-04\n",
      "Epoch 189/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 1.2418e-04 - mae: 0.0075 - mse: 1.2418e-04 - val_loss: 7.1538e-04 - val_mae: 0.0165 - val_mse: 7.1538e-04\n",
      "Epoch 190/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.2681e-04 - mae: 0.0076 - mse: 1.2681e-04 - val_loss: 7.3078e-04 - val_mae: 0.0165 - val_mse: 7.3078e-04\n",
      "Epoch 191/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.3147e-04 - mae: 0.0077 - mse: 1.3147e-04 - val_loss: 7.3704e-04 - val_mae: 0.0168 - val_mse: 7.3704e-04\n",
      "Epoch 192/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.3827e-04 - mae: 0.0079 - mse: 1.3827e-04 - val_loss: 7.4607e-04 - val_mae: 0.0169 - val_mse: 7.4607e-04\n",
      "Epoch 193/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.4623e-04 - mae: 0.0080 - mse: 1.4623e-04 - val_loss: 7.2504e-04 - val_mae: 0.0167 - val_mse: 7.2504e-04\n",
      "Epoch 194/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 1.3674e-04 - mae: 0.0078 - mse: 1.3674e-04 - val_loss: 7.1974e-04 - val_mae: 0.0167 - val_mse: 7.1974e-04\n",
      "Epoch 195/200\n",
      "2440/2440 [==============================] - 0s 58us/step - loss: 1.3485e-04 - mae: 0.0078 - mse: 1.3485e-04 - val_loss: 7.1795e-04 - val_mae: 0.0165 - val_mse: 7.1795e-04\n",
      "Epoch 196/200\n",
      "2440/2440 [==============================] - 0s 54us/step - loss: 1.3506e-04 - mae: 0.0079 - mse: 1.3506e-04 - val_loss: 7.3165e-04 - val_mae: 0.0167 - val_mse: 7.3165e-04\n",
      "Epoch 197/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.3254e-04 - mae: 0.0078 - mse: 1.3254e-04 - val_loss: 7.2423e-04 - val_mae: 0.0164 - val_mse: 7.2423e-04\n",
      "Epoch 198/200\n",
      "2440/2440 [==============================] - 0s 57us/step - loss: 1.2816e-04 - mae: 0.0076 - mse: 1.2816e-04 - val_loss: 7.1680e-04 - val_mae: 0.0163 - val_mse: 7.1680e-04\n",
      "Epoch 199/200\n",
      "2440/2440 [==============================] - 0s 55us/step - loss: 1.3320e-04 - mae: 0.0077 - mse: 1.3320e-04 - val_loss: 7.2532e-04 - val_mae: 0.0165 - val_mse: 7.2532e-04\n",
      "Epoch 200/200\n",
      "2440/2440 [==============================] - 0s 56us/step - loss: 1.2088e-04 - mae: 0.0076 - mse: 1.2088e-04 - val_loss: 7.1277e-04 - val_mae: 0.0162 - val_mse: 7.1277e-04\n"
     ]
    }
   ],
   "source": [
    "# Fit CNN\n",
    "cnnhistory = cnn.fit(X_train_CNN, y_train_CNN,\n",
    "                       epochs=EPOCHS,\n",
    "                       validation_data=(X_test_CNN, y_test_CNN),\n",
    "                       verbose=1,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph CNN Error, mae and mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN Model WITHOUT Mobility Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4222 samples, validate on 1056 samples\n",
      "Epoch 1/200\n",
      "4222/4222 [==============================] - 0s 74us/step - loss: 0.0293 - mae: 0.1033 - mse: 0.0293 - val_loss: 0.0145 - val_mae: 0.0674 - val_mse: 0.0145\n",
      "Epoch 2/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0145 - mae: 0.0632 - mse: 0.0145 - val_loss: 0.0122 - val_mae: 0.0598 - val_mse: 0.0122\n",
      "Epoch 3/200\n",
      "4222/4222 [==============================] - 0s 54us/step - loss: 0.0133 - mae: 0.0587 - mse: 0.0133 - val_loss: 0.0115 - val_mae: 0.0586 - val_mse: 0.0115\n",
      "Epoch 4/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0127 - mae: 0.0572 - mse: 0.0127 - val_loss: 0.0109 - val_mae: 0.0582 - val_mse: 0.0109\n",
      "Epoch 5/200\n",
      "4222/4222 [==============================] - 0s 51us/step - loss: 0.0122 - mae: 0.0558 - mse: 0.0122 - val_loss: 0.0106 - val_mae: 0.0599 - val_mse: 0.0106\n",
      "Epoch 6/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0118 - mae: 0.0544 - mse: 0.0118 - val_loss: 0.0102 - val_mae: 0.0579 - val_mse: 0.0102\n",
      "Epoch 7/200\n",
      "4222/4222 [==============================] - 0s 50us/step - loss: 0.0114 - mae: 0.0532 - mse: 0.0114 - val_loss: 0.0098 - val_mae: 0.0575 - val_mse: 0.0098\n",
      "Epoch 8/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0111 - mae: 0.0531 - mse: 0.0111 - val_loss: 0.0097 - val_mae: 0.0588 - val_mse: 0.0097\n",
      "Epoch 9/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0108 - mae: 0.0522 - mse: 0.0108 - val_loss: 0.0096 - val_mae: 0.0594 - val_mse: 0.0096\n",
      "Epoch 10/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0106 - mae: 0.0515 - mse: 0.0106 - val_loss: 0.0092 - val_mae: 0.0574 - val_mse: 0.0092\n",
      "Epoch 11/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0102 - mae: 0.0499 - mse: 0.0102 - val_loss: 0.0090 - val_mae: 0.0569 - val_mse: 0.0090\n",
      "Epoch 12/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0098 - mae: 0.0489 - mse: 0.0098 - val_loss: 0.0088 - val_mae: 0.0553 - val_mse: 0.0088\n",
      "Epoch 13/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0097 - mae: 0.0487 - mse: 0.0097 - val_loss: 0.0084 - val_mae: 0.0536 - val_mse: 0.0084\n",
      "Epoch 14/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0094 - mae: 0.0478 - mse: 0.0094 - val_loss: 0.0083 - val_mae: 0.0524 - val_mse: 0.0083\n",
      "Epoch 15/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0092 - mae: 0.0472 - mse: 0.0092 - val_loss: 0.0083 - val_mae: 0.0542 - val_mse: 0.0083\n",
      "Epoch 16/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0090 - mae: 0.0475 - mse: 0.0090 - val_loss: 0.0082 - val_mae: 0.0551 - val_mse: 0.0082\n",
      "Epoch 17/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0089 - mae: 0.0473 - mse: 0.0089 - val_loss: 0.0079 - val_mae: 0.0514 - val_mse: 0.0079\n",
      "Epoch 18/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0085 - mae: 0.0461 - mse: 0.0085 - val_loss: 0.0077 - val_mae: 0.0511 - val_mse: 0.0077\n",
      "Epoch 19/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0084 - mae: 0.0451 - mse: 0.0084 - val_loss: 0.0075 - val_mae: 0.0499 - val_mse: 0.0075\n",
      "Epoch 20/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0083 - mae: 0.0449 - mse: 0.0083 - val_loss: 0.0075 - val_mae: 0.0501 - val_mse: 0.0075\n",
      "Epoch 21/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0081 - mae: 0.0447 - mse: 0.0081 - val_loss: 0.0074 - val_mae: 0.0491 - val_mse: 0.0074\n",
      "Epoch 22/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0079 - mae: 0.0435 - mse: 0.0079 - val_loss: 0.0072 - val_mae: 0.0479 - val_mse: 0.0072\n",
      "Epoch 23/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0081 - mae: 0.0438 - mse: 0.0081 - val_loss: 0.0073 - val_mae: 0.0498 - val_mse: 0.0073\n",
      "Epoch 24/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0082 - mae: 0.0452 - mse: 0.0082 - val_loss: 0.0070 - val_mae: 0.0455 - val_mse: 0.0070\n",
      "Epoch 25/200\n",
      "4222/4222 [==============================] - 0s 62us/step - loss: 0.0076 - mae: 0.0422 - mse: 0.0076 - val_loss: 0.0070 - val_mae: 0.0461 - val_mse: 0.0070\n",
      "Epoch 26/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0075 - mae: 0.0418 - mse: 0.0075 - val_loss: 0.0069 - val_mae: 0.0457 - val_mse: 0.0069\n",
      "Epoch 27/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0074 - mae: 0.0415 - mse: 0.0074 - val_loss: 0.0069 - val_mae: 0.0451 - val_mse: 0.0069\n",
      "Epoch 28/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0073 - mae: 0.0413 - mse: 0.0073 - val_loss: 0.0068 - val_mae: 0.0439 - val_mse: 0.0068\n",
      "Epoch 29/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0072 - mae: 0.0408 - mse: 0.0072 - val_loss: 0.0067 - val_mae: 0.0437 - val_mse: 0.0067\n",
      "Epoch 30/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0071 - mae: 0.0404 - mse: 0.0071 - val_loss: 0.0066 - val_mae: 0.0427 - val_mse: 0.0066\n",
      "Epoch 31/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0070 - mae: 0.0396 - mse: 0.0070 - val_loss: 0.0066 - val_mae: 0.0435 - val_mse: 0.0066\n",
      "Epoch 32/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0069 - mae: 0.0393 - mse: 0.0069 - val_loss: 0.0065 - val_mae: 0.0422 - val_mse: 0.0065\n",
      "Epoch 33/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0069 - mae: 0.0393 - mse: 0.0069 - val_loss: 0.0065 - val_mae: 0.0423 - val_mse: 0.0065\n",
      "Epoch 34/200\n",
      "4222/4222 [==============================] - 0s 51us/step - loss: 0.0068 - mae: 0.0390 - mse: 0.0068 - val_loss: 0.0065 - val_mae: 0.0425 - val_mse: 0.0065\n",
      "Epoch 35/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0068 - mae: 0.0389 - mse: 0.0068 - val_loss: 0.0064 - val_mae: 0.0425 - val_mse: 0.0064\n",
      "Epoch 36/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0066 - mae: 0.0377 - mse: 0.0066 - val_loss: 0.0064 - val_mae: 0.0416 - val_mse: 0.0064\n",
      "Epoch 37/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0067 - mae: 0.0378 - mse: 0.0067 - val_loss: 0.0063 - val_mae: 0.0414 - val_mse: 0.0063\n",
      "Epoch 38/200\n",
      "4222/4222 [==============================] - 0s 60us/step - loss: 0.0066 - mae: 0.0378 - mse: 0.0066 - val_loss: 0.0063 - val_mae: 0.0412 - val_mse: 0.0063\n",
      "Epoch 39/200\n",
      "4222/4222 [==============================] - 0s 51us/step - loss: 0.0065 - mae: 0.0372 - mse: 0.0065 - val_loss: 0.0063 - val_mae: 0.0410 - val_mse: 0.0063\n",
      "Epoch 40/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0065 - mae: 0.0368 - mse: 0.0065 - val_loss: 0.0062 - val_mae: 0.0402 - val_mse: 0.0062\n",
      "Epoch 41/200\n",
      "4222/4222 [==============================] - 0s 53us/step - loss: 0.0064 - mae: 0.0369 - mse: 0.0064 - val_loss: 0.0062 - val_mae: 0.0415 - val_mse: 0.0062\n",
      "Epoch 42/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0064 - mae: 0.0369 - mse: 0.0064 - val_loss: 0.0062 - val_mae: 0.0415 - val_mse: 0.0062\n",
      "Epoch 43/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0063 - mae: 0.0362 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0400 - val_mse: 0.0061\n",
      "Epoch 44/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0063 - mae: 0.0360 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0384 - val_mse: 0.0061\n",
      "Epoch 45/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0063 - mae: 0.0363 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0400 - val_mse: 0.0061\n",
      "Epoch 46/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0062 - mae: 0.0354 - mse: 0.0062 - val_loss: 0.0061 - val_mae: 0.0397 - val_mse: 0.0061\n",
      "Epoch 47/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0062 - mae: 0.0353 - mse: 0.0062 - val_loss: 0.0060 - val_mae: 0.0382 - val_mse: 0.0060\n",
      "Epoch 48/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0063 - mae: 0.0361 - mse: 0.0063 - val_loss: 0.0060 - val_mae: 0.0395 - val_mse: 0.0060\n",
      "Epoch 49/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0062 - mae: 0.0350 - mse: 0.0062 - val_loss: 0.0059 - val_mae: 0.0380 - val_mse: 0.0059\n",
      "Epoch 50/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0061 - mae: 0.0349 - mse: 0.0061 - val_loss: 0.0059 - val_mae: 0.0379 - val_mse: 0.0059\n",
      "Epoch 51/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0061 - mae: 0.0348 - mse: 0.0061 - val_loss: 0.0059 - val_mae: 0.0374 - val_mse: 0.0059\n",
      "Epoch 52/200\n",
      "4222/4222 [==============================] - 0s 52us/step - loss: 0.0061 - mae: 0.0350 - mse: 0.0061 - val_loss: 0.0059 - val_mae: 0.0372 - val_mse: 0.0059\n",
      "Epoch 53/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0061 - mae: 0.0349 - mse: 0.0061 - val_loss: 0.0059 - val_mae: 0.0378 - val_mse: 0.0059\n",
      "Epoch 54/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0060 - mae: 0.0343 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0374 - val_mse: 0.0059\n",
      "Epoch 55/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0060 - mae: 0.0344 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0372 - val_mse: 0.0059\n",
      "Epoch 56/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0060 - mae: 0.0342 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0358 - val_mse: 0.0059\n",
      "Epoch 57/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0060 - mae: 0.0345 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0366 - val_mse: 0.0059\n",
      "Epoch 58/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0060 - mae: 0.0343 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0369 - val_mse: 0.0059\n",
      "Epoch 59/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0059 - mae: 0.0339 - mse: 0.0059 - val_loss: 0.0059 - val_mae: 0.0375 - val_mse: 0.0059\n",
      "Epoch 60/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0060 - mae: 0.0341 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0375 - val_mse: 0.0059\n",
      "Epoch 61/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0059 - mae: 0.0340 - mse: 0.0059 - val_loss: 0.0058 - val_mae: 0.0357 - val_mse: 0.0058\n",
      "Epoch 62/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0059 - mae: 0.0338 - mse: 0.0059 - val_loss: 0.0058 - val_mae: 0.0356 - val_mse: 0.0058\n",
      "Epoch 63/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0059 - mae: 0.0338 - mse: 0.0059 - val_loss: 0.0058 - val_mae: 0.0372 - val_mse: 0.0058\n",
      "Epoch 64/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0058 - mae: 0.0335 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0371 - val_mse: 0.0058\n",
      "Epoch 65/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0058 - mae: 0.0331 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0338 - val_mse: 0.0058\n",
      "Epoch 66/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0059 - mae: 0.0339 - mse: 0.0059 - val_loss: 0.0058 - val_mae: 0.0366 - val_mse: 0.0058\n",
      "Epoch 67/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0058 - mae: 0.0332 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0350 - val_mse: 0.0058\n",
      "Epoch 68/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0059 - mae: 0.0342 - mse: 0.0059 - val_loss: 0.0058 - val_mae: 0.0372 - val_mse: 0.0058\n",
      "Epoch 69/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0058 - mae: 0.0333 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0376 - val_mse: 0.0058\n",
      "Epoch 70/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0058 - mae: 0.0330 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0373 - val_mse: 0.0058\n",
      "Epoch 71/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0058 - mae: 0.0330 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0370 - val_mse: 0.0058\n",
      "Epoch 72/200\n",
      "4222/4222 [==============================] - 0s 54us/step - loss: 0.0058 - mae: 0.0332 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0369 - val_mse: 0.0058\n",
      "Epoch 73/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0058 - mae: 0.0338 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0373 - val_mse: 0.0058\n",
      "Epoch 74/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0058 - mae: 0.0331 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0370 - val_mse: 0.0058\n",
      "Epoch 75/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0057 - mae: 0.0325 - mse: 0.0057 - val_loss: 0.0058 - val_mae: 0.0361 - val_mse: 0.0058\n",
      "Epoch 76/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0057 - mae: 0.0321 - mse: 0.0057 - val_loss: 0.0058 - val_mae: 0.0372 - val_mse: 0.0058\n",
      "Epoch 77/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0057 - mae: 0.0330 - mse: 0.0057 - val_loss: 0.0058 - val_mae: 0.0365 - val_mse: 0.0058\n",
      "Epoch 78/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0057 - mae: 0.0329 - mse: 0.0057 - val_loss: 0.0058 - val_mae: 0.0360 - val_mse: 0.0058\n",
      "Epoch 79/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0057 - mae: 0.0325 - mse: 0.0057 - val_loss: 0.0058 - val_mae: 0.0370 - val_mse: 0.0058\n",
      "Epoch 80/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0056 - mae: 0.0323 - mse: 0.0056 - val_loss: 0.0058 - val_mae: 0.0361 - val_mse: 0.0058\n",
      "Epoch 81/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0056 - mae: 0.0321 - mse: 0.0056 - val_loss: 0.0058 - val_mae: 0.0362 - val_mse: 0.0058\n",
      "Epoch 82/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0057 - mae: 0.0328 - mse: 0.0057 - val_loss: 0.0058 - val_mae: 0.0381 - val_mse: 0.0058\n",
      "Epoch 83/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0056 - mae: 0.0321 - mse: 0.0056 - val_loss: 0.0058 - val_mae: 0.0375 - val_mse: 0.0058\n",
      "Epoch 84/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0056 - mae: 0.0321 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0364 - val_mse: 0.0057\n",
      "Epoch 85/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0056 - mae: 0.0323 - mse: 0.0056 - val_loss: 0.0058 - val_mae: 0.0361 - val_mse: 0.0058\n",
      "Epoch 86/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0056 - mae: 0.0322 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0360 - val_mse: 0.0057\n",
      "Epoch 87/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0056 - mae: 0.0322 - mse: 0.0056 - val_loss: 0.0058 - val_mae: 0.0366 - val_mse: 0.0058\n",
      "Epoch 88/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0056 - mae: 0.0321 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0352 - val_mse: 0.0057\n",
      "Epoch 89/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0056 - mae: 0.0319 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0363 - val_mse: 0.0057\n",
      "Epoch 90/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0056 - mae: 0.0325 - mse: 0.0056 - val_loss: 0.0058 - val_mae: 0.0368 - val_mse: 0.0058\n",
      "Epoch 91/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0056 - mae: 0.0319 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0358 - val_mse: 0.0057\n",
      "Epoch 92/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0055 - mae: 0.0315 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0359 - val_mse: 0.0057\n",
      "Epoch 93/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0056 - mae: 0.0324 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0348 - val_mse: 0.0057\n",
      "Epoch 94/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0056 - mae: 0.0318 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0353 - val_mse: 0.0057\n",
      "Epoch 95/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0056 - mae: 0.0322 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0355 - val_mse: 0.0057\n",
      "Epoch 96/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0056 - mae: 0.0322 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0340 - val_mse: 0.0057\n",
      "Epoch 97/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4222/4222 [==============================] - 0s 65us/step - loss: 0.0055 - mae: 0.0317 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0363 - val_mse: 0.0057\n",
      "Epoch 98/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0055 - mae: 0.0316 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0362 - val_mse: 0.0057\n",
      "Epoch 99/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0055 - mae: 0.0321 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0346 - val_mse: 0.0057\n",
      "Epoch 100/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0055 - mae: 0.0314 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0353 - val_mse: 0.0057\n",
      "Epoch 101/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0055 - mae: 0.0313 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0362 - val_mse: 0.0057\n",
      "Epoch 102/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0055 - mae: 0.0311 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0345 - val_mse: 0.0057\n",
      "Epoch 103/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0055 - mae: 0.0312 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0342 - val_mse: 0.0057\n",
      "Epoch 104/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0055 - mae: 0.0319 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0362 - val_mse: 0.0057\n",
      "Epoch 105/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0054 - mae: 0.0307 - mse: 0.0054 - val_loss: 0.0056 - val_mae: 0.0337 - val_mse: 0.0056\n",
      "Epoch 106/200\n",
      "4222/4222 [==============================] - 0s 55us/step - loss: 0.0054 - mae: 0.0307 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0368 - val_mse: 0.0057\n",
      "Epoch 107/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0054 - mae: 0.0306 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0351 - val_mse: 0.0057\n",
      "Epoch 108/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0054 - mae: 0.0306 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0350 - val_mse: 0.0057\n",
      "Epoch 109/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0054 - mae: 0.0307 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0341 - val_mse: 0.0057\n",
      "Epoch 110/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0055 - mae: 0.0314 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0366 - val_mse: 0.0057\n",
      "Epoch 111/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0055 - mae: 0.0314 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0354 - val_mse: 0.0057\n",
      "Epoch 112/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0054 - mae: 0.0312 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0359 - val_mse: 0.0057\n",
      "Epoch 113/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0054 - mae: 0.0307 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0353 - val_mse: 0.0057\n",
      "Epoch 114/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0055 - mae: 0.0312 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0369 - val_mse: 0.0057\n",
      "Epoch 115/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0055 - mae: 0.0317 - mse: 0.0055 - val_loss: 0.0057 - val_mae: 0.0348 - val_mse: 0.0057\n",
      "Epoch 116/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0054 - mae: 0.0312 - mse: 0.0054 - val_loss: 0.0058 - val_mae: 0.0366 - val_mse: 0.0058\n",
      "Epoch 117/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0054 - mae: 0.0310 - mse: 0.0054 - val_loss: 0.0058 - val_mae: 0.0354 - val_mse: 0.0058\n",
      "Epoch 118/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0054 - mae: 0.0309 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0341 - val_mse: 0.0057\n",
      "Epoch 119/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0054 - mae: 0.0306 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0352 - val_mse: 0.0057\n",
      "Epoch 120/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0054 - mae: 0.0307 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0348 - val_mse: 0.0057\n",
      "Epoch 121/200\n",
      "4222/4222 [==============================] - 0s 51us/step - loss: 0.0053 - mae: 0.0304 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0350 - val_mse: 0.0057\n",
      "Epoch 122/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0054 - mae: 0.0304 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0352 - val_mse: 0.0057\n",
      "Epoch 123/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0054 - mae: 0.0306 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0361 - val_mse: 0.0057\n",
      "Epoch 124/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0054 - mae: 0.0304 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0349 - val_mse: 0.0057\n",
      "Epoch 125/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0303 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0362 - val_mse: 0.0057\n",
      "Epoch 126/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0054 - mae: 0.0310 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0341 - val_mse: 0.0057\n",
      "Epoch 127/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0053 - mae: 0.0303 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0366 - val_mse: 0.0057\n",
      "Epoch 128/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0053 - mae: 0.0302 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0347 - val_mse: 0.0057\n",
      "Epoch 129/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0053 - mae: 0.0302 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0353 - val_mse: 0.0057\n",
      "Epoch 130/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0054 - mae: 0.0308 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0349 - val_mse: 0.0057\n",
      "Epoch 131/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0053 - mae: 0.0298 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0349 - val_mse: 0.0057\n",
      "Epoch 132/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0302 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0362 - val_mse: 0.0057\n",
      "Epoch 133/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0300 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0351 - val_mse: 0.0057\n",
      "Epoch 134/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0300 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0351 - val_mse: 0.0057\n",
      "Epoch 135/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0053 - mae: 0.0301 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0338 - val_mse: 0.0057\n",
      "Epoch 136/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0052 - mae: 0.0296 - mse: 0.0052 - val_loss: 0.0057 - val_mae: 0.0358 - val_mse: 0.0057\n",
      "Epoch 137/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0053 - mae: 0.0299 - mse: 0.0053 - val_loss: 0.0056 - val_mae: 0.0345 - val_mse: 0.0056\n",
      "Epoch 138/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0307 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0354 - val_mse: 0.0057\n",
      "Epoch 139/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0303 - mse: 0.0053 - val_loss: 0.0056 - val_mae: 0.0345 - val_mse: 0.0056\n",
      "Epoch 140/200\n",
      "4222/4222 [==============================] - 0s 53us/step - loss: 0.0053 - mae: 0.0298 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0355 - val_mse: 0.0057\n",
      "Epoch 141/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0054 - mae: 0.0316 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0345 - val_mse: 0.0057\n",
      "Epoch 142/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0053 - mae: 0.0309 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0341 - val_mse: 0.0057\n",
      "Epoch 143/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0052 - mae: 0.0295 - mse: 0.0052 - val_loss: 0.0056 - val_mae: 0.0351 - val_mse: 0.0056\n",
      "Epoch 144/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0298 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0344 - val_mse: 0.0057\n",
      "Epoch 145/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0303 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0341 - val_mse: 0.0057\n",
      "Epoch 146/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0053 - mae: 0.0300 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0343 - val_mse: 0.0057\n",
      "Epoch 147/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0052 - mae: 0.0298 - mse: 0.0052 - val_loss: 0.0057 - val_mae: 0.0348 - val_mse: 0.0057\n",
      "Epoch 148/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0053 - mae: 0.0302 - mse: 0.0053 - val_loss: 0.0056 - val_mae: 0.0340 - val_mse: 0.0056\n",
      "Epoch 149/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0054 - mae: 0.0315 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0351 - val_mse: 0.0057\n",
      "Epoch 150/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0053 - mae: 0.0307 - mse: 0.0053 - val_loss: 0.0056 - val_mae: 0.0360 - val_mse: 0.0056\n",
      "Epoch 151/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0053 - mae: 0.0306 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0344 - val_mse: 0.0057\n",
      "Epoch 152/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0052 - mae: 0.0298 - mse: 0.0052 - val_loss: 0.0057 - val_mae: 0.0348 - val_mse: 0.0057\n",
      "Epoch 153/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0053 - mae: 0.0310 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0357 - val_mse: 0.0057\n",
      "Epoch 154/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0051 - mae: 0.0288 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0355 - val_mse: 0.0057\n",
      "Epoch 155/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0052 - mae: 0.0291 - mse: 0.0052 - val_loss: 0.0057 - val_mae: 0.0360 - val_mse: 0.0057\n",
      "Epoch 156/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0052 - mae: 0.0297 - mse: 0.0052 - val_loss: 0.0056 - val_mae: 0.0342 - val_mse: 0.0056\n",
      "Epoch 157/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0052 - mae: 0.0299 - mse: 0.0052 - val_loss: 0.0057 - val_mae: 0.0343 - val_mse: 0.0057\n",
      "Epoch 158/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0052 - mae: 0.0294 - mse: 0.0052 - val_loss: 0.0057 - val_mae: 0.0350 - val_mse: 0.0057\n",
      "Epoch 159/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0053 - mae: 0.0309 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0343 - val_mse: 0.0057\n",
      "Epoch 160/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0052 - mae: 0.0294 - mse: 0.0052 - val_loss: 0.0056 - val_mae: 0.0340 - val_mse: 0.0056\n",
      "Epoch 161/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0052 - mae: 0.0291 - mse: 0.0052 - val_loss: 0.0056 - val_mae: 0.0348 - val_mse: 0.0056\n",
      "Epoch 162/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0053 - mae: 0.0307 - mse: 0.0053 - val_loss: 0.0057 - val_mae: 0.0335 - val_mse: 0.0057\n",
      "Epoch 163/200\n",
      "4222/4222 [==============================] - 0s 53us/step - loss: 0.0053 - mae: 0.0308 - mse: 0.0053 - val_loss: 0.0058 - val_mae: 0.0340 - val_mse: 0.0058\n",
      "Epoch 164/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0053 - mae: 0.0309 - mse: 0.0053 - val_loss: 0.0056 - val_mae: 0.0331 - val_mse: 0.0056\n",
      "Epoch 165/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0053 - mae: 0.0306 - mse: 0.0053 - val_loss: 0.0056 - val_mae: 0.0357 - val_mse: 0.0056\n",
      "Epoch 166/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0052 - mae: 0.0300 - mse: 0.0052 - val_loss: 0.0056 - val_mae: 0.0347 - val_mse: 0.0056\n",
      "Epoch 167/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0051 - mae: 0.0292 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0346 - val_mse: 0.0057\n",
      "Epoch 168/200\n",
      "4222/4222 [==============================] - 0s 52us/step - loss: 0.0051 - mae: 0.0291 - mse: 0.0051 - val_loss: 0.0056 - val_mae: 0.0343 - val_mse: 0.0056\n",
      "Epoch 169/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0051 - mae: 0.0286 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0349 - val_mse: 0.0057\n",
      "Epoch 170/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0051 - mae: 0.0286 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0357 - val_mse: 0.0057\n",
      "Epoch 171/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0051 - mae: 0.0288 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0361 - val_mse: 0.0057\n",
      "Epoch 172/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0052 - mae: 0.0296 - mse: 0.0052 - val_loss: 0.0057 - val_mae: 0.0353 - val_mse: 0.0057\n",
      "Epoch 173/200\n",
      "4222/4222 [==============================] - 0s 49us/step - loss: 0.0051 - mae: 0.0290 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0353 - val_mse: 0.0057\n",
      "Epoch 174/200\n",
      "4222/4222 [==============================] - 0s 52us/step - loss: 0.0051 - mae: 0.0292 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0358 - val_mse: 0.0057\n",
      "Epoch 175/200\n",
      "4222/4222 [==============================] - 0s 53us/step - loss: 0.0051 - mae: 0.0292 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0352 - val_mse: 0.0057\n",
      "Epoch 176/200\n",
      "4222/4222 [==============================] - 0s 52us/step - loss: 0.0051 - mae: 0.0286 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0362 - val_mse: 0.0057\n",
      "Epoch 177/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0051 - mae: 0.0291 - mse: 0.0051 - val_loss: 0.0056 - val_mae: 0.0344 - val_mse: 0.0056\n",
      "Epoch 178/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0051 - mae: 0.0290 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0348 - val_mse: 0.0057\n",
      "Epoch 179/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0051 - mae: 0.0289 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0353 - val_mse: 0.0057\n",
      "Epoch 180/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0051 - mae: 0.0288 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0344 - val_mse: 0.0057\n",
      "Epoch 181/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0051 - mae: 0.0290 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0359 - val_mse: 0.0057\n",
      "Epoch 182/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0051 - mae: 0.0294 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0375 - val_mse: 0.0057\n",
      "Epoch 183/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0051 - mae: 0.0298 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0365 - val_mse: 0.0057\n",
      "Epoch 184/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0050 - mae: 0.0290 - mse: 0.0050 - val_loss: 0.0057 - val_mae: 0.0356 - val_mse: 0.0057\n",
      "Epoch 185/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0053 - mae: 0.0323 - mse: 0.0053 - val_loss: 0.0058 - val_mae: 0.0371 - val_mse: 0.0058\n",
      "Epoch 186/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0054 - mae: 0.0322 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0359 - val_mse: 0.0057\n",
      "Epoch 187/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0051 - mae: 0.0290 - mse: 0.0051 - val_loss: 0.0056 - val_mae: 0.0336 - val_mse: 0.0056\n",
      "Epoch 188/200\n",
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0050 - mae: 0.0290 - mse: 0.0050 - val_loss: 0.0056 - val_mae: 0.0337 - val_mse: 0.0056\n",
      "Epoch 189/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0050 - mae: 0.0282 - mse: 0.0050 - val_loss: 0.0056 - val_mae: 0.0337 - val_mse: 0.0056\n",
      "Epoch 190/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0050 - mae: 0.0282 - mse: 0.0050 - val_loss: 0.0057 - val_mae: 0.0349 - val_mse: 0.0057\n",
      "Epoch 191/200\n",
      "4222/4222 [==============================] - 0s 51us/step - loss: 0.0050 - mae: 0.0283 - mse: 0.0050 - val_loss: 0.0057 - val_mae: 0.0368 - val_mse: 0.0057\n",
      "Epoch 192/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0050 - mae: 0.0286 - mse: 0.0050 - val_loss: 0.0057 - val_mae: 0.0359 - val_mse: 0.0057\n",
      "Epoch 193/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4222/4222 [==============================] - 0s 46us/step - loss: 0.0050 - mae: 0.0288 - mse: 0.0050 - val_loss: 0.0057 - val_mae: 0.0356 - val_mse: 0.0057\n",
      "Epoch 194/200\n",
      "4222/4222 [==============================] - 0s 47us/step - loss: 0.0050 - mae: 0.0288 - mse: 0.0050 - val_loss: 0.0057 - val_mae: 0.0350 - val_mse: 0.0057\n",
      "Epoch 195/200\n",
      "4222/4222 [==============================] - 0s 43us/step - loss: 0.0051 - mae: 0.0289 - mse: 0.0051 - val_loss: 0.0057 - val_mae: 0.0353 - val_mse: 0.0057\n",
      "Epoch 196/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0050 - mae: 0.0287 - mse: 0.0050 - val_loss: 0.0056 - val_mae: 0.0349 - val_mse: 0.0056\n",
      "Epoch 197/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0050 - mae: 0.0286 - mse: 0.0050 - val_loss: 0.0056 - val_mae: 0.0335 - val_mse: 0.0056\n",
      "Epoch 198/200\n",
      "4222/4222 [==============================] - 0s 48us/step - loss: 0.0050 - mae: 0.0292 - mse: 0.0050 - val_loss: 0.0056 - val_mae: 0.0350 - val_mse: 0.0056\n",
      "Epoch 199/200\n",
      "4222/4222 [==============================] - 0s 44us/step - loss: 0.0050 - mae: 0.0291 - mse: 0.0050 - val_loss: 0.0057 - val_mae: 0.0362 - val_mse: 0.0057\n",
      "Epoch 200/200\n",
      "4222/4222 [==============================] - 0s 45us/step - loss: 0.0050 - mae: 0.0286 - mse: 0.0050 - val_loss: 0.0057 - val_mae: 0.0340 - val_mse: 0.0057\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "X_CNN_nomob, y_CNN_nomob = get_CNN_states_dataset(state_cases_df, n_steps=N_STEPS)\n",
    "X_train_CNN_nomob, X_test_CNN_nomob, y_train_CNN_nomob, y_test_CNN_nomob = train_test_split(X_CNN_nomob, y_CNN_nomob, test_size=SPLIT, shuffle=False)\n",
    "\n",
    "# Set up model\n",
    "n_cnn_features = X_train_CNN_nomob.shape[2]\n",
    "cnn_nomob = get_CNN_model(N_STEPS, n_cnn_features)\n",
    "\n",
    "# Fit CNN\n",
    "cnn_nomob_history = cnn_nomob.fit(X_train_CNN_nomob, y_train_CNN_nomob,\n",
    "                                     epochs=EPOCHS,\n",
    "                                     validation_data=(X_test_CNN_nomob, y_test_CNN_nomob),\n",
    "                                     verbose=1,\n",
    "                                     shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yunzhou's Neural Net Model With Mobility Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Set up NN, Yunzhou's model\n",
    "# Model Build functions\n",
    "def build_YNN_model(n_features):\n",
    "    # 2 hidden layers, 128 units, activation functions specified as relu\n",
    "    # and 1 final layer for prediction result\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation=\"relu\", input_shape=[n_features]),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "    \n",
    "    model.compile(loss=\"mse\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mae', 'mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Compile model\n",
    "n_ynn_features = X_train_YNN.shape[1]\n",
    "ynn = build_YNN_model(n_ynn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit NN\n",
    "# early_stop=keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "ynnhistory=ynn.fit(X_train_YNN, y_train_YNN,\n",
    "                   epochs=EPOCHS,\n",
    "                   validation_data=(X_test_YNN, y_test_YNN),\n",
    "                   verbose=0)#,\n",
    "                   #callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13d91d810>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph NN Error, mae and mse\n",
    "plt.plot(ynnhistory.history['loss'])\n",
    "plt.plot(ynnhistory.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yunzhou's Neural Net Model WITHOUT Mobility Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4222 samples, validate on 1056 samples\n",
      "Epoch 1/200\n",
      "4222/4222 [==============================] - 0s 67us/sample - loss: 0.0180 - mean_absolute_error: 0.0689 - mean_squared_error: 0.0180 - val_loss: 0.0091 - val_mean_absolute_error: 0.0493 - val_mean_squared_error: 0.0091\n",
      "Epoch 2/200\n",
      "4222/4222 [==============================] - 0s 36us/sample - loss: 0.0117 - mean_absolute_error: 0.0525 - mean_squared_error: 0.0117 - val_loss: 0.0077 - val_mean_absolute_error: 0.0459 - val_mean_squared_error: 0.0077\n",
      "Epoch 3/200\n",
      "4222/4222 [==============================] - 0s 37us/sample - loss: 0.0101 - mean_absolute_error: 0.0471 - mean_squared_error: 0.0101 - val_loss: 0.0072 - val_mean_absolute_error: 0.0440 - val_mean_squared_error: 0.0072\n",
      "Epoch 4/200\n",
      "4222/4222 [==============================] - 0s 37us/sample - loss: 0.0090 - mean_absolute_error: 0.0444 - mean_squared_error: 0.0090 - val_loss: 0.0055 - val_mean_absolute_error: 0.0369 - val_mean_squared_error: 0.0055\n",
      "Epoch 5/200\n",
      "4222/4222 [==============================] - 0s 39us/sample - loss: 0.0083 - mean_absolute_error: 0.0421 - mean_squared_error: 0.0083 - val_loss: 0.0054 - val_mean_absolute_error: 0.0377 - val_mean_squared_error: 0.0054\n",
      "Epoch 6/200\n",
      "4222/4222 [==============================] - 0s 36us/sample - loss: 0.0076 - mean_absolute_error: 0.0401 - mean_squared_error: 0.0076 - val_loss: 0.0044 - val_mean_absolute_error: 0.0347 - val_mean_squared_error: 0.0044\n",
      "Epoch 7/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0070 - mean_absolute_error: 0.0384 - mean_squared_error: 0.0070 - val_loss: 0.0040 - val_mean_absolute_error: 0.0332 - val_mean_squared_error: 0.0040\n",
      "Epoch 8/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0066 - mean_absolute_error: 0.0366 - mean_squared_error: 0.0066 - val_loss: 0.0043 - val_mean_absolute_error: 0.0380 - val_mean_squared_error: 0.0043\n",
      "Epoch 9/200\n",
      "4222/4222 [==============================] - 0s 38us/sample - loss: 0.0060 - mean_absolute_error: 0.0342 - mean_squared_error: 0.0060 - val_loss: 0.0059 - val_mean_absolute_error: 0.0454 - val_mean_squared_error: 0.0059\n",
      "Epoch 10/200\n",
      "4222/4222 [==============================] - 0s 36us/sample - loss: 0.0058 - mean_absolute_error: 0.0346 - mean_squared_error: 0.0058 - val_loss: 0.0043 - val_mean_absolute_error: 0.0361 - val_mean_squared_error: 0.0043\n",
      "Epoch 11/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0053 - mean_absolute_error: 0.0330 - mean_squared_error: 0.0053 - val_loss: 0.0037 - val_mean_absolute_error: 0.0343 - val_mean_squared_error: 0.0037\n",
      "Epoch 12/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0050 - mean_absolute_error: 0.0315 - mean_squared_error: 0.0050 - val_loss: 0.0035 - val_mean_absolute_error: 0.0324 - val_mean_squared_error: 0.0035\n",
      "Epoch 13/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0047 - mean_absolute_error: 0.0303 - mean_squared_error: 0.0047 - val_loss: 0.0033 - val_mean_absolute_error: 0.0307 - val_mean_squared_error: 0.0033\n",
      "Epoch 14/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0044 - mean_absolute_error: 0.0291 - mean_squared_error: 0.0044 - val_loss: 0.0033 - val_mean_absolute_error: 0.0291 - val_mean_squared_error: 0.0033\n",
      "Epoch 15/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0041 - mean_absolute_error: 0.0279 - mean_squared_error: 0.0041 - val_loss: 0.0038 - val_mean_absolute_error: 0.0341 - val_mean_squared_error: 0.0038\n",
      "Epoch 16/200\n",
      "4222/4222 [==============================] - 0s 36us/sample - loss: 0.0041 - mean_absolute_error: 0.0283 - mean_squared_error: 0.0041 - val_loss: 0.0042 - val_mean_absolute_error: 0.0373 - val_mean_squared_error: 0.0042\n",
      "Epoch 17/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0039 - mean_absolute_error: 0.0275 - mean_squared_error: 0.0039 - val_loss: 0.0030 - val_mean_absolute_error: 0.0291 - val_mean_squared_error: 0.0030\n",
      "Epoch 18/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0037 - mean_absolute_error: 0.0262 - mean_squared_error: 0.0037 - val_loss: 0.0030 - val_mean_absolute_error: 0.0292 - val_mean_squared_error: 0.0030\n",
      "Epoch 19/200\n",
      "4222/4222 [==============================] - 0s 37us/sample - loss: 0.0036 - mean_absolute_error: 0.0259 - mean_squared_error: 0.0036 - val_loss: 0.0029 - val_mean_absolute_error: 0.0295 - val_mean_squared_error: 0.0029\n",
      "Epoch 20/200\n",
      "4222/4222 [==============================] - 0s 41us/sample - loss: 0.0035 - mean_absolute_error: 0.0255 - mean_squared_error: 0.0035 - val_loss: 0.0026 - val_mean_absolute_error: 0.0267 - val_mean_squared_error: 0.0026\n",
      "Epoch 21/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0036 - mean_absolute_error: 0.0254 - mean_squared_error: 0.0036 - val_loss: 0.0029 - val_mean_absolute_error: 0.0304 - val_mean_squared_error: 0.0029\n",
      "Epoch 22/200\n",
      "4222/4222 [==============================] - 0s 36us/sample - loss: 0.0035 - mean_absolute_error: 0.0252 - mean_squared_error: 0.0035 - val_loss: 0.0026 - val_mean_absolute_error: 0.0273 - val_mean_squared_error: 0.0026\n",
      "Epoch 23/200\n",
      "4222/4222 [==============================] - 0s 36us/sample - loss: 0.0035 - mean_absolute_error: 0.0248 - mean_squared_error: 0.0035 - val_loss: 0.0027 - val_mean_absolute_error: 0.0282 - val_mean_squared_error: 0.0027\n",
      "Epoch 24/200\n",
      "4222/4222 [==============================] - 0s 36us/sample - loss: 0.0034 - mean_absolute_error: 0.0247 - mean_squared_error: 0.0034 - val_loss: 0.0028 - val_mean_absolute_error: 0.0288 - val_mean_squared_error: 0.0028\n",
      "Epoch 25/200\n",
      "4222/4222 [==============================] - 0s 36us/sample - loss: 0.0033 - mean_absolute_error: 0.0238 - mean_squared_error: 0.0033 - val_loss: 0.0025 - val_mean_absolute_error: 0.0254 - val_mean_squared_error: 0.0025\n",
      "Epoch 26/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0034 - mean_absolute_error: 0.0240 - mean_squared_error: 0.0034 - val_loss: 0.0023 - val_mean_absolute_error: 0.0251 - val_mean_squared_error: 0.0023\n",
      "Epoch 27/200\n",
      "4222/4222 [==============================] - 0s 36us/sample - loss: 0.0031 - mean_absolute_error: 0.0233 - mean_squared_error: 0.0031 - val_loss: 0.0032 - val_mean_absolute_error: 0.0302 - val_mean_squared_error: 0.0032\n",
      "Epoch 28/200\n",
      "4222/4222 [==============================] - 0s 36us/sample - loss: 0.0033 - mean_absolute_error: 0.0235 - mean_squared_error: 0.0033 - val_loss: 0.0029 - val_mean_absolute_error: 0.0261 - val_mean_squared_error: 0.0029\n",
      "Epoch 29/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0031 - mean_absolute_error: 0.0229 - mean_squared_error: 0.0031 - val_loss: 0.0022 - val_mean_absolute_error: 0.0236 - val_mean_squared_error: 0.0022\n",
      "Epoch 30/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0036 - mean_absolute_error: 0.0240 - mean_squared_error: 0.0036 - val_loss: 0.0068 - val_mean_absolute_error: 0.0446 - val_mean_squared_error: 0.0068\n",
      "Epoch 31/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0030 - mean_absolute_error: 0.0237 - mean_squared_error: 0.0030 - val_loss: 0.0025 - val_mean_absolute_error: 0.0276 - val_mean_squared_error: 0.0025\n",
      "Epoch 32/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0030 - mean_absolute_error: 0.0232 - mean_squared_error: 0.0030 - val_loss: 0.0047 - val_mean_absolute_error: 0.0363 - val_mean_squared_error: 0.0047\n",
      "Epoch 33/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0028 - mean_absolute_error: 0.0219 - mean_squared_error: 0.0028 - val_loss: 0.0025 - val_mean_absolute_error: 0.0245 - val_mean_squared_error: 0.0025\n",
      "Epoch 34/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0033 - mean_absolute_error: 0.0233 - mean_squared_error: 0.0033 - val_loss: 0.0041 - val_mean_absolute_error: 0.0338 - val_mean_squared_error: 0.0041\n",
      "Epoch 35/200\n",
      "4222/4222 [==============================] - 0s 37us/sample - loss: 0.0029 - mean_absolute_error: 0.0224 - mean_squared_error: 0.0029 - val_loss: 0.0023 - val_mean_absolute_error: 0.0252 - val_mean_squared_error: 0.0023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0028 - mean_absolute_error: 0.0222 - mean_squared_error: 0.0028 - val_loss: 0.0065 - val_mean_absolute_error: 0.0428 - val_mean_squared_error: 0.0065\n",
      "Epoch 37/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0028 - mean_absolute_error: 0.0224 - mean_squared_error: 0.0028 - val_loss: 0.0060 - val_mean_absolute_error: 0.0370 - val_mean_squared_error: 0.0060\n",
      "Epoch 38/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0032 - mean_absolute_error: 0.0229 - mean_squared_error: 0.0032 - val_loss: 0.0023 - val_mean_absolute_error: 0.0270 - val_mean_squared_error: 0.0023\n",
      "Epoch 39/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0030 - mean_absolute_error: 0.0229 - mean_squared_error: 0.0030 - val_loss: 0.0070 - val_mean_absolute_error: 0.0440 - val_mean_squared_error: 0.0070\n",
      "Epoch 40/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0026 - mean_absolute_error: 0.0215 - mean_squared_error: 0.0026 - val_loss: 0.0027 - val_mean_absolute_error: 0.0288 - val_mean_squared_error: 0.0027\n",
      "Epoch 41/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0028 - mean_absolute_error: 0.0217 - mean_squared_error: 0.0028 - val_loss: 0.0027 - val_mean_absolute_error: 0.0256 - val_mean_squared_error: 0.0027\n",
      "Epoch 42/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0030 - mean_absolute_error: 0.0218 - mean_squared_error: 0.0030 - val_loss: 0.0024 - val_mean_absolute_error: 0.0280 - val_mean_squared_error: 0.0024\n",
      "Epoch 43/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0028 - mean_absolute_error: 0.0218 - mean_squared_error: 0.0028 - val_loss: 0.0026 - val_mean_absolute_error: 0.0268 - val_mean_squared_error: 0.0026\n",
      "Epoch 44/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0028 - mean_absolute_error: 0.0214 - mean_squared_error: 0.0028 - val_loss: 0.0019 - val_mean_absolute_error: 0.0235 - val_mean_squared_error: 0.0019\n",
      "Epoch 45/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0026 - mean_absolute_error: 0.0208 - mean_squared_error: 0.0026 - val_loss: 0.0028 - val_mean_absolute_error: 0.0269 - val_mean_squared_error: 0.0028\n",
      "Epoch 46/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0027 - mean_absolute_error: 0.0209 - mean_squared_error: 0.0027 - val_loss: 0.0025 - val_mean_absolute_error: 0.0266 - val_mean_squared_error: 0.0025\n",
      "Epoch 47/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0026 - mean_absolute_error: 0.0212 - mean_squared_error: 0.0026 - val_loss: 0.0023 - val_mean_absolute_error: 0.0249 - val_mean_squared_error: 0.0023\n",
      "Epoch 48/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0026 - mean_absolute_error: 0.0210 - mean_squared_error: 0.0026 - val_loss: 0.0025 - val_mean_absolute_error: 0.0256 - val_mean_squared_error: 0.0025\n",
      "Epoch 49/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0027 - mean_absolute_error: 0.0212 - mean_squared_error: 0.0027 - val_loss: 0.0024 - val_mean_absolute_error: 0.0249 - val_mean_squared_error: 0.0024\n",
      "Epoch 50/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0027 - mean_absolute_error: 0.0207 - mean_squared_error: 0.0027 - val_loss: 0.0035 - val_mean_absolute_error: 0.0276 - val_mean_squared_error: 0.0035\n",
      "Epoch 51/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0024 - mean_absolute_error: 0.0200 - mean_squared_error: 0.0024 - val_loss: 0.0029 - val_mean_absolute_error: 0.0262 - val_mean_squared_error: 0.0029\n",
      "Epoch 52/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0026 - mean_absolute_error: 0.0204 - mean_squared_error: 0.0026 - val_loss: 0.0025 - val_mean_absolute_error: 0.0252 - val_mean_squared_error: 0.0025\n",
      "Epoch 53/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0024 - mean_absolute_error: 0.0198 - mean_squared_error: 0.0024 - val_loss: 0.0058 - val_mean_absolute_error: 0.0401 - val_mean_squared_error: 0.0058\n",
      "Epoch 54/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0024 - mean_absolute_error: 0.0202 - mean_squared_error: 0.0024 - val_loss: 0.0027 - val_mean_absolute_error: 0.0262 - val_mean_squared_error: 0.0027\n",
      "Epoch 55/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0026 - mean_absolute_error: 0.0202 - mean_squared_error: 0.0026 - val_loss: 0.0057 - val_mean_absolute_error: 0.0400 - val_mean_squared_error: 0.0057\n",
      "Epoch 56/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0025 - mean_absolute_error: 0.0200 - mean_squared_error: 0.0025 - val_loss: 0.0036 - val_mean_absolute_error: 0.0297 - val_mean_squared_error: 0.0036\n",
      "Epoch 57/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0025 - mean_absolute_error: 0.0202 - mean_squared_error: 0.0025 - val_loss: 0.0031 - val_mean_absolute_error: 0.0260 - val_mean_squared_error: 0.0031\n",
      "Epoch 58/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0027 - mean_absolute_error: 0.0202 - mean_squared_error: 0.0027 - val_loss: 0.0038 - val_mean_absolute_error: 0.0320 - val_mean_squared_error: 0.0038\n",
      "Epoch 59/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0024 - mean_absolute_error: 0.0197 - mean_squared_error: 0.0024 - val_loss: 0.0034 - val_mean_absolute_error: 0.0280 - val_mean_squared_error: 0.0034\n",
      "Epoch 60/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0024 - mean_absolute_error: 0.0198 - mean_squared_error: 0.0024 - val_loss: 0.0045 - val_mean_absolute_error: 0.0353 - val_mean_squared_error: 0.0045\n",
      "Epoch 61/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0022 - mean_absolute_error: 0.0191 - mean_squared_error: 0.0022 - val_loss: 0.0029 - val_mean_absolute_error: 0.0261 - val_mean_squared_error: 0.0029\n",
      "Epoch 62/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0024 - mean_absolute_error: 0.0198 - mean_squared_error: 0.0024 - val_loss: 0.0052 - val_mean_absolute_error: 0.0394 - val_mean_squared_error: 0.0052\n",
      "Epoch 63/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0024 - mean_absolute_error: 0.0197 - mean_squared_error: 0.0024 - val_loss: 0.0026 - val_mean_absolute_error: 0.0256 - val_mean_squared_error: 0.0026\n",
      "Epoch 64/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0022 - mean_absolute_error: 0.0195 - mean_squared_error: 0.0022 - val_loss: 0.0026 - val_mean_absolute_error: 0.0253 - val_mean_squared_error: 0.0026\n",
      "Epoch 65/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0023 - mean_absolute_error: 0.0193 - mean_squared_error: 0.0023 - val_loss: 0.0047 - val_mean_absolute_error: 0.0360 - val_mean_squared_error: 0.0047\n",
      "Epoch 66/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0022 - mean_absolute_error: 0.0189 - mean_squared_error: 0.0022 - val_loss: 0.0026 - val_mean_absolute_error: 0.0252 - val_mean_squared_error: 0.0026\n",
      "Epoch 67/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0023 - mean_absolute_error: 0.0197 - mean_squared_error: 0.0023 - val_loss: 0.0025 - val_mean_absolute_error: 0.0271 - val_mean_squared_error: 0.0025\n",
      "Epoch 68/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0022 - mean_absolute_error: 0.0196 - mean_squared_error: 0.0022 - val_loss: 0.0027 - val_mean_absolute_error: 0.0259 - val_mean_squared_error: 0.0027\n",
      "Epoch 69/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0021 - mean_absolute_error: 0.0189 - mean_squared_error: 0.0021 - val_loss: 0.0021 - val_mean_absolute_error: 0.0236 - val_mean_squared_error: 0.0021\n",
      "Epoch 70/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0023 - mean_absolute_error: 0.0194 - mean_squared_error: 0.0023 - val_loss: 0.0029 - val_mean_absolute_error: 0.0270 - val_mean_squared_error: 0.0029\n",
      "Epoch 71/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0025 - mean_absolute_error: 0.0203 - mean_squared_error: 0.0025 - val_loss: 0.0030 - val_mean_absolute_error: 0.0282 - val_mean_squared_error: 0.0030\n",
      "Epoch 72/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0022 - mean_absolute_error: 0.0195 - mean_squared_error: 0.0022 - val_loss: 0.0031 - val_mean_absolute_error: 0.0277 - val_mean_squared_error: 0.0031\n",
      "Epoch 73/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0024 - mean_absolute_error: 0.0199 - mean_squared_error: 0.0024 - val_loss: 0.0031 - val_mean_absolute_error: 0.0272 - val_mean_squared_error: 0.0031\n",
      "Epoch 74/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0022 - mean_absolute_error: 0.0193 - mean_squared_error: 0.0022 - val_loss: 0.0029 - val_mean_absolute_error: 0.0251 - val_mean_squared_error: 0.0029\n",
      "Epoch 75/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0022 - mean_absolute_error: 0.0194 - mean_squared_error: 0.0022 - val_loss: 0.0043 - val_mean_absolute_error: 0.0337 - val_mean_squared_error: 0.0043\n",
      "Epoch 76/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0022 - mean_absolute_error: 0.0192 - mean_squared_error: 0.0022 - val_loss: 0.0046 - val_mean_absolute_error: 0.0368 - val_mean_squared_error: 0.0046\n",
      "Epoch 77/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0020 - mean_absolute_error: 0.0187 - mean_squared_error: 0.0020 - val_loss: 0.0026 - val_mean_absolute_error: 0.0264 - val_mean_squared_error: 0.0026\n",
      "Epoch 78/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0023 - mean_absolute_error: 0.0196 - mean_squared_error: 0.0023 - val_loss: 0.0028 - val_mean_absolute_error: 0.0254 - val_mean_squared_error: 0.0028\n",
      "Epoch 79/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0022 - mean_absolute_error: 0.0192 - mean_squared_error: 0.0022 - val_loss: 0.0033 - val_mean_absolute_error: 0.0261 - val_mean_squared_error: 0.0033\n",
      "Epoch 80/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0020 - mean_absolute_error: 0.0188 - mean_squared_error: 0.0020 - val_loss: 0.0026 - val_mean_absolute_error: 0.0258 - val_mean_squared_error: 0.0026\n",
      "Epoch 81/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0021 - mean_absolute_error: 0.0193 - mean_squared_error: 0.0021 - val_loss: 0.0041 - val_mean_absolute_error: 0.0335 - val_mean_squared_error: 0.0041\n",
      "Epoch 82/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0022 - mean_absolute_error: 0.0195 - mean_squared_error: 0.0022 - val_loss: 0.0027 - val_mean_absolute_error: 0.0274 - val_mean_squared_error: 0.0027\n",
      "Epoch 83/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0021 - mean_absolute_error: 0.0193 - mean_squared_error: 0.0021 - val_loss: 0.0026 - val_mean_absolute_error: 0.0271 - val_mean_squared_error: 0.0026\n",
      "Epoch 84/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0021 - mean_absolute_error: 0.0188 - mean_squared_error: 0.0021 - val_loss: 0.0028 - val_mean_absolute_error: 0.0251 - val_mean_squared_error: 0.0028\n",
      "Epoch 85/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0021 - mean_absolute_error: 0.0183 - mean_squared_error: 0.0021 - val_loss: 0.0040 - val_mean_absolute_error: 0.0288 - val_mean_squared_error: 0.0040\n",
      "Epoch 86/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0021 - mean_absolute_error: 0.0191 - mean_squared_error: 0.0021 - val_loss: 0.0026 - val_mean_absolute_error: 0.0254 - val_mean_squared_error: 0.0026\n",
      "Epoch 87/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0022 - mean_absolute_error: 0.0197 - mean_squared_error: 0.0022 - val_loss: 0.0023 - val_mean_absolute_error: 0.0245 - val_mean_squared_error: 0.0023\n",
      "Epoch 88/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0022 - mean_absolute_error: 0.0193 - mean_squared_error: 0.0022 - val_loss: 0.0032 - val_mean_absolute_error: 0.0272 - val_mean_squared_error: 0.0032\n",
      "Epoch 89/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0022 - mean_absolute_error: 0.0189 - mean_squared_error: 0.0022 - val_loss: 0.0030 - val_mean_absolute_error: 0.0270 - val_mean_squared_error: 0.0030\n",
      "Epoch 90/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0021 - mean_absolute_error: 0.0188 - mean_squared_error: 0.0021 - val_loss: 0.0029 - val_mean_absolute_error: 0.0257 - val_mean_squared_error: 0.0029\n",
      "Epoch 91/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0022 - mean_absolute_error: 0.0187 - mean_squared_error: 0.0022 - val_loss: 0.0032 - val_mean_absolute_error: 0.0271 - val_mean_squared_error: 0.0032\n",
      "Epoch 92/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0022 - mean_absolute_error: 0.0192 - mean_squared_error: 0.0022 - val_loss: 0.0025 - val_mean_absolute_error: 0.0246 - val_mean_squared_error: 0.0025\n",
      "Epoch 93/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0020 - mean_absolute_error: 0.0178 - mean_squared_error: 0.0020 - val_loss: 0.0034 - val_mean_absolute_error: 0.0274 - val_mean_squared_error: 0.0034\n",
      "Epoch 94/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0022 - mean_absolute_error: 0.0194 - mean_squared_error: 0.0022 - val_loss: 0.0025 - val_mean_absolute_error: 0.0248 - val_mean_squared_error: 0.0025\n",
      "Epoch 95/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0020 - mean_absolute_error: 0.0184 - mean_squared_error: 0.0020 - val_loss: 0.0048 - val_mean_absolute_error: 0.0374 - val_mean_squared_error: 0.0048\n",
      "Epoch 96/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0021 - mean_absolute_error: 0.0190 - mean_squared_error: 0.0021 - val_loss: 0.0024 - val_mean_absolute_error: 0.0250 - val_mean_squared_error: 0.0024\n",
      "Epoch 97/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0021 - mean_absolute_error: 0.0186 - mean_squared_error: 0.0021 - val_loss: 0.0039 - val_mean_absolute_error: 0.0314 - val_mean_squared_error: 0.0039\n",
      "Epoch 98/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0020 - mean_absolute_error: 0.0183 - mean_squared_error: 0.0020 - val_loss: 0.0022 - val_mean_absolute_error: 0.0244 - val_mean_squared_error: 0.0022\n",
      "Epoch 99/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0020 - mean_absolute_error: 0.0184 - mean_squared_error: 0.0020 - val_loss: 0.0027 - val_mean_absolute_error: 0.0262 - val_mean_squared_error: 0.0027\n",
      "Epoch 100/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0020 - mean_absolute_error: 0.0185 - mean_squared_error: 0.0020 - val_loss: 0.0052 - val_mean_absolute_error: 0.0424 - val_mean_squared_error: 0.0052\n",
      "Epoch 101/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0018 - mean_absolute_error: 0.0182 - mean_squared_error: 0.0018 - val_loss: 0.0025 - val_mean_absolute_error: 0.0271 - val_mean_squared_error: 0.0025\n",
      "Epoch 102/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0020 - mean_absolute_error: 0.0184 - mean_squared_error: 0.0020 - val_loss: 0.0028 - val_mean_absolute_error: 0.0261 - val_mean_squared_error: 0.0028\n",
      "Epoch 103/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0019 - mean_absolute_error: 0.0182 - mean_squared_error: 0.0019 - val_loss: 0.0023 - val_mean_absolute_error: 0.0244 - val_mean_squared_error: 0.0023\n",
      "Epoch 104/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0020 - mean_absolute_error: 0.0180 - mean_squared_error: 0.0020 - val_loss: 0.0024 - val_mean_absolute_error: 0.0263 - val_mean_squared_error: 0.0024\n",
      "Epoch 105/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0020 - mean_absolute_error: 0.0178 - mean_squared_error: 0.0020 - val_loss: 0.0024 - val_mean_absolute_error: 0.0247 - val_mean_squared_error: 0.0024\n",
      "Epoch 106/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0018 - mean_absolute_error: 0.0171 - mean_squared_error: 0.0018 - val_loss: 0.0025 - val_mean_absolute_error: 0.0258 - val_mean_squared_error: 0.0025\n",
      "Epoch 107/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0021 - mean_absolute_error: 0.0193 - mean_squared_error: 0.0021 - val_loss: 0.0022 - val_mean_absolute_error: 0.0241 - val_mean_squared_error: 0.0022\n",
      "Epoch 108/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0020 - mean_absolute_error: 0.0179 - mean_squared_error: 0.0020 - val_loss: 0.0033 - val_mean_absolute_error: 0.0275 - val_mean_squared_error: 0.0033\n",
      "Epoch 109/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0020 - mean_absolute_error: 0.0187 - mean_squared_error: 0.0020 - val_loss: 0.0026 - val_mean_absolute_error: 0.0242 - val_mean_squared_error: 0.0026\n",
      "Epoch 110/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0020 - mean_absolute_error: 0.0182 - mean_squared_error: 0.0020 - val_loss: 0.0030 - val_mean_absolute_error: 0.0287 - val_mean_squared_error: 0.0030\n",
      "Epoch 111/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0019 - mean_absolute_error: 0.0185 - mean_squared_error: 0.0019 - val_loss: 0.0022 - val_mean_absolute_error: 0.0270 - val_mean_squared_error: 0.0022\n",
      "Epoch 112/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0020 - mean_absolute_error: 0.0184 - mean_squared_error: 0.0020 - val_loss: 0.0020 - val_mean_absolute_error: 0.0243 - val_mean_squared_error: 0.0020\n",
      "Epoch 113/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0020 - mean_absolute_error: 0.0183 - mean_squared_error: 0.0020 - val_loss: 0.0026 - val_mean_absolute_error: 0.0270 - val_mean_squared_error: 0.0026\n",
      "Epoch 114/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0020 - mean_absolute_error: 0.0184 - mean_squared_error: 0.0020 - val_loss: 0.0026 - val_mean_absolute_error: 0.0271 - val_mean_squared_error: 0.0026\n",
      "Epoch 115/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0020 - mean_absolute_error: 0.0187 - mean_squared_error: 0.0020 - val_loss: 0.0021 - val_mean_absolute_error: 0.0238 - val_mean_squared_error: 0.0021\n",
      "Epoch 116/200\n",
      "4222/4222 [==============================] - 0s 36us/sample - loss: 0.0021 - mean_absolute_error: 0.0191 - mean_squared_error: 0.0021 - val_loss: 0.0023 - val_mean_absolute_error: 0.0266 - val_mean_squared_error: 0.0023\n",
      "Epoch 117/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0020 - mean_absolute_error: 0.0189 - mean_squared_error: 0.0020 - val_loss: 0.0026 - val_mean_absolute_error: 0.0278 - val_mean_squared_error: 0.0026\n",
      "Epoch 118/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0020 - mean_absolute_error: 0.0180 - mean_squared_error: 0.0020 - val_loss: 0.0033 - val_mean_absolute_error: 0.0273 - val_mean_squared_error: 0.0033\n",
      "Epoch 119/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0019 - mean_absolute_error: 0.0181 - mean_squared_error: 0.0019 - val_loss: 0.0033 - val_mean_absolute_error: 0.0255 - val_mean_squared_error: 0.0033\n",
      "Epoch 120/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0018 - mean_absolute_error: 0.0177 - mean_squared_error: 0.0018 - val_loss: 0.0048 - val_mean_absolute_error: 0.0386 - val_mean_squared_error: 0.0048\n",
      "Epoch 121/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0019 - mean_absolute_error: 0.0183 - mean_squared_error: 0.0019 - val_loss: 0.0022 - val_mean_absolute_error: 0.0251 - val_mean_squared_error: 0.0022\n",
      "Epoch 122/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0020 - mean_absolute_error: 0.0180 - mean_squared_error: 0.0020 - val_loss: 0.0029 - val_mean_absolute_error: 0.0255 - val_mean_squared_error: 0.0029\n",
      "Epoch 123/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0019 - mean_absolute_error: 0.0178 - mean_squared_error: 0.0019 - val_loss: 0.0019 - val_mean_absolute_error: 0.0246 - val_mean_squared_error: 0.0019\n",
      "Epoch 124/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0021 - mean_absolute_error: 0.0189 - mean_squared_error: 0.0021 - val_loss: 0.0030 - val_mean_absolute_error: 0.0259 - val_mean_squared_error: 0.0030\n",
      "Epoch 125/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0019 - mean_absolute_error: 0.0189 - mean_squared_error: 0.0019 - val_loss: 0.0021 - val_mean_absolute_error: 0.0255 - val_mean_squared_error: 0.0021\n",
      "Epoch 126/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0019 - mean_absolute_error: 0.0185 - mean_squared_error: 0.0019 - val_loss: 0.0023 - val_mean_absolute_error: 0.0244 - val_mean_squared_error: 0.0023\n",
      "Epoch 127/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0019 - mean_absolute_error: 0.0182 - mean_squared_error: 0.0019 - val_loss: 0.0020 - val_mean_absolute_error: 0.0262 - val_mean_squared_error: 0.0020\n",
      "Epoch 128/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0020 - mean_absolute_error: 0.0188 - mean_squared_error: 0.0020 - val_loss: 0.0020 - val_mean_absolute_error: 0.0261 - val_mean_squared_error: 0.0020\n",
      "Epoch 129/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0019 - mean_absolute_error: 0.0183 - mean_squared_error: 0.0019 - val_loss: 0.0025 - val_mean_absolute_error: 0.0232 - val_mean_squared_error: 0.0025\n",
      "Epoch 130/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0019 - mean_absolute_error: 0.0189 - mean_squared_error: 0.0019 - val_loss: 0.0022 - val_mean_absolute_error: 0.0285 - val_mean_squared_error: 0.0022\n",
      "Epoch 131/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0019 - mean_absolute_error: 0.0184 - mean_squared_error: 0.0019 - val_loss: 0.0022 - val_mean_absolute_error: 0.0241 - val_mean_squared_error: 0.0022\n",
      "Epoch 132/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0019 - mean_absolute_error: 0.0181 - mean_squared_error: 0.0019 - val_loss: 0.0036 - val_mean_absolute_error: 0.0330 - val_mean_squared_error: 0.0036\n",
      "Epoch 133/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0018 - mean_absolute_error: 0.0184 - mean_squared_error: 0.0018 - val_loss: 0.0020 - val_mean_absolute_error: 0.0268 - val_mean_squared_error: 0.0020\n",
      "Epoch 134/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0021 - mean_absolute_error: 0.0192 - mean_squared_error: 0.0021 - val_loss: 0.0018 - val_mean_absolute_error: 0.0240 - val_mean_squared_error: 0.0018\n",
      "Epoch 135/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0019 - mean_absolute_error: 0.0191 - mean_squared_error: 0.0019 - val_loss: 0.0020 - val_mean_absolute_error: 0.0234 - val_mean_squared_error: 0.0020\n",
      "Epoch 136/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0018 - mean_absolute_error: 0.0180 - mean_squared_error: 0.0018 - val_loss: 0.0018 - val_mean_absolute_error: 0.0220 - val_mean_squared_error: 0.0018\n",
      "Epoch 137/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0019 - mean_absolute_error: 0.0182 - mean_squared_error: 0.0019 - val_loss: 0.0026 - val_mean_absolute_error: 0.0251 - val_mean_squared_error: 0.0026\n",
      "Epoch 138/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0018 - mean_absolute_error: 0.0182 - mean_squared_error: 0.0018 - val_loss: 0.0023 - val_mean_absolute_error: 0.0278 - val_mean_squared_error: 0.0023\n",
      "Epoch 139/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0018 - mean_absolute_error: 0.0181 - mean_squared_error: 0.0018 - val_loss: 0.0023 - val_mean_absolute_error: 0.0212 - val_mean_squared_error: 0.0023\n",
      "Epoch 140/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0018 - mean_absolute_error: 0.0184 - mean_squared_error: 0.0018 - val_loss: 0.0035 - val_mean_absolute_error: 0.0295 - val_mean_squared_error: 0.0035\n",
      "Epoch 141/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0017 - mean_absolute_error: 0.0178 - mean_squared_error: 0.0017 - val_loss: 0.0019 - val_mean_absolute_error: 0.0231 - val_mean_squared_error: 0.0019\n",
      "Epoch 142/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0019 - mean_absolute_error: 0.0186 - mean_squared_error: 0.0019 - val_loss: 0.0018 - val_mean_absolute_error: 0.0236 - val_mean_squared_error: 0.0018\n",
      "Epoch 143/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0017 - mean_absolute_error: 0.0175 - mean_squared_error: 0.0017 - val_loss: 0.0021 - val_mean_absolute_error: 0.0225 - val_mean_squared_error: 0.0021\n",
      "Epoch 144/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0017 - mean_absolute_error: 0.0186 - mean_squared_error: 0.0017 - val_loss: 0.0023 - val_mean_absolute_error: 0.0257 - val_mean_squared_error: 0.0023\n",
      "Epoch 145/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0016 - mean_absolute_error: 0.0177 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_absolute_error: 0.0244 - val_mean_squared_error: 0.0019\n",
      "Epoch 146/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0019 - mean_absolute_error: 0.0182 - mean_squared_error: 0.0019 - val_loss: 0.0027 - val_mean_absolute_error: 0.0267 - val_mean_squared_error: 0.0027\n",
      "Epoch 147/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0018 - mean_absolute_error: 0.0182 - mean_squared_error: 0.0018 - val_loss: 0.0019 - val_mean_absolute_error: 0.0244 - val_mean_squared_error: 0.0019\n",
      "Epoch 148/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0017 - mean_absolute_error: 0.0177 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_absolute_error: 0.0244 - val_mean_squared_error: 0.0020\n",
      "Epoch 149/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0017 - mean_absolute_error: 0.0178 - mean_squared_error: 0.0017 - val_loss: 0.0038 - val_mean_absolute_error: 0.0337 - val_mean_squared_error: 0.0038\n",
      "Epoch 150/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0016 - mean_absolute_error: 0.0173 - mean_squared_error: 0.0016 - val_loss: 0.0021 - val_mean_absolute_error: 0.0252 - val_mean_squared_error: 0.0021\n",
      "Epoch 151/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0019 - mean_absolute_error: 0.0183 - mean_squared_error: 0.0019 - val_loss: 0.0030 - val_mean_absolute_error: 0.0288 - val_mean_squared_error: 0.0030\n",
      "Epoch 152/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0018 - mean_absolute_error: 0.0181 - mean_squared_error: 0.0018 - val_loss: 0.0019 - val_mean_absolute_error: 0.0232 - val_mean_squared_error: 0.0019\n",
      "Epoch 153/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0018 - mean_absolute_error: 0.0180 - mean_squared_error: 0.0018 - val_loss: 0.0037 - val_mean_absolute_error: 0.0326 - val_mean_squared_error: 0.0037\n",
      "Epoch 154/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0018 - mean_absolute_error: 0.0185 - mean_squared_error: 0.0018 - val_loss: 0.0023 - val_mean_absolute_error: 0.0279 - val_mean_squared_error: 0.0023\n",
      "Epoch 155/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0018 - mean_absolute_error: 0.0182 - mean_squared_error: 0.0018 - val_loss: 0.0019 - val_mean_absolute_error: 0.0241 - val_mean_squared_error: 0.0019\n",
      "Epoch 156/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0018 - mean_absolute_error: 0.0178 - mean_squared_error: 0.0018 - val_loss: 0.0022 - val_mean_absolute_error: 0.0238 - val_mean_squared_error: 0.0022\n",
      "Epoch 157/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0017 - mean_absolute_error: 0.0179 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_absolute_error: 0.0225 - val_mean_squared_error: 0.0020\n",
      "Epoch 158/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0018 - mean_absolute_error: 0.0174 - mean_squared_error: 0.0018 - val_loss: 0.0022 - val_mean_absolute_error: 0.0230 - val_mean_squared_error: 0.0022\n",
      "Epoch 159/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0017 - mean_absolute_error: 0.0175 - mean_squared_error: 0.0017 - val_loss: 0.0017 - val_mean_absolute_error: 0.0234 - val_mean_squared_error: 0.0017\n",
      "Epoch 160/200\n",
      "4222/4222 [==============================] - 0s 37us/sample - loss: 0.0018 - mean_absolute_error: 0.0176 - mean_squared_error: 0.0018 - val_loss: 0.0017 - val_mean_absolute_error: 0.0249 - val_mean_squared_error: 0.0017\n",
      "Epoch 161/200\n",
      "4222/4222 [==============================] - 0s 39us/sample - loss: 0.0017 - mean_absolute_error: 0.0173 - mean_squared_error: 0.0017 - val_loss: 0.0019 - val_mean_absolute_error: 0.0219 - val_mean_squared_error: 0.0019\n",
      "Epoch 162/200\n",
      "4222/4222 [==============================] - 0s 36us/sample - loss: 0.0017 - mean_absolute_error: 0.0173 - mean_squared_error: 0.0017 - val_loss: 0.0021 - val_mean_absolute_error: 0.0272 - val_mean_squared_error: 0.0021\n",
      "Epoch 163/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0017 - mean_absolute_error: 0.0177 - mean_squared_error: 0.0017 - val_loss: 0.0031 - val_mean_absolute_error: 0.0328 - val_mean_squared_error: 0.0031\n",
      "Epoch 164/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0018 - mean_absolute_error: 0.0185 - mean_squared_error: 0.0018 - val_loss: 0.0020 - val_mean_absolute_error: 0.0246 - val_mean_squared_error: 0.0020\n",
      "Epoch 165/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0018 - mean_absolute_error: 0.0175 - mean_squared_error: 0.0018 - val_loss: 0.0020 - val_mean_absolute_error: 0.0215 - val_mean_squared_error: 0.0020\n",
      "Epoch 166/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0015 - mean_absolute_error: 0.0173 - mean_squared_error: 0.0015 - val_loss: 0.0023 - val_mean_absolute_error: 0.0259 - val_mean_squared_error: 0.0023\n",
      "Epoch 167/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0016 - mean_absolute_error: 0.0175 - mean_squared_error: 0.0016 - val_loss: 0.0023 - val_mean_absolute_error: 0.0250 - val_mean_squared_error: 0.0023\n",
      "Epoch 168/200\n",
      "4222/4222 [==============================] - 0s 37us/sample - loss: 0.0018 - mean_absolute_error: 0.0186 - mean_squared_error: 0.0018 - val_loss: 0.0024 - val_mean_absolute_error: 0.0270 - val_mean_squared_error: 0.0024\n",
      "Epoch 169/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0017 - mean_absolute_error: 0.0174 - mean_squared_error: 0.0017 - val_loss: 0.0023 - val_mean_absolute_error: 0.0253 - val_mean_squared_error: 0.0023\n",
      "Epoch 170/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0021 - mean_absolute_error: 0.0183 - mean_squared_error: 0.0021 - val_loss: 0.0022 - val_mean_absolute_error: 0.0238 - val_mean_squared_error: 0.0022\n",
      "Epoch 171/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0017 - mean_absolute_error: 0.0181 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_absolute_error: 0.0231 - val_mean_squared_error: 0.0020\n",
      "Epoch 172/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0017 - mean_absolute_error: 0.0181 - mean_squared_error: 0.0017 - val_loss: 0.0040 - val_mean_absolute_error: 0.0302 - val_mean_squared_error: 0.0040\n",
      "Epoch 173/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0017 - mean_absolute_error: 0.0181 - mean_squared_error: 0.0017 - val_loss: 0.0029 - val_mean_absolute_error: 0.0288 - val_mean_squared_error: 0.0029\n",
      "Epoch 174/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0016 - mean_absolute_error: 0.0176 - mean_squared_error: 0.0016 - val_loss: 0.0028 - val_mean_absolute_error: 0.0259 - val_mean_squared_error: 0.0028\n",
      "Epoch 175/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0017 - mean_absolute_error: 0.0182 - mean_squared_error: 0.0017 - val_loss: 0.0022 - val_mean_absolute_error: 0.0263 - val_mean_squared_error: 0.0022\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0019 - mean_absolute_error: 0.0185 - mean_squared_error: 0.0019 - val_loss: 0.0019 - val_mean_absolute_error: 0.0244 - val_mean_squared_error: 0.0019\n",
      "Epoch 177/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0017 - mean_absolute_error: 0.0178 - mean_squared_error: 0.0017 - val_loss: 0.0038 - val_mean_absolute_error: 0.0327 - val_mean_squared_error: 0.0038\n",
      "Epoch 178/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0016 - mean_absolute_error: 0.0175 - mean_squared_error: 0.0016 - val_loss: 0.0028 - val_mean_absolute_error: 0.0254 - val_mean_squared_error: 0.0028\n",
      "Epoch 179/200\n",
      "4222/4222 [==============================] - 0s 31us/sample - loss: 0.0017 - mean_absolute_error: 0.0177 - mean_squared_error: 0.0017 - val_loss: 0.0018 - val_mean_absolute_error: 0.0258 - val_mean_squared_error: 0.0018\n",
      "Epoch 180/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0017 - mean_absolute_error: 0.0184 - mean_squared_error: 0.0017 - val_loss: 0.0026 - val_mean_absolute_error: 0.0260 - val_mean_squared_error: 0.0026\n",
      "Epoch 181/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0017 - mean_absolute_error: 0.0180 - mean_squared_error: 0.0017 - val_loss: 0.0023 - val_mean_absolute_error: 0.0273 - val_mean_squared_error: 0.0023\n",
      "Epoch 182/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0019 - mean_absolute_error: 0.0179 - mean_squared_error: 0.0019 - val_loss: 0.0022 - val_mean_absolute_error: 0.0240 - val_mean_squared_error: 0.0022\n",
      "Epoch 183/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0019 - mean_absolute_error: 0.0182 - mean_squared_error: 0.0019 - val_loss: 0.0017 - val_mean_absolute_error: 0.0195 - val_mean_squared_error: 0.0017\n",
      "Epoch 184/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0019 - mean_absolute_error: 0.0186 - mean_squared_error: 0.0019 - val_loss: 0.0020 - val_mean_absolute_error: 0.0246 - val_mean_squared_error: 0.0020\n",
      "Epoch 185/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0020 - mean_absolute_error: 0.0185 - mean_squared_error: 0.0020 - val_loss: 0.0025 - val_mean_absolute_error: 0.0254 - val_mean_squared_error: 0.0025\n",
      "Epoch 186/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0016 - mean_absolute_error: 0.0171 - mean_squared_error: 0.0016 - val_loss: 0.0021 - val_mean_absolute_error: 0.0258 - val_mean_squared_error: 0.0021\n",
      "Epoch 187/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0018 - mean_absolute_error: 0.0181 - mean_squared_error: 0.0018 - val_loss: 0.0019 - val_mean_absolute_error: 0.0209 - val_mean_squared_error: 0.0019\n",
      "Epoch 188/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0017 - mean_absolute_error: 0.0177 - mean_squared_error: 0.0017 - val_loss: 0.0026 - val_mean_absolute_error: 0.0270 - val_mean_squared_error: 0.0026\n",
      "Epoch 189/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0021 - mean_absolute_error: 0.0190 - mean_squared_error: 0.0021 - val_loss: 0.0020 - val_mean_absolute_error: 0.0269 - val_mean_squared_error: 0.0020\n",
      "Epoch 190/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0017 - mean_absolute_error: 0.0181 - mean_squared_error: 0.0017 - val_loss: 0.0022 - val_mean_absolute_error: 0.0282 - val_mean_squared_error: 0.0022\n",
      "Epoch 191/200\n",
      "4222/4222 [==============================] - 0s 37us/sample - loss: 0.0018 - mean_absolute_error: 0.0182 - mean_squared_error: 0.0018 - val_loss: 0.0020 - val_mean_absolute_error: 0.0236 - val_mean_squared_error: 0.0020\n",
      "Epoch 192/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0016 - mean_absolute_error: 0.0175 - mean_squared_error: 0.0016 - val_loss: 0.0016 - val_mean_absolute_error: 0.0219 - val_mean_squared_error: 0.0016\n",
      "Epoch 193/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0018 - mean_absolute_error: 0.0178 - mean_squared_error: 0.0018 - val_loss: 0.0025 - val_mean_absolute_error: 0.0294 - val_mean_squared_error: 0.0025\n",
      "Epoch 194/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0015 - mean_absolute_error: 0.0174 - mean_squared_error: 0.0015 - val_loss: 0.0019 - val_mean_absolute_error: 0.0243 - val_mean_squared_error: 0.0019\n",
      "Epoch 195/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0016 - mean_absolute_error: 0.0172 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_absolute_error: 0.0201 - val_mean_squared_error: 0.0019\n",
      "Epoch 196/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0023 - mean_absolute_error: 0.0193 - mean_squared_error: 0.0023 - val_loss: 0.0019 - val_mean_absolute_error: 0.0242 - val_mean_squared_error: 0.0019\n",
      "Epoch 197/200\n",
      "4222/4222 [==============================] - 0s 34us/sample - loss: 0.0022 - mean_absolute_error: 0.0196 - mean_squared_error: 0.0022 - val_loss: 0.0019 - val_mean_absolute_error: 0.0231 - val_mean_squared_error: 0.0019\n",
      "Epoch 198/200\n",
      "4222/4222 [==============================] - 0s 35us/sample - loss: 0.0016 - mean_absolute_error: 0.0173 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_absolute_error: 0.0228 - val_mean_squared_error: 0.0019\n",
      "Epoch 199/200\n",
      "4222/4222 [==============================] - 0s 32us/sample - loss: 0.0016 - mean_absolute_error: 0.0171 - mean_squared_error: 0.0016 - val_loss: 0.0017 - val_mean_absolute_error: 0.0228 - val_mean_squared_error: 0.0017\n",
      "Epoch 200/200\n",
      "4222/4222 [==============================] - 0s 33us/sample - loss: 0.0017 - mean_absolute_error: 0.0173 - mean_squared_error: 0.0017 - val_loss: 0.0019 - val_mean_absolute_error: 0.0267 - val_mean_squared_error: 0.0019\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "X_YNN_nomob, y_YNN_nomob = get_YNN_states_dataset(state_cases_df, n_steps=N_STEPS)\n",
    "X_train_YNN_nomob, X_test_YNN_nomob, y_train_YNN_nomob, y_test_YNN_nomob = train_test_split(X_YNN_nomob, y_YNN_nomob, test_size=SPLIT, shuffle=False)\n",
    "\n",
    "# Set up model\n",
    "n_ynn_features = X_train_YNN_nomob.shape[1]\n",
    "ynn_nomob = build_YNN_model(n_ynn_features)\n",
    "\n",
    "# Fit CNN\n",
    "ynn_nomob_history = ynn_nomob.fit(X_train_YNN_nomob, y_train_YNN_nomob,\n",
    "                                     epochs=EPOCHS,\n",
    "                                     validation_data=(X_test_YNN_nomob, y_test_YNN_nomob),\n",
    "                                     verbose=1,\n",
    "                                     shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAZD0lEQVR4nO3dfZRddX3v8fdHIqDFkgBzQwipAxgLUa6BTikFV8uDVeBagi1CslSiF41U6NKF19VQulanXYvWqkgXreKNQgkWeRBhMQrVG5Ko1CXgQENICMgAckkayBQRn9MSv/1j/86PzeTMzJnJ2Wefmfm81jpr9vntp2/2TM7n7KffVkRgZmYG8Iq6CzAzs+7hUDAzs8yhYGZmmUPBzMwyh4KZmWWz6i5gTxx00EHR29tbdxlmZlPK/fff/x8R0dNs3JQOhd7eXgYHB+suw8xsSpH01GjjfPjIzMwyh4KZmWUOBTMzyxwKZmaWVRYKkvaVdJ+kByVtlvRXqf0wSfdKGpJ0k6S9U/s+6f1QGt9bVW1mZtZclXsKO4FTIuJNwGLgNEnHA38HXBERrwOeB85P058PPJ/ar0jTmZlZB1UWClH4aXr7yvQK4BTgltS+GjgrDS9J70njT5WkquozM7PdVXpOQdJekjYAO4A1wOPAjyLixTTJVmB+Gp4PPA2Qxr8AHNhkmSskDUoaHB4errJ8M7MZp9JQiIhdEbEYOBQ4DjiyDctcFRF9EdHX09P0hjwzM5ukjlx9FBE/AtYDvwvMltS4k/pQYFsa3gYsAEjj9wee60R9NjN95oJ1dZdg1nWqvPqoR9LsNPwq4A+ALRThcHaabDlwexoeSO9J49eFHwtnZtZRVe4pzAPWS9oIfA9YExFfA/4MuFjSEMU5g6vT9FcDB6b2i4GVFdZWif7+/rpLmJj+/euuwMy6TGUd4kXERuCYJu1PUJxfGNn+S+CdVdVjNqb+/aH/hbqrMKud72g2M7PMoWBmZplDwczMMoeCzXgHr99QdwlmXcOhYGZmmUPBzMwyh4KZmWUOBTMzyxwKZmaWORTMzCxzKJiZWeZQMDOzzKFgZmaZQ8HMzDKHgpmZZQ4FMzPLHApmZpY5FMzMLHMomJlZ5lAwM7PMoWBmZplDwczMMoeCmZllDgUzM8scCmZmljkUzMwsqywUJC2QtF7Sw5I2S/pwau+XtE3ShvQ6ozTPJZKGJD0q6W1V1WZmZs3NqnDZLwIfjYgHJL0GuF/SmjTuioj4VHliSYuApcAbgEOAuyS9PiJ2VVijmZmVVLanEBHbI+KBNPwTYAswf4xZlgA3RsTOiHgSGAKOq6o+MzPbXUfOKUjqBY4B7k1NF0naKOkaSXNS23zg6dJsW2kSIpJWSBqUNDg8PFxh1WZmM0/loSBpP+ArwEci4sfAVcARwGJgO3D5RJYXEasioi8i+np6etper5nZTFZpKEh6JUUgXB8RtwJExLMRsSsifgV8npcOEW0DFpRmPzS1mZlZh1R59ZGAq4EtEfHpUvu80mTvADal4QFgqaR9JB0GLATuq6o+MzPbXZVXH50IvAd4SNKG1PbnwDJJi4EAfgB8ECAiNku6GXiY4sqlC33lkZlZZ1UWChHxr4CajLpzjHkuAy6rqiYzMxub72g2M7PMoWBmZplDwczMMoeCmZllDgUzM8scCmZmljkUzMwscyjMUGvXHVF3CWbWhRwKZmaWORTMzCxzKJiZWeZQMDOzzKFgZmaZQ8HMzDKHgpmZZQ4FMzPLHApmZpY5FMzMLHMomJlZ5lAwM7PMoWBmZplDwczMMoeCmZllDgUzM8scCmZmljkUzMwsqywUJC2QtF7Sw5I2S/pwaj9A0hpJj6Wfc1K7JF0paUjSRknHVlWbmZk1V+WewovARyNiEXA8cKGkRcBKYG1ELATWpvcApwML02sFcFWFtZmZWROVhUJEbI+IB9LwT4AtwHxgCbA6TbYaOCsNLwGui8I9wGxJ86qqz8zMdteRcwqSeoFjgHuBuRGxPY16BpibhucDT5dm25raRi5rhaRBSYPDw8OV1WxmNhNVHgqS9gO+AnwkIn5cHhcRAcRElhcRqyKiLyL6enp62lipmZlVGgqSXkkRCNdHxK2p+dnGYaH0c0dq3wYsKM1+aGozM7MOqfLqIwFXA1si4tOlUQPA8jS8HLi91H5eugrpeOCF0mEmMzPrgFkVLvtE4D3AQ5I2pLY/Bz4O3CzpfOAp4Jw07k7gDGAI+DnwvgprMzOzJioLhYj4V0CjjD61yfQBXFhVPWZmNj7f0WxmZplDwczMMoeCmZllDgUzM8scCmZmljkUzMwscyiYmVnmUDAzs8yhYGZmmUPBzMwyh4KZmWUOhRlo68q76y7BzLpUS6Eg6cRW2szMbGprdU/hH1psMzOzKWzMrrMl/S5wAtAj6eLSqF8H9qqyMDMz67zxnqewN7Bfmu41pfYfA2dXVZSZmdVjzFCIiG8B35J0bUQ81aGazMysJq0+eW0fSauA3vI8EXFKFUWZmVk9Wg2FLwOfA74A7KquHDMzq1OrofBiRFxVaSVmZla7Vi9J/aqkD0maJ+mAxqvSyszMrONa3VNYnn5+rNQWwOHtLcfMzOrUUihExGFVF2JmZvVrKRQkndesPSKua285ZmZWp1YPH/12aXhf4FTgAcChYGY2jbR0ojki/rT0+gBwLMWdzqOSdI2kHZI2ldr6JW2TtCG9ziiNu0TSkKRHJb1tsv8gMzObvMl2nf0zYLzzDNcCpzVpvyIiFqfXnQCSFgFLgTekeT4ryX0rmZl1WKvnFL5KcbURFB3hHQXcPNY8EfFtSb0t1rEEuDEidgJPShoCjgO+2+L8ZmbWBq2eU/hUafhF4KmI2DrJdV6UTlwPAh+NiOeB+cA9pWm2pjYzM+ugVs8pfAt4hKKn1DnAf05yfVcBRwCLge3A5RNdgKQVkgYlDQ4PD0+yDDMza6bVJ6+dA9wHvBM4B7hX0oS7zo6IZyNiV0T8Cvg8xSEigG3AgtKkh6a2ZstYFRF9EdHX09Mz0RLMzGwMrR4+uhT47YjYASCpB7gLuGUiK5M0LyK2p7fvABpXJg0AX5L0aeAQYCFFCJmZWQe1GgqvaARC8hzj7GVIugE4CThI0lbgL4GTJC2mOGn9A+CDABGxWdLNwMMU5ywujAj3xmpm1mGthsLXJX0DuCG9Pxe4c6wZImJZk+arx5j+MuCyFusxM7MKjPeM5tcBcyPiY5L+CHhzGvVd4PqqizMzs84ab0/h74FLACLiVuBWAElHp3F/WGl1ZmbWUeNdfTQ3Ih4a2ZjaeiupyMzMajNeKMweY9yr2lmImZnVb7xQGJT0gZGNkt4P3F9NSWZmVpfxzil8BLhN0rt4KQT6gL0p7jMwM7NpZMxQiIhngRMknQy8MTXfERHrKq/MzMw6rtXHca4H1ldci5mZ1Wyyz1MwM7NpyKFgZmaZQ8HMzDKHgpmZZQ4FMzPLHApmZpY5FMzMLHMomJlZ5lAwM7PMoWBmZplDwczMMoeCmZllDgUzM8scCmZmljkUzMwscyiYmVnmUDAzs8yhYGZmWWWhIOkaSTskbSq1HSBpjaTH0s85qV2SrpQ0JGmjpGOrqsvMzEZX5Z7CtcBpI9pWAmsjYiGwNr0HOB1YmF4rgKsqrMvMzEZRWShExLeBH45oXgKsTsOrgbNK7ddF4R5gtqR5VdVmZmbNdfqcwtyI2J6GnwHmpuH5wNOl6bamtt1IWiFpUNLg8PBwdZWamc1AtZ1ojogAYhLzrYqIvojo6+npqaAyM7OZq9Oh8GzjsFD6uSO1bwMWlKY7NLWZmVkHdToUBoDlaXg5cHup/bx0FdLxwAulw0xmZtYhs6pasKQbgJOAgyRtBf4S+Dhws6TzgaeAc9LkdwJnAEPAz4H3VVWXmZmNrrJQiIhlo4w6tcm0AVxYVS1mZtYa39FsZmaZQ8HMzDKHgpmZZQ4FMzPLHApmZpY5FMzMLHMomJlZ5lBok8vPfXvdJZiZ7TGHgpmZZQ4FMzPLHAptsOXIo+ouwcysLRwKZmaWORTMzCxzKJiZWeZQMDOzzKFgZmaZQ8HMzDKHgpmZZQ4FMzPLHApmZpY5FMzMLHMo2G7cbYfZzOVQMDOzzKGwB9auO6LuEszM2sqhYGZmmUPBzMyyWXWsVNIPgJ8Au4AXI6JP0gHATUAv8APgnIh4vo76zMxmqjr3FE6OiMUR0ZferwTWRsRCYG16b9aSg9dvqLsEs2mhmw4fLQFWp+HVwFk11mJmNiPVFQoB/D9J90takdrmRsT2NPwMMLfZjJJWSBqUNDg8PNyJWq1L+X4Ks/arKxTeHBHHAqcDF0r6vfLIiAiK4NhNRKyKiL6I6Ovp6elAqRPjwxhmNpXVEgoRsS393AHcBhwHPCtpHkD6uaOO2szMZrKOh4KkX5P0msYw8FZgEzAALE+TLQdu73RtZmYzXR2XpM4FbpPUWP+XIuLrkr4H3CzpfOAp4JwaajMzm9E6HgoR8QTwpibtzwGndroeMzN7STddkmpmZjVzKJiZWeZQMDOzzKFgVgPfeGfdyqFgU1P//nVXYDYtORSsa/SuvKPuEsxmPIeCvVzN38CPXn10reufatytirWbQ8GsTRxoNh04FMys7aoMyK0r72btuiPoXXmHg7gCDoUZrnEVTH9/f72FzFDl7d72K5L692fLkUdx+blvp7+/P3+IfuaCdfmD1Wwkh4Jl0+H4dOODtds+8JqF7uXnvh3ovu0+2W23deXdL3v/mQvWtaOcjvLFDg4F6wKND8eGiXyYjJx3qhv5wVpuH3nIJAdNaY+gqvWPp/w7mw4frI29qZEae10Hr9+w23Y/eP2GvCc2lTkUbNqZ7AdbO7Tz0MxYx8sr/+BJV6GNDJqJHuLq9O9iott9vHMSjQ//mcShYF2h2w73TNRo347b9Q2+EyYdNHv4oem7u7uLQ8GsItPhMMpY8mGULjdejY0w3JMvJtPpQg2Hgk155WPr00Hj3MF0U776aeSHaN17U+XDXDP9MleHglk3mEaB1qqRx+unyp7HdOdQMDOzzKFg1iH+Ftw5Pnk9eQ4Fq/14rpl1D4eCmZllDgUzM8scCmZmljkUzMwscyiYmVnmUDAzs6zrQkHSaZIelTQkaWXd9ZiZdVLd91h0VShI2gv4DHA6sAhYJmlRvVWZGTB1uuKYKnV2qa4KBeA4YCginoiI/wRuBJbUXJPZjOe7sWcORUTdNWSSzgZOi4j3p/fvAX4nIi4qTbMCWJHe/ibw6ARXcxDwH20ot926sa5urAm6s65urAm6s65urAm6s66qanptRPQ0GzGrgpVVKiJWAasmO7+kwYjoa2NJbdGNdXVjTdCddXVjTdCddXVjTdCdddVRU7cdPtoGLCi9PzS1mZlZB3RbKHwPWCjpMEl7A0uBgZprMjObMbrq8FFEvCjpIuAbwF7ANRGxuc2rmfShp4p1Y13dWBN0Z13dWBN0Z13dWBN0Z10dr6mrTjSbmVm9uu3wkZmZ1cihYGZm2bQMBUnvlLRZ0q8kjXo512hdaqQT3fem9pvSSe89rekASWskPZZ+zmkyzcmSNpRev5R0Vhp3raQnS+MW72lNrdaVpttVWvdAqb2ubbVY0nfT73mjpHNL49q6rcbrekXSPunfPpS2RW9p3CWp/VFJb9uTOiZY08WSHk7bZq2k15bGNf1ddqiu90oaLq3//aVxy9Pv/DFJyztY0xWler4v6UelcZVsK0nXSNohadMo4yXpylTzRknHlsZVsp2yiJh2L+Aoihvbvgn0jTLNXsDjwOHA3sCDwKI07mZgaRr+HPAnbajpE8DKNLwS+Ltxpj8A+CHw6vT+WuDsCrZVS3UBPx2lvZZtBbweWJiGDwG2A7Pbva3G+jspTfMh4HNpeClwUxpelKbfBzgsLWevDtV0culv508aNY31u+xQXe8F/nGUv/cn0s85aXhOJ2oaMf2fUlzgUvW2+j3gWGDTKOPPAP4FEHA8cG+V26n8mpZ7ChGxJSLGu9O5aZcakgScAtySplsNnNWGspakZbW6zLOBf4mIn7dh3WOZaF1ZndsqIr4fEY+l4X8HdgBN79DcQ610vVKu9xbg1LRtlgA3RsTOiHgSGErLq7ymiFhf+tu5h+Ken6rtSTc1bwPWRMQPI+J5YA1wWg01LQNuaMN6xxQR36b40jeaJcB1UbgHmC1pHtVtp2xahkKL5gNPl95vTW0HAj+KiBdHtO+puRGxPQ0/A8wdZ/ql7P7HeVnalbxC0j5tqGkide0raVDSPY1DWnTJtpJ0HMW3wMdLze3aVqP9nTSdJm2LFyi2TSvzVlVT2fkU3zobmv0u26HVuv44/W5ukdS4WbX2bZUOsR0GrCs1V7WtxjNa3VVtp6yr7lOYCEl3AQc3GXVpRNze6Xpg7JrKbyIiJI16LXD6RnA0xf0aDZdQfEDuTXHt8p8Bf93Bul4bEdskHQ6sk/QQxYffpLR5W30RWB4Rv0rNk95W042kdwN9wO+Xmnf7XUbE482X0HZfBW6IiJ2SPkixh3VKh9Y9nqXALRGxq9RW57aqxZQNhYh4yx4uYrQuNZ6j2FWblb71tdzVxlg1SXpW0ryI2J4+yHaMsahzgNsi4r9Ky258c94p6Z+A/9NKTe2qKyK2pZ9PSPomcAzwFWrcVpJ+HbiD4ovAPaVlT3pbNdFK1yuNabZKmgXsT/F3VFW3LS0tV9JbKEL29yNiZ6N9lN9lOz7oxq0rIp4rvf0CxfmjxrwnjZj3m52oqWQpcGG5ocJtNZ7R6q5qO2Uz+fBR0y41ojibs57imD7AcqAdex4DaVmtLHO345rpw7FxHP8soOlVC1XUJWlO4xCMpIOAE4GH69xW6Xd2G8Vx11tGjGvntmql65VyvWcD69K2GQCWqrg66TBgIXDfHtTSck2SjgH+L3BmROwotTf9Xbahplbrmld6eyawJQ1/A3hrqm8O8FZevqdcWU2priMpTtx+t9RW5bYazwBwXroK6XjghfRlp6rt9JJ2nrXulhfwDopjbTuBZ4FvpPZDgDtL050BfJ8i+S8ttR9O8Z93CPgysE8bajoQWAs8BtwFHJDa+4AvlKbrpfg28IoR868DHqL4gPtnYL82batx6wJOSOt+MP08v+5tBbwb+C9gQ+m1uIpt1ezvhOJw1JlpeN/0bx9K2+Lw0ryXpvkeBU5v49/4eDXdlf72G9tmYLzfZYfq+ltgc1r/euDI0rz/O23DIeB9naopve8HPj5ivsq2FcWXvu3pb3grxXmfC4AL0nhRPHDs8bTuvtK8lWynxsvdXJiZWTaTDx+ZmdkIDgUzM8scCmZmljkUzMwscyiYmVnmUDAzs8yhYLUqdU28SdKXJb16D5Z1kqSvpeEz1aSb5NK0syV9qPT+EEm3jDb9BOv4poqumhtdLrdluaOsq1fSLyRtKLWFpH8uvZ+lorvqxrYpd1/9sKQPlKY9TdJ9kh5J42+S9Btp3CclPSNpT+4Qty43Zbu5sGnjFxGxGEDS9RQ38Hy6MTLdlax4qV+jlkTEAE3uXC2ZTdHl9WfT9P/OS3dmt8O7ImJwtJGlrkGavm91vuTxxjZMfga8UdKrIuIXwB+we9cON0XERZL+B7BZxbMCeoB/oLipa0ta35kUN1T+/4j4mKSfjVejTW3eU7BucjfwuvTt91FJ11HclbxA0ltVPFTngbRHsR/kb7aPSHoA+KPGgtK34X9Mw3Ml3SbpwfQ6Afg4cET6NvzJtM5Nafp9Jf2TpIck/Zukk0vLvFXS11U84OQTTICKh/98TtK9wCck9Uv6oqTvAF8cZ70DktZR3OndijuB/5WGR+0OOoouMB4HXkvRceDfNAIhjR+IoptnmyEcCtYVVHQkdzrFLf1Q9BP02Yh4A8U3378A3hIRxwKDwMWS9gU+D/wh8Fs073UV4ErgWxHxJooHm2ymeHjP4xGxOCI+NmL6Cyk6aD2a4gN1dVoXwGLgXIpebM/VS10/j3R96fDRJ0vthwInRMTF6f2i9O9aNs56j6V4cFC5t9Ox3EjR79K+wP8E7m02kYrePw+n6DLhDcADLS7fpikfPrK6vap0PPxu4GqKPqqeipd6Pj2e4sPzO8XRJPam6LjsSODJSA/bScfRVzRZxynAeQBRdIv8gkZ57GjyZorDKETEI5KeonjSG8DaiHghre9him/YTzdZxmiHj74cL++aeSAd4hlvvWsiYqyHsrxMRGxU8VjQZRR7DSOdK+nNFP2DfTAifpi2Lenf1uh/6tXAqoj4VKvrtqnNoWB1y+cUGtKHU/nYtSg+FJeNmK4tz6meoJ2l4V1M/P/QyGPyrR6jn8yx/AHgUxRdLR84YtxNEXHRiLbNFHskD0bRxfXidFJ5v0ms26YoHz6yqeAe4ERJrwOQ9GuSXg88AvRKOiJNt2yU+ddSPKcYSXtJ2h/4CfCaUaa/G3hXmv71wG9Q9HJatXav9xrgryLioXGnLHwCuFTSUaW2SV8NZlOTQ8G6XkQMUzzw/QZJG0mHjiLilxSHi+5IJ5pHe3DRh4GTVTwt7n6KB7c/R3E4atOIY/5QXJH0ijT9TcB7o/SQmhaVzync1eI87VhvFhFbI+LKCUz/EMW2ui6d6P8OcBTwpcnWYFOPu842m8LSeYOvRcQbO7S+fuCnPscwfXlPwWxq2wXsX755rSppj+rdTO78hk0R3lMwM7PMewpmZpY5FMzMLHMomJlZ5lAwM7PsvwG5mnEpISwzOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.004527903903583825"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the predicted vs. actual, we can see that the values are fitted pretty well\n",
    "test_pred = ynn.predict(X_test_YNN)\n",
    "plt.scatter(test_pred, y_test_YNN)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# The errors seem to have much normality, but a bit right-skewed\n",
    "errors = test_pred - y_test_YNN\n",
    "plt.hist(errors, bins=25)\n",
    "plt.xlabel(\"Prediction Error [MPG]\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# And the residuals mean is close to zero. \n",
    "np.mean(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.007992969348711363"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Still, we can see that the predicted is pretty close to the actual, \n",
    "# but with less precisions as the model with Mobility data\n",
    "test_ynn2 = ynn_nomob.predict(X_test_YNN_nomob)\n",
    "plt.scatter(test_ynn2, y_test_YNN_nomob)\n",
    "plt.xlabel(\"predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# Errors shows much normality, but seems to have a little right-skewed\n",
    "errors2 = test_ynn2 - y_test_YNN_nomob\n",
    "plt.hist(errors2, bins=25)\n",
    "plt.xlabel(\"Prediction Error [MPG]\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# And the error mean is pretty closed to zero\n",
    "np.mean(errors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
