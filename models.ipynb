{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, normalize\n",
    "#\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#import tensorflow_docs as tfdocs\n",
    "#import tensorflow_docs.plots\n",
    "#import tensorflow_docs.modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Display\n",
    "from IPython.display import SVG\n",
    "import pydot, pydotplus, graphviz\n",
    "from keras.utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set filenames here\n",
    "cfi = 'data/covid_cases.csv' # Case time series\n",
    "dfi = 'data/covid_deaths.csv' # Death time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and shape data\n",
    "casesdf = pd.read_csv(cfi).iloc[:,11:].apply(sum).values\n",
    "deathdf = pd.read_csv(dfi).iloc[:,12:].apply(sum).values\n",
    "casesdf = casesdf.reshape(len(casesdf), 1)\n",
    "deathdf = deathdf.reshape(len(deathdf), 1)\n",
    "#\n",
    "# Import dataset, confirmed cases time series\n",
    "origdf = pd.read_csv(\"data/confirmed_diff.csv\")\n",
    "df = origdf.copy()\n",
    "df.dropna()\n",
    "df = df[['region', 'Value', 'Province_State']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getters\n",
    "def get_dataset(dataset, n_steps=1, split=0.75, shuff=False):\n",
    "    #print(dataset.shape)\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - n_steps - 1):\n",
    "        a = dataset[i:(i+n_steps)]\n",
    "        X.append(a)\n",
    "        y.append(dataset[i + n_steps][0])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return train_test_split(X, y, test_size=(1-split), shuffle=shuff)\n",
    "\n",
    "def organize_data(x, days_offset=10):\n",
    "    # Unused\n",
    "    x['id'] = range(len(x)) \n",
    "    # Standardize x-value\n",
    "    x['norm-0'] = (x['Value'] - x['Value'].mean()) / x['Value'].std()\n",
    "    # Create columns for past days\n",
    "    for item in range(days_offset):\n",
    "        x['norm-%s'%str(item+1)] = x.shift(periods=(1+item))['norm-0']\n",
    "    # And truncate the NAs\n",
    "    out = x[days_offset:]\n",
    "    return out\n",
    "\n",
    "def get_ynn(train_dataset):\n",
    "    # 2 hidden layers, 128 units, activation functions specified as relu\n",
    "    # and 1 final layer for prediction result\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation=\"relu\", input_shape=[len(train_dataset.keys())]),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "    \n",
    "    model.compile(loss=\"mse\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mae', 'mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_cnn(n_steps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, \n",
    "                     activation='relu', \n",
    "                     input_shape=(n_steps, n_features)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "def get_lstm(n_steps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(n_steps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = 'adam', loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "def get_stacked_lstm(n_steps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "    model.add(LSTM(128, input_shape=(n_steps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = 'adam', loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "def evaluate_model(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def plot_test(y, y_hat):\n",
    "    plt.plot(y, 'bo-', label='Actual')\n",
    "    plt.plot(y_hat, 'ro-', label='Predicted')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN/LSTM Settings\n",
    "n_steps = 5\n",
    "n_features = 1\n",
    "tr_split = .8\n",
    "\n",
    "# YNN Settings\n",
    "# Offset for prediction, default 10 for predicting a particular date by its previous 10 days confirmed cases\n",
    "OFFSET = 10\n",
    "EPOCHS = 500\n",
    "\n",
    "# All regions in the dataset\n",
    "regions = df['region'].unique()\n",
    "\n",
    "# Construct for the region dictionary & region encoding\n",
    "region_dict = {regions[x]:x for x in range(len(regions))}\n",
    "\n",
    "# Training States subset\n",
    "subset = [\"South Carolina\", \"California\", \n",
    "          \"North Carolina\", \"New York\", \n",
    "          \"New Jersey\", \"Georgia\", \n",
    "          \"Ohio\", \"Florida\", \n",
    "          \"Massachusetts\", \"Washington\"]\n",
    "\n",
    "# Testing States subset\n",
    "test_region = ['Illinois', 'Texas', 'Nevada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select Dataset for CNN/LSTM ###\n",
    "\n",
    "#dataset = casesdf\n",
    "dataset = deathdf\n",
    "\n",
    "### Setup Dataset for YNN ###\n",
    "traindf = df[df['Province_State'].isin(subset)]\n",
    "testdf = df[df['Province_State'].isin(test_region)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "\n",
    "# CNN/LSTM\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# YNN\n",
    "# Normalizes training dataset and create past 10 days data\n",
    "tr_df = traindf.groupby('region').apply(organize_data)\n",
    "# encodes the regions\n",
    "tr_df['region_encode'] = tr_df.apply(lambda x: region_dict[x['region']], axis=1)\n",
    "\n",
    "# Normalizes testing dataset and create past 10 days data\n",
    "te_df = testdf.groupby(\"region\").apply(organize_data)\n",
    "# encodes the regions\n",
    "te_df['region_encode'] = te_df.apply(lambda x: region_dict[x['region']], axis=1)\n",
    "# drops the hierarchical indexes\n",
    "tr_df.index = tr_df.index.droplevel()\n",
    "te_df.index = te_df.index.droplevel()\n",
    "# drops miscellaneous values\n",
    "tr_df.pop(\"Value\")\n",
    "tr_df.pop(\"id\")\n",
    "tr_df.pop(\"region\")\n",
    "tr_df.pop(\"Province_State\")\n",
    "# --------\n",
    "te_df.pop(\"Value\")\n",
    "te_df.pop(\"id\")\n",
    "te_df.pop(\"region\")\n",
    "te_df.pop(\"Province_State\")\n",
    "\n",
    "# Training and Testing targets\n",
    "train_labels = tr_df.pop('norm-0')\n",
    "test_labels = te_df.pop(\"norm-0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data for CNN/LSTM\n",
    "X_train, X_test, Y_train, Y_test = get_dataset(dataset, n_steps, tr_split, False)\n",
    "cnn = get_cnn(n_steps, n_features)\n",
    "lstm = get_lstm(n_steps, n_features)\n",
    "slstm = get_stacked_lstm(n_steps, n_features)\n",
    "ynn = get_ynn(tr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit it\n",
    "cnnhistory = cnn.fit(X_train, Y_train,\n",
    "                       epochs=100,\n",
    "                       validation_data=(X_test,Y_test),\n",
    "                       verbose=0,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmhistory = lstm.fit(X_train, Y_train,\n",
    "                       epochs=200,\n",
    "                       validation_data=(X_test,Y_test),\n",
    "                       verbose=0,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "slstmhistory = slstm.fit(X_train, Y_train,\n",
    "                        epochs=200,\n",
    "                        validation_data=(X_test,Y_test),\n",
    "                        verbose=0,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criteria for stopping, patience set to 15\n",
    "early_stop=keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "# train the model\n",
    "ynnhistory=ynn.fit(tr_df, train_labels, \n",
    "                   epochs=EPOCHS,\n",
    "                   validation_split=0.1, \n",
    "                   verbose=0, \n",
    "                   callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: CNN | Loss: 0.001 | MAE: 0.024 | MSE: 0.001\n"
     ]
    }
   ],
   "source": [
    "closs, cmae, cmse = cnn.evaluate(X_test, Y_test, verbose=2)\n",
    "print(f'Model: CNN | Loss: {closs:.3f} | MAE: {cmae:.3f} | MSE: {cmse:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LSTM | Loss: 0.002 | MAE: 0.033 | MSE: 0.002\n"
     ]
    }
   ],
   "source": [
    "lloss, lmae, lmse = lstm.evaluate(X_test, Y_test, verbose=2)\n",
    "print(f'Model: LSTM | Loss: {lloss:.3f} | MAE: {lmae:.3f} | MSE: {lmse:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: YNN | Loss: 0.018 | MAE: 0.108 | MSE: 0.018\n"
     ]
    }
   ],
   "source": [
    "sloss, smae, smse = slstm.evaluate(X_test, Y_test, verbose=2)\n",
    "print(f'Model: YNN | Loss: {sloss:.3f} | MAE: {smae:.3f} | MSE: {smse:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1090/1090 - 1s - loss: 0.1255 - mae: 0.1713 - mse: 0.1255\n",
      "Model: LSTM | Loss: 0.002 | MAE: 0.033 | MSE: 0.002\n"
     ]
    }
   ],
   "source": [
    "yloss, ymae, ymse = ynn.evaluate(te_df, test_labels, verbose=2)\n",
    "print(f'Model: YNN | Loss: {lloss:.3f} | MAE: {lmae:.3f} | MSE: {lmse:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = scaler.inverse_transform(Y_test.reshape(len(Y_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpreds = scaler.inverse_transform(cnn.predict(X_test))\n",
    "plot_test(ys, cpreds)\n",
    "evaluate_model(cnnhistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpreds = scaler.inverse_transform(lstm.predict(X_test))\n",
    "plot_test(ys, lpreds)\n",
    "evaluate_model(lstmhistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreds = scaler.inverse_transform(slstm.predict(X_test))\n",
    "plot_test(ys, spreds)\n",
    "evaluate_model(slstmhistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add YNN Test Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
